<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>MÃ©canisme d'Attention | IA4Ndada</title>

    <!-- MathJax pour les formules mathÃ©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">ğŸ  Accueil</a>
          <span>â€º</span>
          <span>ğŸ’¬ LLMs & IA Moderne</span>
          <span>â€º</span>
          <span>MÃ©canisme d'Attention</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>ğŸ‘ï¸ MÃ©canisme d'Attention : La RÃ©volution de l'IA Moderne</h1>
      <p class="subtitle">
        Module 5.1 - LLMs & Intelligence Artificielle Moderne
      </p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>ğŸ¯ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajoutÃ©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajoutÃ© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>ğŸ‰ Checkpoint - MÃ©canisme d'Attention</h3>
        <p>
          FÃ©licitations ! Vous comprenez maintenant le mÃ©canisme qui a
          rÃ©volutionnÃ© l'IA et rendu possible ChatGPT.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme complÃ©tÃ©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="../dl/rnn.html" class="nav-link" id="prev-link"
          >â† Module prÃ©cÃ©dent : RNN/LSTM</a
        >
        <a href="transformers.html" class="nav-link" id="next-link"
          >Module suivant : Transformers â†’</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module Attention
      const moduleConfig = {
        id: "llm-attention",
        title: "MÃ©canisme d'Attention : La RÃ©volution de l'IA Moderne",
        category: "LLMs & IA Moderne",
        objectives: [
          "Comprendre pourquoi l'attention a rÃ©volutionnÃ© l'IA",
          "MaÃ®triser le calcul manuel du mÃ©canisme d'attention",
          "Distinguer les diffÃ©rents types d'attention (self, cross, multi-head)",
          "Comprendre comment l'attention permet la comprÃ©hension du langage",
          "Voir le lien direct avec ChatGPT et les LLMs modernes",
        ],
        content: [
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "La rÃ©volution qui a changÃ© l'IA pour toujours",
            content: `
                        <p>Le <strong>mÃ©canisme d'attention</strong> est l'innovation qui a permis l'Ã©mergence de ChatGPT, GPT-4, et tous les LLMs modernes. C'est la diffÃ©rence entre une IA qui "lit" mot par mot et une IA qui "comprend" vraiment.</p>
                        
                        <p><strong>ğŸ”‘ RÃ©volution conceptuelle :</strong></p>
                        <ul>
                            <li>ğŸ¤– <strong>Avant (RNN/LSTM)</strong> : traitement sÃ©quentiel, mÃ©moire limitÃ©e</li>
                            <li>ğŸ§  <strong>AprÃ¨s (Attention)</strong> : accÃ¨s direct Ã  toute l'information, comprÃ©hension globale</li>
                        </ul>
                        
                        <p><strong>ğŸš€ Impact rÃ©volutionnaire :</strong></p>
                        <ul>
                            <li>ğŸ’¬ <strong>ChatGPT</strong> : conversations naturelles et contextuelles</li>
                            <li>ğŸŒ <strong>Traduction</strong> : Google Translate 10x plus prÃ©cis</li>
                            <li>ğŸ” <strong>Recherche</strong> : comprendre l'intention derriÃ¨re les requÃªtes</li>
                            <li>ğŸ“ <strong>RÃ©daction</strong> : gÃ©nÃ©ration de texte cohÃ©rent et crÃ©atif</li>
                            <li>ğŸ§  <strong>Raisonnement</strong> : rÃ©solution de problÃ¨mes complexes</li>
                        </ul>
                        
                        <p><strong>ğŸ“Š Chiffres impressionnants :</strong></p>
                        <ul>
                            <li>âš¡ <strong>ParallÃ©lisation</strong> : 100x plus rapide que les RNN</li>
                            <li>ğŸ§  <strong>Contexte</strong> : peut "voir" des milliers de mots simultanÃ©ment</li>
                            <li>ğŸ¯ <strong>PrÃ©cision</strong> : 95%+ sur des tÃ¢ches de comprÃ©hension</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Point clÃ© :</strong> L'attention ne traite plus le langage comme une sÃ©quence linÃ©aire, mais comme un <strong>rÃ©seau de relations</strong> oÃ¹ chaque mot peut influencer tous les autres.</p>
                    `,
          },
          {
            type: "intuition",
            icon: "ğŸ§ ",
            title: "L'analogie de la conversation au marchÃ© de Sandaga",
            content: `
                        <p>Imaginez une <strong>conversation animÃ©e au marchÃ© de Sandaga</strong> Ã  Dakar :</p>
                        
                        <p><strong>ğŸ—£ï¸ Phrase complexe :</strong></p>
                        <p><em>"Le tissu bleu que ma sÅ“ur Aminata a achetÃ© hier chez le marchand de la rue 10 Ã©tait vraiment magnifique."</em></p>
                        
                        <p><strong>ğŸ§  Comment votre cerveau comprend-il cette phrase ?</strong></p>
                        
                        <p><strong>âŒ Approche sÃ©quentielle (comme RNN) :</strong></p>
                        <ul>
                            <li>Mot 1 : "Le" â†’ aucune info</li>
                            <li>Mot 2 : "tissu" â†’ ok, on parle de tissu</li>
                            <li>Mot 3 : "bleu" â†’ tissu bleu</li>
                            <li>...</li>
                            <li>Mot 15 : "magnifique" â†’ mais magnifique quoi dÃ©jÃ  ? ğŸ¤”</li>
                        </ul>
                        
                        <p><strong>âœ… Approche attention (comme votre cerveau) :</strong></p>
                        <ul>
                            <li>ğŸ‘ï¸ <strong>Vision globale</strong> : lit toute la phrase d'un coup</li>
                            <li>ğŸ”— <strong>Connexions intelligentes</strong> : "magnifique" se rapporte Ã  "tissu"</li>
                            <li>ğŸ¯ <strong>Focus adaptatif</strong> : comprend que "Aminata" est l'acheteuse</li>
                            <li>ğŸ“ <strong>Contexte spatial</strong> : "rue 10" prÃ©cise le lieu</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ C'est exactement ce que fait l'attention :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>Regarder partout</strong> : accÃ¨s simultanÃ© Ã  tous les mots</li>
                            <li>ğŸ¯ <strong>Focaliser intelligemment</strong> : identifier les relations importantes</li>
                            <li>ğŸ§  <strong>Comprendre globalement</strong> : saisir le sens complet</li>
                        </ul>
                    `,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Qu'est-ce que l'attention exactement ?",
            content: `
                        <p>L'<strong>attention</strong> est un mÃ©canisme qui permet Ã  un modÃ¨le de <strong>se concentrer sÃ©lectivement</strong> sur les parties les plus pertinentes de l'information disponible.</p>
                        
                        <p><strong>ğŸ”‘ IdÃ©e fondamentale :</strong></p>
                        <p>Au lieu de traiter toute l'information de maniÃ¨re Ã©gale, l'attention calcule des <strong>"poids d'importance"</strong> pour chaque Ã©lÃ©ment et se concentre sur les plus importants.</p>
                        
                        <p><strong>ğŸ¯ Trois composants essentiels :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>Query (Q)</strong> : "Qu'est-ce que je cherche ?"</li>
                            <li>ğŸ—ï¸ <strong>Key (K)</strong> : "Qu'est-ce qui est disponible ?"</li>
                            <li>ğŸ’ <strong>Value (V)</strong> : "Quelle est l'information utile ?"</li>
                        </ul>
                        
                        <p><strong>ğŸ§  Analogie de la bibliothÃ¨que :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>Query</strong> : votre question "Livres sur l'IA au SÃ©nÃ©gal"</li>
                            <li>ğŸ—ï¸ <strong>Keys</strong> : titres et mots-clÃ©s de tous les livres</li>
                            <li>ğŸ’ <strong>Values</strong> : contenu complet des livres</li>
                            <li>âš–ï¸ <strong>Attention</strong> : scores de pertinence pour chaque livre</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ RÃ©sultat :</strong> Au lieu de lire tous les livres, vous vous concentrez sur les 3 plus pertinents avec des poids [0.6, 0.3, 0.1].</p>
                        
                        <p><strong>ğŸ’¡ En NLP :</strong> Pour comprendre "magnifique" dans notre phrase, l'attention regarde tous les mots et dÃ©cide que "tissu" est le plus pertinent (poids Ã©levÃ©).</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Formalisation mathÃ©matique de l'attention",
            content: `
                        <p><strong>ğŸ“ Formule fondamentale de l'attention :</strong></p>
                        <p>$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$</p>
                        
                        <p><strong>ğŸ” DÃ©cryptage Ã©tape par Ã©tape :</strong></p>
                        
                        <p><strong>1ï¸âƒ£ Calcul des scores de similaritÃ© :</strong></p>
                        <p>$$S = QK^T$$</p>
                        <p>Chaque Ã©lÃ©ment \\(S_{ij} = \\vec{q}_i \\cdot \\vec{k}_j\\) mesure la similaritÃ© entre la query i et la key j (voir <a href="../math/vectors.html">Module 1.1 - Vecteurs</a>)</p>
                        
                        <p><strong>2ï¸âƒ£ Normalisation par la dimension :</strong></p>
                        <p>$$S_{norm} = \\frac{S}{\\sqrt{d_k}}$$</p>
                        <p>Division par \\(\\sqrt{d_k}\\) pour Ã©viter que les scores deviennent trop grands (stabilitÃ© numÃ©rique)</p>
                        
                        <p><strong>3ï¸âƒ£ Conversion en probabilitÃ©s :</strong></p>
                        <p>$$A = \\text{softmax}(S_{norm})$$</p>
                        <p>Softmax transforme les scores en probabilitÃ©s qui somment Ã  1 (voir <a href="../math/probability.html">Module 1.6 - ProbabilitÃ©s</a>)</p>
                        
                        <p><strong>4ï¸âƒ£ AgrÃ©gation pondÃ©rÃ©e :</strong></p>
                        <p>$$\\text{Output} = AV$$</p>
                        <p>Combinaison des valeurs pondÃ©rÃ©es par les scores d'attention</p>
                        
                        <p><strong>ğŸ”‘ Dimensions :</strong></p>
                        <ul>
                            <li>Q : (n_queries Ã— d_k)</li>
                            <li>K : (n_keys Ã— d_k)</li>
                            <li>V : (n_keys Ã— d_v)</li>
                            <li>Output : (n_queries Ã— d_v)</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Calcul manuel d'attention sur 3 mots",
            content: `
                        <p><strong>ğŸ“ Exemple concret :</strong> Phrase "Aminata mange thiÃ©boudienne"</p>
                        
                        <p><strong>ğŸ”¢ ReprÃ©sentations vectorielles simplifiÃ©es (dimension 2) :</strong></p>
                        <ul>
                            <li>Aminata : [1, 0] (personne)</li>
                            <li>mange : [0, 1] (action)</li>
                            <li>thiÃ©boudienne : [1, 1] (nourriture sÃ©nÃ©galaise)</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Question :</strong> Pour comprendre "mange", sur quoi doit-on se concentrer ?</p>
                        
                        <p><strong>ğŸ“ Matrices Q, K, V (simplifiÃ©es) :</strong></p>
                        <p>$$Q = K = V = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ”¢ Ã‰tape 1 - Scores de similaritÃ© :</strong></p>
                        <p>$$S = QK^T = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ”¢ Ã‰tape 2 - Normalisation (d_k = 2) :</strong></p>
                        <p>$$S_{norm} = \\frac{S}{\\sqrt{2}} = \\begin{bmatrix} 0.71 & 0 & 0.71 \\\\ 0 & 0.71 & 0.71 \\\\ 0.71 & 0.71 & 1.41 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ”¢ Ã‰tape 3 - Softmax (ligne 2 = "mange") :</strong></p>
                        <p>Pour "mange" : scores [0, 0.71, 0.71]</p>
                        <p>$$\\text{softmax}([0, 0.71, 0.71]) = [0.33, 0.33, 0.33]$$</p>
                        
                        <p><strong>ğŸ’¡ InterprÃ©tation :</strong> "mange" accorde une attention Ã©gale Ã  "Aminata" et "thiÃ©boudienne" - logique pour comprendre qui mange quoi !</p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice pratique : calcul manuel complet",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Phrase : "Fatou aime Dakar"</p>
                        
                        <p><strong>ğŸ”¢ Vecteurs (dimension 2) :</strong></p>
                        <ul>
                            <li>Fatou : [2, 0] (personne forte)</li>
                            <li>aime : [1, 1] (sentiment)</li>
                            <li>Dakar : [0, 2] (lieu important)</li>
                        </ul>
                        
                        <p><strong>ğŸ“ Calculez l'attention pour chaque mot :</strong></p>
                        <ol>
                            <li>Matrice des scores S = QK^T</li>
                            <li>Normalisation par âˆš2</li>
                            <li>Softmax pour chaque ligne</li>
                            <li>InterprÃ©tez les rÃ©sultats</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('attention-manual-calculation')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="attention-manual-calculation" style="display: none;">
                        <ol>
                            <li><strong>Scores S :</strong><br>
                                $$S = \\begin{bmatrix} 2 & 0 \\\\ 1 & 1 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} 2 & 1 & 0 \\\\ 0 & 1 & 2 \\end{bmatrix} = \\begin{bmatrix} 4 & 2 & 0 \\\\ 2 & 2 & 2 \\\\ 0 & 2 & 4 \\end{bmatrix}$$</li>
                            <li><strong>Normalisation :</strong><br>
                                $$S_{norm} = \\frac{S}{\\sqrt{2}} = \\begin{bmatrix} 2.83 & 1.41 & 0 \\\\ 1.41 & 1.41 & 1.41 \\\\ 0 & 1.41 & 2.83 \\end{bmatrix}$$</li>
                            <li><strong>Softmax par ligne :</strong><br>
                                â€¢ Fatou : [0.84, 0.16, 0.00] â†’ se concentre sur elle-mÃªme<br>
                                â€¢ aime : [0.33, 0.33, 0.33] â†’ attention Ã©quilibrÃ©e<br>
                                â€¢ Dakar : [0.00, 0.16, 0.84] â†’ se concentre sur lui-mÃªme</li>
                            <li><strong>InterprÃ©tation :</strong><br>
                                â€¢ "Fatou" et "Dakar" s'auto-rÃ©fÃ©rencent (entitÃ©s importantes)<br>
                                â€¢ "aime" connecte Ã©quitablement Fatou et Dakar<br>
                                â€¢ Le verbe fait le lien entre sujet et objet !</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Self-Attention : quand chaque mot regarde tous les autres",
            content: `
                        <p><strong>ğŸ¤” Qu'est-ce que la Self-Attention ?</strong></p>
                        <p>La <strong>self-attention</strong> permet Ã  chaque mot d'une phrase de "regarder" tous les autres mots (y compris lui-mÃªme) pour mieux se comprendre dans le contexte.</p>
                        
                        <p><strong>ğŸ”‘ DiffÃ©rence cruciale :</strong></p>
                        <ul>
                            <li>ğŸ”„ <strong>Attention classique</strong> : une sÃ©quence regarde une autre sÃ©quence</li>
                            <li>ğŸ‘ï¸ <strong>Self-attention</strong> : une sÃ©quence se regarde elle-mÃªme</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Exemple concret :</strong></p>
                        <p><em>"Le prÃ©sident du SÃ©nÃ©gal a visitÃ© l'universitÃ©. Il Ã©tait impressionnÃ©."</em></p>
                        
                        <p><strong>ğŸ§  Self-attention rÃ©sout l'ambiguÃ¯tÃ© :</strong></p>
                        <ul>
                            <li>â“ <strong>Question</strong> : "Il" fait rÃ©fÃ©rence Ã  qui ?</li>
                            <li>ğŸ‘ï¸ <strong>Self-attention</strong> : regarde tous les mots prÃ©cÃ©dents</li>
                            <li>ğŸ¯ <strong>Conclusion</strong> : "Il" = "prÃ©sident" (score d'attention Ã©levÃ©)</li>
                        </ul>
                        
                        <p><strong>ğŸ“Š Matrice d'attention rÃ©sultante :</strong></p>
                        <p>Chaque ligne montre sur quoi se concentre chaque mot :</p>
                        <ul>
                            <li>"Le" â†’ [0.1, 0.7, 0.1, 0.1, ...] (se concentre sur "prÃ©sident")</li>
                            <li>"prÃ©sident" â†’ [0.1, 0.6, 0.1, 0.2, ...] (se concentre sur lui-mÃªme et "SÃ©nÃ©gal")</li>
                            <li>"Il" â†’ [0.05, 0.8, 0.05, 0.1, ...] (forte attention sur "prÃ©sident")</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ RÃ©volution :</strong> Plus besoin de rÃ¨gles grammaticales explicites ! Le modÃ¨le dÃ©couvre automatiquement les relations syntaxiques et sÃ©mantiques.</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Multi-Head Attention : plusieurs perspectives simultanÃ©es",
            content: `
                        <p><strong>ğŸ¤” Pourquoi une seule "tÃªte" d'attention ne suffit-elle pas ?</strong></p>
                        <p>Comme les humains, l'IA a besoin de <strong>plusieurs perspectives</strong> pour comprendre pleinement. Une tÃªte peut se concentrer sur la syntaxe, une autre sur la sÃ©mantique, etc.</p>
                        
                        <p><strong>ğŸ“ Formalisation Multi-Head :</strong></p>
                        <p>$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$</p>
                        
                        <p><strong>ğŸ” OÃ¹ chaque tÃªte calcule :</strong></p>
                        <p>$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$</p>
                        
                        <p><strong>ğŸ”§ Matrices de projection :</strong></p>
                        <ul style="list-style: none; padding-left: 0">
                            <li><strong>â€¢ \\(W_i^Q\\)</strong> : projette Q dans l'espace de la tÃªte i</li>
                            <li><strong>â€¢ \\(W_i^K\\)</strong> : projette K dans l'espace de la tÃªte i</li>
                            <li><strong>â€¢ \\(W_i^V\\)</strong> : projette V dans l'espace de la tÃªte i</li>
                            <li><strong>â€¢ \\(W^O\\)</strong> : combine toutes les tÃªtes</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Avantages du Multi-Head :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>SpÃ©cialisation</strong> : chaque tÃªte apprend un type de relation</li>
                            <li>ğŸ§  <strong>Richesse</strong> : capture multiple aspects simultanÃ©ment</li>
                            <li>âš¡ <strong>ParallÃ©lisation</strong> : calcul simultanÃ© de toutes les tÃªtes</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Exemple de spÃ©cialisation :</strong></p>
                        <ul>
                            <li><strong>TÃªte 1</strong> : relations sujet-verbe</li>
                            <li><strong>TÃªte 2</strong> : relations verbe-objet</li>
                            <li><strong>TÃªte 3</strong> : relations sÃ©mantiques</li>
                            <li><strong>TÃªte 4</strong> : dÃ©pendances Ã  long terme</li>
                        </ul>
                    `,
          },
          {
            type: "code",
            title: "ImplÃ©mentation attention from scratch",
            description:
              "ImplÃ©mentons le mÃ©canisme d'attention Ã©tape par Ã©tape :",
            code: `import numpy as np

class AttentionMechanism:
    def __init__(self, d_model=4):
        """
        MÃ©canisme d'attention simple
        d_model: dimension des embeddings
        """
        self.d_model = d_model
        self.d_k = d_model  # Dimension des keys/queries
        
    def softmax(self, x):
        """Fonction softmax stable numÃ©riquement"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    def attention(self, Q, K, V, mask=None):
        """
        Calcule l'attention entre Q, K, V
        Q: queries (n_q, d_k)
        K: keys (n_k, d_k)  
        V: values (n_k, d_v)
        """
        print("ğŸ§  CALCUL D'ATTENTION")
        print("=" * 30)
        
        # Ã‰tape 1: Scores de similaritÃ©
        scores = Q @ K.T
        print("ğŸ“Š Scores bruts (Q @ K^T):")
        print(scores.round(2))
        
        # Ã‰tape 2: Normalisation
        scores_norm = scores / np.sqrt(self.d_k)
        print(f"\\nğŸ“ Scores normalisÃ©s (Ã·âˆš{self.d_k}):")
        print(scores_norm.round(2))
        
        # Ã‰tape 3: Masquage (optionnel)
        if mask is not None:
            scores_norm = np.where(mask, scores_norm, -np.inf)
        
        # Ã‰tape 4: Softmax
        attention_weights = self.softmax(scores_norm)
        print("\\nâš–ï¸ Poids d'attention (softmax):")
        print(attention_weights.round(3))
        
        # Ã‰tape 5: AgrÃ©gation pondÃ©rÃ©e
        output = attention_weights @ V
        print("\\nğŸ¯ Sortie finale (poids @ V):")
        print(output.round(3))
        
        return output, attention_weights

# Test avec phrase sÃ©nÃ©galaise
attention = AttentionMechanism(d_model=3)

# Embeddings simplifiÃ©s pour "Fatou mange ceebu"
# Dimension 3: [personne, action, nourriture]
embeddings = np.array([
    [1.0, 0.1, 0.0],  # Fatou (personne)
    [0.1, 1.0, 0.1],  # mange (action)
    [0.0, 0.2, 1.0]   # ceebu (nourriture sÃ©nÃ©galaise)
])

mots = ["Fatou", "mange", "ceebu"]
print("ğŸ‡¸ğŸ‡³ ANALYSE: 'Fatou mange ceebu'")
print("ğŸ“Š Embeddings:")
for i, mot in enumerate(mots):
    print(f"  {mot}: {embeddings[i]}")

print()

# Self-attention: Q = K = V = embeddings
output, weights = attention.attention(embeddings, embeddings, embeddings)

print("\\nğŸ” INTERPRÃ‰TATION DES POIDS:")
for i, mot_query in enumerate(mots):
    print(f"\\n'{mot_query}' se concentre sur:")
    for j, mot_key in enumerate(mots):
        poids = weights[i, j]
        if poids > 0.4:
            niveau = "ğŸ”¥ FORTE"
        elif poids > 0.25:
            niveau = "ğŸ”¸ MOYENNE"
        else:
            niveau = "ğŸ”¹ FAIBLE"
        print(f"  {mot_key}: {poids:.3f} ({niveau})")`,
          },
          {
            type: "code",
            title: "Attention avec masquage",
            description:
              "ImplÃ©mentons l'attention avec masquage pour les modÃ¨les gÃ©nÃ©ratifs :",
            code: `def creer_masque_causal(taille):
    """CrÃ©e un masque triangulaire pour empÃªcher de voir le futur"""
    masque = np.tril(np.ones((taille, taille)))
    return masque.astype(bool)

# Test avec masquage causal (pour modÃ¨les gÃ©nÃ©ratifs comme GPT)
print("ğŸ­ ATTENTION AVEC MASQUAGE CAUSAL")
print("=" * 40)

# Phrase: "Dakar est belle"
mots_dakar = ["Dakar", "est", "belle"]
embeddings_dakar = np.array([
    [1.0, 0.0, 0.1],  # Dakar (lieu)
    [0.2, 1.0, 0.0],  # est (verbe)
    [0.1, 0.3, 1.0]   # belle (adjectif)
])

print("ğŸ“ Phrase: 'Dakar est belle'")
print("ğŸ¯ Objectif: PrÃ©dire le mot suivant sans voir le futur")

# Masque causal
masque = creer_masque_causal(3)
print("\\nğŸ­ Masque causal:")
print(masque.astype(int))
print("(1 = visible, 0 = masquÃ©)")

# Attention avec masque
output_masked, weights_masked = attention.attention(
    embeddings_dakar, embeddings_dakar, embeddings_dakar, mask=masque
)

print("\\nğŸ” ANALYSE AVEC MASQUAGE:")
for i, mot in enumerate(mots_dakar):
    print(f"\\n'{mot}' (position {i}) peut voir:")
    for j, mot_visible in enumerate(mots_dakar):
        if j <= i:  # Seulement les mots prÃ©cÃ©dents + lui-mÃªme
            poids = weights_masked[i, j]
            print(f"  {mot_visible}: {poids:.3f}")
        else:
            print(f"  {mot_visible}: MASQUÃ‰ ğŸš«")`,
          },
          {
            type: "code",
            title: "Multi-Head Attention simplifiÃ©e",
            description: "ImplÃ©mentons plusieurs tÃªtes d'attention :",
            code: `class MultiHeadAttention:
    def __init__(self, d_model=6, num_heads=2):
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # Dimension par tÃªte
        
        # Matrices de projection (simplifiÃ©es)
        np.random.seed(42)
        self.W_Q = np.random.randn(num_heads, d_model, self.d_k) * 0.1
        self.W_K = np.random.randn(num_heads, d_model, self.d_k) * 0.1
        self.W_V = np.random.randn(num_heads, d_model, self.d_k) * 0.1
        self.W_O = np.random.randn(d_model, d_model) * 0.1
        
    def attention_head(self, Q, K, V, head_idx):
        """Calcule l'attention pour une tÃªte spÃ©cifique"""
        # Projections
        Q_proj = Q @ self.W_Q[head_idx]
        K_proj = K @ self.W_K[head_idx]
        V_proj = V @ self.W_V[head_idx]
        
        # Attention
        scores = Q_proj @ K_proj.T / np.sqrt(self.d_k)
        weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)
        output = weights @ V_proj
        
        return output, weights
    
    def forward(self, X):
        """Calcul multi-head complet"""
        print(f"ğŸ§  MULTI-HEAD ATTENTION ({self.num_heads} tÃªtes)")
        print("=" * 40)
        
        heads_output = []
        heads_weights = []
        
        for i in range(self.num_heads):
            head_out, head_weights = self.attention_head(X, X, X, i)
            heads_output.append(head_out)
            heads_weights.append(head_weights)
            
            print(f"\\nğŸ‘ï¸ TÃŠTE {i+1}:")
            print("Poids d'attention:")
            print(head_weights.round(3))
        
        # ConcatÃ©nation des tÃªtes
        concat_heads = np.concatenate(heads_output, axis=-1)
        
        # Projection finale
        output = concat_heads @ self.W_O
        
        return output, heads_weights

# Test Multi-Head
print("ğŸ‡¸ğŸ‡³ TEST MULTI-HEAD: 'ThiÃ©boudienne dÃ©licieuse tradition'")

# Embeddings plus riches (dimension 6)
embeddings_rich = np.array([
    [1.0, 0.0, 0.8, 0.2, 0.0, 0.1],  # ThiÃ©boudienne
    [0.1, 0.9, 0.1, 0.8, 0.0, 0.0],  # dÃ©licieuse  
    [0.2, 0.1, 1.0, 0.1, 0.9, 0.8]   # tradition
])

mots_rich = ["ThiÃ©boudienne", "dÃ©licieuse", "tradition"]

multi_head = MultiHeadAttention(d_model=6, num_heads=2)
output_mh, weights_mh = multi_head.forward(embeddings_rich)

print("\\nğŸ¯ ANALYSE DES TÃŠTES:")
print("TÃªte 1 pourrait se spÃ©cialiser dans: relations culinaires")
print("TÃªte 2 pourrait se spÃ©cialiser dans: relations culturelles")`,
          },
          {
            type: "code",
            title: "Attention pour traduction automatique",
            description: "Application pratique : traduction Wolof â†’ FranÃ§ais :",
            code: `# Simulation traduction Wolof â†’ FranÃ§ais
print("ğŸŒ TRADUCTION AUTOMATIQUE: WOLOF â†’ FRANÃ‡AIS")
print("=" * 50)

# Phrase wolof: "Nanga def?" (Comment allez-vous?)
mots_wolof = ["Nanga", "def", "?"]
mots_francais = ["Comment", "allez-vous", "?"]

# Embeddings simplifiÃ©s (dimension 4)
# [salutation, verbe, politesse, ponctuation]
embed_wolof = np.array([
    [1.0, 0.2, 0.8, 0.0],  # Nanga (salutation polie)
    [0.1, 1.0, 0.1, 0.0],  # def (verbe Ãªtre/faire)
    [0.0, 0.0, 0.0, 1.0]   # ? (ponctuation)
])

embed_francais = np.array([
    [0.9, 0.1, 0.7, 0.0],  # Comment (question polie)
    [0.2, 0.9, 0.8, 0.0],  # allez-vous (verbe + politesse)
    [0.0, 0.0, 0.0, 1.0]   # ? (ponctuation)
])

print("ğŸ“Š Embeddings Wolof:")
for i, mot in enumerate(mots_wolof):
    print(f"  {mot}: {embed_wolof[i]}")

print("\\nğŸ“Š Embeddings FranÃ§ais:")
for i, mot in enumerate(mots_francais):
    print(f"  {mot}: {embed_francais[i]}")

# Cross-attention: Wolof (K,V) â†’ FranÃ§ais (Q)
attention_trad = AttentionMechanism(d_model=4)
output_trad, weights_trad = attention_trad.attention(
    embed_francais,  # Q: ce qu'on veut gÃ©nÃ©rer
    embed_wolof,     # K: ce qu'on a en source
    embed_wolof      # V: information source
)

print("\\nğŸ”— ALIGNEMENT WOLOF â†’ FRANÃ‡AIS:")
for i, mot_fr in enumerate(mots_francais):
    print(f"\\n'{mot_fr}' s'aligne avec:")
    for j, mot_wo in enumerate(mots_wolof):
        poids = weights_trad[i, j]
        if poids > 0.4:
            niveau = "ğŸ¯ FORTE"
        elif poids > 0.2:
            niveau = "ğŸ”¸ MOYENNE"
        else:
            niveau = "ğŸ”¹ FAIBLE"
        print(f"  '{mot_wo}': {poids:.3f} ({niveau})")

print("\\nğŸ’¡ Observation: 'Comment' s'aligne avec 'Nanga' (salutations)")
print("ğŸ’¡ 'allez-vous' s'aligne avec 'def' (verbes d'Ã©tat)")`,
          },
          {
            type: "code",
            title: "Visualisation des patterns d'attention",
            description: "Visualisons comment l'attention se concentre :",
            code: `import matplotlib.pyplot as plt

def visualiser_attention(weights, mots_source, mots_cible, titre):
    """Visualise la matrice d'attention comme une heatmap"""
    plt.figure(figsize=(10, 8))
    
    # CrÃ©ation de la heatmap
    im = plt.imshow(weights, cmap='Blues', aspect='auto', vmin=0, vmax=1)
    
    # Configuration des axes
    plt.xticks(range(len(mots_source)), mots_source, rotation=45)
    plt.yticks(range(len(mots_cible)), mots_cible)
    plt.xlabel('Mots source (Keys)', fontsize=12)
    plt.ylabel('Mots cible (Queries)', fontsize=12)
    plt.title(titre, fontsize=14, pad=20)
    
    # Ajout des valeurs dans les cellules
    for i in range(len(mots_cible)):
        for j in range(len(mots_source)):
            plt.text(j, i, f'{weights[i, j]:.2f}', 
                    ha='center', va='center', 
                    color='white' if weights[i, j] > 0.5 else 'black',
                    fontweight='bold')
    
    # Barre de couleur
    plt.colorbar(im, label='Poids d\'attention')
    plt.tight_layout()
    plt.show()

# Visualisation self-attention
print("ğŸ“Š VISUALISATION SELF-ATTENTION")
visualiser_attention(weights, mots, mots, 
                    "Self-Attention: 'Fatou aime Dakar'")

# Visualisation cross-attention (traduction)
print("\\nğŸŒ VISUALISATION CROSS-ATTENTION")
visualiser_attention(weights_trad, mots_wolof, mots_francais,
                    "Cross-Attention: Wolof â†’ FranÃ§ais")

print("\\nğŸ¨ Heatmaps gÃ©nÃ©rÃ©es !")
print("ğŸ’¡ Plus la couleur est foncÃ©e, plus l'attention est forte")`,
          },
          {
            type: "code",
            title: "Attention positionnelle",
            description:
              "Ajoutons l'encodage positionnel pour comprendre l'ordre :",
            code: `def encodage_positionnel(seq_len, d_model):
    """
    CrÃ©e des encodages positionnels sinusoÃ¯daux
    Permet au modÃ¨le de comprendre l'ordre des mots
    """
    pos_encoding = np.zeros((seq_len, d_model))
    
    for pos in range(seq_len):
        for i in range(0, d_model, 2):
            # FrÃ©quences diffÃ©rentes pour chaque dimension
            freq = 1 / (10000 ** (i / d_model))
            
            pos_encoding[pos, i] = np.sin(pos * freq)
            if i + 1 < d_model:
                pos_encoding[pos, i + 1] = np.cos(pos * freq)
    
    return pos_encoding

# Test encodage positionnel
print("ğŸ“ ENCODAGE POSITIONNEL")
print("=" * 30)

seq_len = 5
d_model = 4
pos_enc = encodage_positionnel(seq_len, d_model)

print("ğŸ”¢ Encodages positionnels (5 positions, 4 dimensions):")
print("Position | Dim0   | Dim1   | Dim2   | Dim3")
print("-" * 45)
for pos in range(seq_len):
    print(f"{pos:8d} | {pos_enc[pos, 0]:6.3f} | {pos_enc[pos, 1]:6.3f} | {pos_enc[pos, 2]:6.3f} | {pos_enc[pos, 3]:6.3f}")

# Visualisation
plt.figure(figsize=(12, 6))
for dim in range(d_model):
    plt.plot(range(seq_len), pos_enc[:, dim], 'o-', label=f'Dimension {dim}')

plt.xlabel('Position dans la sÃ©quence')
plt.ylabel('Valeur d\'encodage')
plt.title('Encodages Positionnels SinusoÃ¯daux')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print("\\nğŸ’¡ Chaque position a une 'signature' unique")
print("ğŸ¯ Le modÃ¨le peut distinguer 'Aminata aime Fatou' de 'Fatou aime Aminata'")`,
          },
          {
            type: "code",
            title: "Application complÃ¨te : analyse de sentiment",
            description:
              "Utilisons l'attention pour analyser le sentiment d'un texte :",
            code: `class AnalyseurSentiment:
    def __init__(self):
        # Dictionnaire de mots avec polaritÃ© Ã©motionnelle
        self.lexique_sentiment = {
            # Mots positifs sÃ©nÃ©galais
            "magnifique": 0.9, "excellent": 0.8, "dÃ©licieux": 0.8,
            "teranga": 0.9, "accueillant": 0.7, "gÃ©nÃ©reux": 0.7,
            
            # Mots nÃ©gatifs
            "mauvais": -0.8, "terrible": -0.9, "dÃ©cevant": -0.6,
            "cher": -0.4, "lent": -0.5, "sale": -0.7,
            
            # Mots neutres
            "restaurant": 0.0, "service": 0.0, "prix": 0.0,
            "nourriture": 0.1, "ambiance": 0.0
        }
    
    def encoder_mots(self, phrase):
        """Encode les mots avec sentiment + position"""
        mots = phrase.lower().split()
        embeddings = []
        
        for i, mot in enumerate(mots):
            # Embedding basique: [sentiment, position_normalisÃ©e, longueur_mot]
            sentiment = self.lexique_sentiment.get(mot, 0.0)
            position = i / len(mots)  # Position relative
            longueur = len(mot) / 10  # Longueur normalisÃ©e
            
            embedding = [sentiment, position, longueur]
            embeddings.append(embedding)
        
        return np.array(embeddings), mots
    
    def analyser_avec_attention(self, phrase):
        """Analyse le sentiment avec mÃ©canisme d'attention"""
        embeddings, mots = self.encoder_mots(phrase)
        
        print(f"ğŸ“ Phrase: '{phrase}'")
        print("\\nğŸ“Š Embeddings [sentiment, position, longueur]:")
        for i, mot in enumerate(mots):
            print(f"  {mot}: {embeddings[i].round(3)}")
        
        # Self-attention pour comprendre les relations
        attention_simple = AttentionMechanism(d_model=3)
        output, weights = attention_simple.attention(embeddings, embeddings, embeddings)
        
        # Calcul du sentiment global pondÃ©rÃ©
        sentiments = embeddings[:, 0]  # PremiÃ¨re dimension = sentiment
        
        # Moyenne pondÃ©rÃ©e par l'attention
        sentiment_global = 0
        for i in range(len(mots)):
            # Poids d'attention moyen pour ce mot
            poids_moyen = np.mean(weights[:, i])
            sentiment_global += sentiments[i] * poids_moyen
        
        print(f"\\nğŸ¯ ANALYSE FINALE:")
        print(f"Sentiment global: {sentiment_global:.3f}")
        
        if sentiment_global > 0.3:
            conclusion = "ğŸ˜Š POSITIF"
        elif sentiment_global < -0.3:
            conclusion = "ğŸ˜ NÃ‰GATIF"
        else:
            conclusion = "ğŸ˜ NEUTRE"
        
        print(f"Classification: {conclusion}")
        
        return sentiment_global, weights

# Test sur avis restaurant sÃ©nÃ©galais
analyseur = AnalyseurSentiment()

print("ğŸ½ï¸ ANALYSE DE SENTIMENT - AVIS RESTAURANT")
print("=" * 50)

avis1 = "Le restaurant teranga Ã©tait magnifique et dÃ©licieux"
sentiment1, _ = analyseur.analyser_avec_attention(avis1)

print("\\n" + "="*50)

avis2 = "Service lent et nourriture dÃ©cevante cher"
sentiment2, _ = analyseur.analyser_avec_attention(avis2)

print("\\nğŸ“Š COMPARAISON:")
print(f"Avis 1: {sentiment1:.3f} ({'POSITIF' if sentiment1 > 0 else 'NÃ‰GATIF'})")
print(f"Avis 2: {sentiment2:.3f} ({'POSITIF' if sentiment2 > 0 else 'NÃ‰GATIF'})")`,
          },
          {
            type: "warning",
            icon: "âš ï¸",
            title: "Pourquoi l'attention a rÃ©volutionnÃ© l'IA ?",
            content: `
                        <p><strong>ğŸš€ L'attention a rÃ©solu les limitations fondamentales des architectures prÃ©cÃ©dentes :</strong></p>
                        
                        <p><strong>âŒ ProblÃ¨mes des RNN/LSTM :</strong></p>
                        <ul>
                            <li>ğŸŒ <strong>Traitement sÃ©quentiel</strong> : impossible de parallÃ©liser</li>
                            <li>ğŸ§  <strong>MÃ©moire limitÃ©e</strong> : oubli des informations lointaines</li>
                            <li>â° <strong>DÃ©pendances longues</strong> : difficile de relier des Ã©lÃ©ments Ã©loignÃ©s</li>
                            <li>ğŸ”„ <strong>Gradient qui disparaÃ®t</strong> : mÃªme avec LSTM</li>
                        </ul>
                        
                        <p><strong>âœ… Solutions apportÃ©es par l'attention :</strong></p>
                        <ul>
                            <li>âš¡ <strong>ParallÃ©lisation totale</strong> : tous les mots traitÃ©s simultanÃ©ment</li>
                            <li>ğŸ§  <strong>MÃ©moire illimitÃ©e</strong> : accÃ¨s direct Ã  toute la sÃ©quence</li>
                            <li>ğŸ”— <strong>Relations directes</strong> : connexion immÃ©diate entre mots Ã©loignÃ©s</li>
                            <li>ğŸ“ˆ <strong>Gradients stables</strong> : chemins directs pour la rÃ©tropropagation</li>
                        </ul>
                        
                        <p><strong>ğŸŒŸ RÃ©volutions concrÃ¨tes :</strong></p>
                        <ul>
                            <li>ğŸ’¬ <strong>ChatGPT</strong> : conversations cohÃ©rentes sur des milliers de mots</li>
                            <li>ğŸŒ <strong>Traduction</strong> : Google Translate 10x plus prÃ©cis</li>
                            <li>ğŸ“ <strong>GÃ©nÃ©ration de texte</strong> : articles, code, poÃ©sie cohÃ©rents</li>
                            <li>ğŸ” <strong>ComprÃ©hension</strong> : rÃ©ponse Ã  des questions complexes</li>
                            <li>ğŸ¨ <strong>CrÃ©ativitÃ©</strong> : gÃ©nÃ©ration d'images (DALL-E), musique</li>
                        </ul>
                        
                        <p><strong>ğŸ“Š Impact quantitatif :</strong></p>
                        <ul>
                            <li>âš¡ <strong>Vitesse</strong> : 100x plus rapide que les RNN</li>
                            <li>ğŸ¯ <strong>PrÃ©cision</strong> : +30% sur les tÃ¢ches de comprÃ©hension</li>
                            <li>ğŸ“ <strong>Contexte</strong> : de 1000 Ã  100 000+ tokens</li>
                            <li>ğŸ§  <strong>ParamÃ¨tres</strong> : de millions Ã  milliards (GPT-4)</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Point clÃ© :</strong> L'attention n'est pas juste une amÃ©lioration technique - c'est un <strong>changement de paradigme</strong> qui a permis Ã  l'IA de vraiment "comprendre" le langage comme les humains.</p>
                        
                        <p><strong>ğŸ”® Prochaine Ã©tape :</strong> Transformers - l'architecture complÃ¨te qui utilise l'attention pour crÃ©er l'IA moderne !</p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : analyse d'attention sur phrase complexe",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Analysez cette phrase complexe avec self-attention :</p>
                        <p><em>"Le prÃ©sident que les SÃ©nÃ©galais ont Ã©lu gouverne avec sagesse."</em></p>
                        
                        <p><strong>ğŸ“ Questions :</strong></p>
                        <ol>
                            <li>Identifiez les relations que l'attention devrait capturer</li>
                            <li>PrÃ©disez les poids d'attention Ã©levÃ©s</li>
                            <li>Expliquez comment l'attention rÃ©sout l'ambiguÃ¯tÃ© de "que"</li>
                            <li>Comparez avec l'approche RNN sÃ©quentielle</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('complex-attention-analysis')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="complex-attention-analysis" style="display: none;">
                        <ol>
                            <li><strong>Relations importantes :</strong><br>
                                â€¢ "prÃ©sident" â†” "Ã©lu" (qui a Ã©tÃ© Ã©lu ?)<br>
                                â€¢ "que" â†” "prÃ©sident" (rÃ©fÃ©rence du pronom relatif)<br>
                                â€¢ "SÃ©nÃ©galais" â†” "ont Ã©lu" (qui a Ã©lu ?)<br>
                                â€¢ "gouverne" â†” "prÃ©sident" (qui gouverne ?)<br>
                                â€¢ "sagesse" â†” "gouverne" (comment gouverne-t-il ?)</li>
                            <li><strong>Poids Ã©levÃ©s attendus :</strong><br>
                                â€¢ "que" â†’ "prÃ©sident" (0.8+)<br>
                                â€¢ "ont" â†’ "SÃ©nÃ©galais" (0.7+)<br>
                                â€¢ "Ã©lu" â†’ "prÃ©sident" (0.7+)<br>
                                â€¢ "gouverne" â†’ "prÃ©sident" (0.8+)</li>
                            <li><strong>RÃ©solution d'ambiguÃ¯tÃ© :</strong><br>
                                L'attention permet Ã  "que" de regarder directement "prÃ©sident" mÃªme s'ils sont sÃ©parÃ©s par d'autres mots. Plus besoin de mÃ©moriser sÃ©quentiellement !</li>
                            <li><strong>Avantage vs RNN :</strong><br>
                                â€¢ RNN : doit mÃ©moriser "prÃ©sident" pendant 6 Ã©tapes<br>
                                â€¢ Attention : connexion directe instantanÃ©e<br>
                                â€¢ RÃ©sultat : comprÃ©hension plus fiable et rapide</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Types d'attention et leurs applications",
            content: `
                        <p><strong>ğŸ¯ Il existe plusieurs variantes d'attention, chacune optimisÃ©e pour des tÃ¢ches spÃ©cifiques :</strong></p>
                        
                        <p><strong>1ï¸âƒ£ Self-Attention :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>Principe</strong> : Q = K = V (mÃªme sÃ©quence)</li>
                            <li>ğŸ¯ <strong>Usage</strong> : comprÃ©hension de texte, encodage</li>
                            <li>ğŸ’¬ <strong>Exemple</strong> : BERT pour comprendre le contexte</li>
                        </ul>
                        
                        <p><strong>2ï¸âƒ£ Cross-Attention :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>Principe</strong> : Q â‰  K = V (sÃ©quences diffÃ©rentes)</li>
                            <li>ğŸ¯ <strong>Usage</strong> : traduction, rÃ©sumÃ©, question-rÃ©ponse</li>
                            <li>ğŸŒ <strong>Exemple</strong> : traduction Wolof â†’ FranÃ§ais</li>
                        </ul>
                        
                        <p><strong>3ï¸âƒ£ Masked Self-Attention :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>Principe</strong> : masquer les mots futurs</li>
                            <li>ğŸ¯ <strong>Usage</strong> : gÃ©nÃ©ration de texte</li>
                            <li>ğŸ’¬ <strong>Exemple</strong> : GPT pour prÃ©dire le mot suivant</li>
                        </ul>
                        
                        <p><strong>4ï¸âƒ£ Multi-Head Attention :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>Principe</strong> : plusieurs tÃªtes spÃ©cialisÃ©es</li>
                            <li>ğŸ¯ <strong>Usage</strong> : capture multiple aspects</li>
                            <li>ğŸ§  <strong>Exemple</strong> : syntaxe + sÃ©mantique + pragmatique</li>
                        </ul>
                        
                        <p><strong>ğŸ”® Ã‰volutions rÃ©centes :</strong></p>
                        <ul>
                            <li>âš¡ <strong>Flash Attention</strong> : optimisation mÃ©moire</li>
                            <li>ğŸ¯ <strong>Sparse Attention</strong> : attention sÃ©lective</li>
                            <li>ğŸ”„ <strong>Rotary Position Embedding</strong> : encodage positionnel amÃ©liorÃ©</li>
                            <li>ğŸ§  <strong>Group Query Attention</strong> : efficacitÃ© pour gros modÃ¨les</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : conception d'attention spÃ©cialisÃ©e",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Pour chaque tÃ¢che, concevez le type d'attention optimal :</p>
                        
                        <ol>
                            <li><strong>RÃ©sumÃ© automatique</strong> : transformer un article long en rÃ©sumÃ© court</li>
                            <li><strong>Chatbot conversationnel</strong> : rÃ©pondre en tenant compte de l'historique</li>
                            <li><strong>Correction grammaticale</strong> : corriger les erreurs dans un texte</li>
                            <li><strong>GÃ©nÃ©ration de code</strong> : Ã©crire du Python Ã  partir d'une description</li>
                            <li><strong>Question-rÃ©ponse</strong> : rÃ©pondre Ã  une question sur un texte</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('attention-design-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="attention-design-exercise" style="display: none;">
                        <ol>
                            <li><strong>RÃ©sumÃ© automatique :</strong><br>
                                <strong>Type :</strong> Cross-attention<br>
                                <strong>Architecture :</strong> Q = rÃ©sumÃ© en cours, K = V = article complet<br>
                                <strong>Objectif :</strong> Le rÃ©sumÃ© "interroge" l'article pour extraire l'essentiel</li>
                            <li><strong>Chatbot conversationnel :</strong><br>
                                <strong>Type :</strong> Self-attention + Cross-attention<br>
                                <strong>Architecture :</strong> Self-attention sur l'historique + Cross-attention pour la rÃ©ponse<br>
                                <strong>Objectif :</strong> Comprendre le contexte conversationnel complet</li>
                            <li><strong>Correction grammaticale :</strong><br>
                                <strong>Type :</strong> Self-attention<br>
                                <strong>Architecture :</strong> Encoder-decoder avec self-attention<br>
                                <strong>Objectif :</strong> Comprendre les dÃ©pendances grammaticales longues</li>
                            <li><strong>GÃ©nÃ©ration de code :</strong><br>
                                <strong>Type :</strong> Cross-attention + Masked self-attention<br>
                                <strong>Architecture :</strong> Description (K,V) â†’ Code (Q) avec masquage causal<br>
                                <strong>Objectif :</strong> Traduire langage naturel en code structurÃ©</li>
                            <li><strong>Question-rÃ©ponse :</strong><br>
                                <strong>Type :</strong> Cross-attention bidirectionnelle<br>
                                <strong>Architecture :</strong> Question â†” Texte avec attention mutuelle<br>
                                <strong>Objectif :</strong> Localiser la rÃ©ponse dans le texte</li>
                        </ol>
                        </div>
                    `,
          },
        ],
        quiz: {
          question:
            "ğŸ¤” Dans le mÃ©canisme d'attention, que reprÃ©sente la formule QK^T ?",
          options: [
            "A) La sortie finale du mÃ©canisme",
            "B) Les scores de similaritÃ© entre queries et keys",
            "C) Les poids aprÃ¨s softmax",
            "D) L'agrÃ©gation des valeurs",
          ],
          correct: 1,
          explanation:
            "QK^T calcule les scores de similaritÃ© bruts entre chaque query et chaque key. C'est la premiÃ¨re Ã©tape qui dÃ©termine quels Ã©lÃ©ments sont pertinents les uns pour les autres, avant la normalisation et le softmax.",
        },
        prevModule: "../dl/rnn.html",
        nextModule: "transformers.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
