<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Transformers | IA4Ndada</title>

    <!-- MathJax pour les formules mathÃ©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">ğŸ  Accueil</a>
          <span>â€º</span>
          <span>ğŸ’¬ LLMs & IA Moderne</span>
          <span>â€º</span>
          <span>Transformers</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>ğŸ”„ Transformers : L'Architecture qui a RÃ©volutionnÃ© l'IA</h1>
      <p class="subtitle">Module 5.2 - LLMs & IA Moderne</p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>ğŸ¯ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajoutÃ©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajoutÃ© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>ğŸ‰ Checkpoint - Transformers</h3>
        <p>
          FÃ©licitations ! Vous comprenez maintenant l'architecture qui a rendu
          possible ChatGPT et toute l'IA moderne.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme complÃ©tÃ©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="attention.html" class="nav-link" id="prev-link"
          >â† Module prÃ©cÃ©dent : Attention</a
        >
        <a href="gpt-bert.html" class="nav-link" id="next-link"
          >Module suivant : GPT & BERT â†’</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module Transformers
      const moduleConfig = {
        id: "llm-transformers",
        title: "Transformers : L'Architecture qui a RÃ©volutionnÃ© l'IA",
        category: "LLMs & IA Moderne",
        objectives: [
          "Comprendre pourquoi les Transformers ont rÃ©volutionnÃ© l'IA",
          "MaÃ®triser l'architecture Encoder-Decoder complÃ¨te",
          "Calculer manuellement un bloc Transformer simple",
          "Comprendre l'encodage positionnel et la normalisation",
          "ImplÃ©menter un Transformer miniature from scratch",
        ],
        content: [
          {
            type: "concept",
            icon: "ğŸ’¡",
            title:
              "La rÃ©volution Transformer : 2017, l'annÃ©e qui a tout changÃ©",
            content: `
                        <p>En 2017, l'article <strong>"Attention Is All You Need"</strong> de Google a dÃ©clenchÃ© la plus grande rÃ©volution de l'IA moderne. Les <strong>Transformers</strong> ont rendu possible ChatGPT, GPT-4, BERT, et tous les LLMs actuels.</p>
                        
                        <p><strong>ğŸ”‘ RÃ©volution conceptuelle :</strong></p>
                        <ul>
                            <li>âŒ <strong>Avant 2017</strong> : RNN/LSTM sÃ©quentiels, lents, limitÃ©s</li>
                            <li>âœ… <strong>AprÃ¨s 2017</strong> : Transformers parallÃ¨les, rapides, illimitÃ©s</li>
                        </ul>
                        
                        <p><strong>ğŸš€ Impact quantifiÃ© :</strong></p>
                        <ul>
                            <li>âš¡ <strong>Vitesse</strong> : 10x plus rapide Ã  entraÃ®ner</li>
                            <li>ğŸ§  <strong>CapacitÃ©</strong> : de 100M Ã  175B paramÃ¨tres (GPT-3)</li>
                            <li>ğŸ“Š <strong>Performance</strong> : records battus dans tous les domaines</li>
                            <li>ğŸŒ <strong>Applications</strong> : langage, vision, code, science</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Pourquoi rÃ©volutionnaire ?</strong></p>
                        <ul>
                            <li>ğŸ”„ <strong>ParallÃ©lisation</strong> : traite tous les mots simultanÃ©ment</li>
                            <li>ğŸ‘ï¸ <strong>Attention globale</strong> : chaque mot "voit" tous les autres</li>
                            <li>ğŸ“ <strong>Pas de limite</strong> : sÃ©quences de longueur arbitraire</li>
                            <li>ğŸ¯ <strong>Transfert learning</strong> : un modÃ¨le, mille applications</li>
                        </ul>
                        
                        <p><strong>ğŸŒŸ RÃ©sultat :</strong> De la traduction automatique en 2017 Ã  ChatGPT qui rÃ©volutionne le monde en 2023 !</p>
                    `,
          },
          {
            type: "intuition",
            icon: "ğŸ§ ",
            title: "L'analogie de l'assemblÃ©e nationale sÃ©nÃ©galaise",
            content: `
                        <p>Imaginez une <strong>session de l'AssemblÃ©e Nationale du SÃ©nÃ©gal</strong> oÃ¹ 165 dÃ©putÃ©s dÃ©battent d'un projet de loi :</p>
                        
                        <p><strong>ğŸ›ï¸ MÃ©thode traditionnelle (RNN/LSTM) :</strong></p>
                        <ul>
                            <li>ğŸ—£ï¸ <strong>Parole sÃ©quentielle</strong> : un dÃ©putÃ© parle, les autres Ã©coutent</li>
                            <li>â° <strong>MÃ©moire limitÃ©e</strong> : difficile de se rappeler ce qui a Ã©tÃ© dit il y a 2h</li>
                            <li>ğŸŒ <strong>Lenteur</strong> : 165 interventions = 165 Ã©tapes sÃ©quentielles</li>
                            <li>ğŸ“‰ <strong>Perte d'information</strong> : les premiers arguments sont oubliÃ©s</li>
                        </ul>
                        
                        <p><strong>ğŸš€ MÃ©thode Transformer :</strong></p>
                        <ul>
                            <li>ğŸ‘¥ <strong>DÃ©bat simultanÃ©</strong> : tous les dÃ©putÃ©s peuvent "s'Ã©couter" en mÃªme temps</li>
                            <li>ğŸ§  <strong>MÃ©moire parfaite</strong> : chaque dÃ©putÃ© a accÃ¨s Ã  TOUS les arguments prÃ©cÃ©dents</li>
                            <li>âš¡ <strong>ParallÃ©lisme</strong> : traitement simultanÃ© de toutes les interventions</li>
                            <li>ğŸ¯ <strong>Attention sÃ©lective</strong> : chaque dÃ©putÃ© se concentre sur les arguments pertinents</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ RÃ©sultat :</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>DÃ©cisions plus Ã©clairÃ©es</strong> : prise en compte de tous les Ã©lÃ©ments</li>
                            <li>âš¡ <strong>Processus plus rapide</strong> : pas d'attente sÃ©quentielle</li>
                            <li>ğŸ§  <strong>ComprÃ©hension globale</strong> : vision d'ensemble du dÃ©bat</li>
                        </ul>
                        
                        <p><strong>ğŸ”‘ C'est exactement ce que fait un Transformer :</strong> il permet Ã  chaque mot de "dÃ©battre" avec tous les autres mots simultanÃ©ment pour construire une comprÃ©hension globale du texte !</p>
                    `,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title:
              "Architecture Encoder-Decoder : la structure rÃ©volutionnaire",
            content: `
                        <p>L'architecture <strong>Transformer</strong> se compose de deux parties principales qui travaillent ensemble comme un systÃ¨me de traduction parfait.</p>
                        
                        <p><strong>ğŸ” Vue d'ensemble :</strong></p>
                        <ul>
                            <li>ğŸ“¥ <strong>Encoder</strong> : comprend et encode le texte d'entrÃ©e</li>
                            <li>ğŸ“¤ <strong>Decoder</strong> : gÃ©nÃ¨re le texte de sortie mot par mot</li>
                            <li>ğŸ”— <strong>Cross-Attention</strong> : le decoder "regarde" l'encoder</li>
                        </ul>
                        
                        <p><strong>ğŸ—ï¸ Structure dÃ©taillÃ©e :</strong></p>
                        
                        <p><strong>ğŸ“¥ ENCODER (6 couches identiques) :</strong></p>
                        <ol>
                            <li>ğŸ”¤ <strong>Embeddings</strong> + encodage positionnel</li>
                            <li>ğŸ‘ï¸ <strong>Multi-Head Self-Attention</strong></li>
                            <li>â• <strong>Connexion rÃ©siduelle</strong> + normalisation</li>
                            <li>ğŸ§  <strong>Feed-Forward Network</strong></li>
                            <li>â• <strong>Connexion rÃ©siduelle</strong> + normalisation</li>
                        </ol>
                        
                        <p><strong>ğŸ“¤ DECODER (6 couches identiques) :</strong></p>
                        <ol>
                            <li>ğŸ”¤ <strong>Embeddings</strong> + encodage positionnel</li>
                            <li>ğŸ‘ï¸ <strong>Masked Multi-Head Self-Attention</strong></li>
                            <li>â• <strong>Connexion rÃ©siduelle</strong> + normalisation</li>
                            <li>ğŸ”— <strong>Multi-Head Cross-Attention</strong> (vers encoder)</li>
                            <li>â• <strong>Connexion rÃ©siduelle</strong> + normalisation</li>
                            <li>ğŸ§  <strong>Feed-Forward Network</strong></li>
                            <li>â• <strong>Connexion rÃ©siduelle</strong> + normalisation</li>
                            <li>ğŸ“Š <strong>Couche de sortie</strong> (probabilitÃ©s des mots)</li>
                        </ol>
                        
                        <p><strong>ğŸ’¡ Innovations clÃ©s :</strong></p>
                        <ul>
                            <li>ğŸ”„ <strong>Connexions rÃ©siduelles</strong> : Ã©vitent la disparition des gradients</li>
                            <li>ğŸ“Š <strong>Layer Normalization</strong> : stabilise l'entraÃ®nement</li>
                            <li>ğŸ­ <strong>Masquage</strong> : empÃªche de "tricher" en regardant le futur</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Formalisation mathÃ©matique complÃ¨te",
            content: `
                        <p><strong>ğŸ“ Formalisons rigoureusement l'architecture Transformer :</strong></p>
                        
                        <p><strong>ğŸ”¤ 1. Embeddings et encodage positionnel :</strong></p>
                        <p>$$\\vec{x}_i = \\text{Embedding}(\\text{token}_i) + \\text{PE}(i)$$</p>
                        
                        <p><strong>ğŸ‘ï¸ 2. Multi-Head Attention :</strong></p>
                        <p>$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O$$</p>
                        <p>$$\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$$</p>
                        
                        <p><strong>ğŸ§  3. Feed-Forward Network :</strong></p>
                        <p>$$\\text{FFN}(x) = \\max(0, x W_1 + b_1) W_2 + b_2$$</p>
                        
                        <p><strong>â• 4. Connexions rÃ©siduelles :</strong></p>
                        <p>$$\\text{output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$</p>
                        
                        <p><strong>ğŸ—ï¸ 5. Bloc Encoder complet :</strong></p>
                        <p>$$\\vec{z}_1 = \\text{LayerNorm}(\\vec{x} + \\text{MultiHead}(\\vec{x}, \\vec{x}, \\vec{x}))$$</p>
                        <p>$$\\vec{z}_2 = \\text{LayerNorm}(\\vec{z}_1 + \\text{FFN}(\\vec{z}_1))$$</p>
                        
                        <p><strong>ğŸ¯ 6. Bloc Decoder complet :</strong></p>
                        <p>$$\\vec{y}_1 = \\text{LayerNorm}(\\vec{y} + \\text{MaskedMultiHead}(\\vec{y}, \\vec{y}, \\vec{y}))$$</p>
                        <p>$$\\vec{y}_2 = \\text{LayerNorm}(\\vec{y}_1 + \\text{CrossAttention}(\\vec{y}_1, \\vec{z}, \\vec{z}))$$</p>
                        <p>$$\\vec{y}_3 = \\text{LayerNorm}(\\vec{y}_2 + \\text{FFN}(\\vec{y}_2))$$</p>
                        
                        <p><strong>ğŸ” Dimensions typiques :</strong></p>
                        <ul>
                            <li>\\(d_{model} = 512\\) (dimension des embeddings)</li>
                            <li>\\(h = 8\\) (nombre de tÃªtes d'attention)</li>
                            <li>\\(d_k = d_v = 64\\) (dimension par tÃªte)</li>
                            <li>\\(d_{ff} = 2048\\) (dimension feed-forward)</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Calcul manuel : bloc Transformer simple",
            content: `
                        <p><strong>ğŸ“ Exemple concret :</strong> Calculons un bloc Transformer sur 2 mots</p>
                        
                        <p><strong>ğŸ”¤ EntrÃ©e :</strong> "Dakar belle" â†’ 2 tokens</p>
                        <p>$$X = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.5 & 0.6 & 0.7 & 0.8 \\end{bmatrix}_{2 \\times 4}$$</p>
                        <p>(2 mots Ã— 4 dimensions d'embedding)</p>
                        
                        <p><strong>ğŸ‘ï¸ Ã‰tape 1 : Self-Attention (1 tÃªte) :</strong></p>
                        <p>Matrices de projection (simplifiÃ©es) :</p>
                        <p>$$W^Q = W^K = W^V = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}$$</p>
                        
                        <p>Donc \\(Q = K = V = X\\)</p>
                        
                        <p><strong>ğŸ”¢ Calcul des scores d'attention :</strong></p>
                        <p>$$\\text{Scores} = \\frac{QK^T}{\\sqrt{d_k}} = \\frac{XX^T}{\\sqrt{4}}$$</p>
                        
                        <p>$$QK^T = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.5 & 0.6 & 0.7 & 0.8 \\end{bmatrix} \\begin{bmatrix} 0.1 & 0.5 \\\\ 0.2 & 0.6 \\\\ 0.3 & 0.7 \\\\ 0.4 & 0.8 \\end{bmatrix}$$</p>
                        
                        <p>$$= \\begin{bmatrix} 0.01+0.04+0.09+0.16 & 0.05+0.12+0.21+0.32 \\\\ 0.05+0.12+0.21+0.32 & 0.25+0.36+0.49+0.64 \\end{bmatrix} = \\begin{bmatrix} 0.30 & 0.70 \\\\ 0.70 & 1.74 \\end{bmatrix}$$</p>
                        
                        <p>$$\\text{Scores} = \\frac{1}{2} \\begin{bmatrix} 0.30 & 0.70 \\\\ 0.70 & 1.74 \\end{bmatrix} = \\begin{bmatrix} 0.15 & 0.35 \\\\ 0.35 & 0.87 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ“Š Softmax par ligne :</strong></p>
                        <p>Ligne 1 : \\(e^{0.15} = 1.16, e^{0.35} = 1.42\\) â†’ \\([0.45, 0.55]\\)</p>
                        <p>Ligne 2 : \\(e^{0.35} = 1.42, e^{0.87} = 2.39\\) â†’ \\([0.37, 0.63]\\)</p>
                        
                        <p>$$\\text{Attention} = \\begin{bmatrix} 0.45 & 0.55 \\\\ 0.37 & 0.63 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ¯ Sortie attention :</strong></p>
                        <p>$$\\text{Output} = \\text{Attention} \\times V = \\begin{bmatrix} 0.45 & 0.55 \\\\ 0.37 & 0.63 \\end{bmatrix} \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.5 & 0.6 & 0.7 & 0.8 \\end{bmatrix}$$</p>
                        
                        <p>$$= \\begin{bmatrix} 0.32 & 0.42 & 0.52 & 0.62 \\\\ 0.35 & 0.45 & 0.55 & 0.65 \\end{bmatrix}$$</p>
                    `,
          },
          {
            type: "code",
            title: "ImplÃ©mentation Transformer from scratch",
            description: "CrÃ©ons un Transformer miniature complet :",
            code: `import numpy as np

class TransformerBlock:
    def __init__(self, d_model=4, n_heads=1, d_ff=8):
        """
        Bloc Transformer simplifiÃ©
        d_model: dimension des embeddings
        n_heads: nombre de tÃªtes d'attention  
        d_ff: dimension feed-forward
        """
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.d_ff = d_ff
        
        # Matrices d'attention (simplifiÃ©es Ã  l'identitÃ©)
        self.W_q = np.eye(d_model)
        self.W_k = np.eye(d_model) 
        self.W_v = np.eye(d_model)
        self.W_o = np.eye(d_model)
        
        # Feed-forward network
        self.W1 = np.random.randn(d_model, d_ff) * 0.1
        self.b1 = np.zeros(d_ff)
        self.W2 = np.random.randn(d_ff, d_model) * 0.1
        self.b2 = np.zeros(d_model)
        
        print(f"ğŸ§  Transformer Block crÃ©Ã©:")
        print(f"   d_model={d_model}, n_heads={n_heads}, d_ff={d_ff}")
    
    def layer_norm(self, x, epsilon=1e-6):
        """Layer Normalization"""
        mean = np.mean(x, axis=-1, keepdims=True)
        std = np.std(x, axis=-1, keepdims=True)
        return (x - mean) / (std + epsilon)
    
    def softmax(self, x, axis=-1):
        """Softmax stable"""
        exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)
    
    def attention(self, Q, K, V, mask=None):
        """MÃ©canisme d'attention"""
        # Calcul des scores
        scores = np.matmul(Q, K.transpose()) / np.sqrt(self.d_k)
        
        # Application du masque si fourni
        if mask is not None:
            scores = np.where(mask == 0, -1e9, scores)
        
        # Softmax
        attention_weights = self.softmax(scores)
        
        # Application aux valeurs
        output = np.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def feed_forward(self, x):
        """RÃ©seau feed-forward avec ReLU"""
        # PremiÃ¨re transformation linÃ©aire + ReLU
        hidden = np.maximum(0, np.matmul(x, self.W1) + self.b1)
        
        # DeuxiÃ¨me transformation linÃ©aire
        output = np.matmul(hidden, self.W2) + self.b2
        
        return output
    
    def forward(self, x):
        """Propagation avant complÃ¨te"""
        print(f"\\nğŸ”„ PROPAGATION AVANT")
        print(f"EntrÃ©e shape: {x.shape}")
        
        # 1. Self-Attention
        Q = np.matmul(x, self.W_q)
        K = np.matmul(x, self.W_k) 
        V = np.matmul(x, self.W_v)
        
        attn_output, attn_weights = self.attention(Q, K, V)
        print(f"Attention weights:")
        print(attn_weights.round(3))
        
        # 2. Connexion rÃ©siduelle + Layer Norm
        x1 = self.layer_norm(x + attn_output)
        print(f"AprÃ¨s attention + rÃ©siduelle: {x1.round(3)}")
        
        # 3. Feed-Forward
        ff_output = self.feed_forward(x1)
        
        # 4. Connexion rÃ©siduelle + Layer Norm
        x2 = self.layer_norm(x1 + ff_output)
        print(f"Sortie finale: {x2.round(3)}")
        
        return x2, attn_weights

# Test du Transformer Block
print("ğŸš€ TEST TRANSFORMER BLOCK")
print("=" * 40)

# DonnÃ©es d'entrÃ©e : 2 mots, 4 dimensions
X = np.array([
    [0.1, 0.2, 0.3, 0.4],  # "Dakar"
    [0.5, 0.6, 0.7, 0.8]   # "belle"
])

transformer = TransformerBlock(d_model=4, n_heads=1, d_ff=8)
output, attention = transformer.forward(X)

print(f"\\nğŸ“Š RÃ‰SUMÃ‰")
print(f"EntrÃ©e: {X.shape}")
print(f"Sortie: {output.shape}")
print(f"Attention: chaque mot regarde tous les mots")`,
          },
          {
            type: "code",
            title: "Encodage positionnel sinusoÃ¯dal",
            description:
              "ImplÃ©mentons l'encodage positionnel rÃ©volutionnaire :",
            code: `def encodage_positionnel(seq_len, d_model):
    """
    Encodage positionnel sinusoÃ¯dal du paper original
    Permet au modÃ¨le de comprendre l'ordre des mots
    """
    PE = np.zeros((seq_len, d_model))
    
    # Positions
    position = np.arange(seq_len).reshape(-1, 1)
    
    # FrÃ©quences
    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    
    # Application des fonctions sin/cos
    PE[:, 0::2] = np.sin(position * div_term)  # Indices pairs
    PE[:, 1::2] = np.cos(position * div_term)  # Indices impairs
    
    return PE

# Test de l'encodage positionnel
print("ğŸ“ ENCODAGE POSITIONNEL")
print("=" * 30)

seq_length = 5
d_model = 8
PE = encodage_positionnel(seq_length, d_model)

print(f"SÃ©quence de {seq_length} mots, {d_model} dimensions:")
print("Position | Encodage positionnel")
print("-" * 40)
for i in range(seq_length):
    encodage_str = " ".join([f"{x:6.3f}" for x in PE[i]])
    print(f"   {i}     | {encodage_str}")

# Visualisation des patterns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
plt.imshow(PE.T, cmap='RdYlBu', aspect='auto')
plt.colorbar(label='Valeur')
plt.xlabel('Position dans la sÃ©quence')
plt.ylabel('Dimension d\\'embedding')
plt.title('Encodage Positionnel SinusoÃ¯dal')
plt.show()

print("\\nğŸ’¡ Observation: Chaque position a un 'code' unique !")
print("   Les patterns sinusoÃ¯daux permettent au modÃ¨le")
print("   de comprendre les relations de distance entre mots.")`,
          },
          {
            type: "code",
            title: "Transformer complet pour traduction",
            description:
              "Assemblons un Transformer complet pour la traduction :",
            code: `class TransformerSimple:
    def __init__(self, vocab_size=100, d_model=8, n_heads=2, n_layers=2):
        """Transformer simplifiÃ© pour traduction"""
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.n_heads = n_heads
        self.n_layers = n_layers
        
        # Embeddings (simulÃ©s)
        self.embedding = np.random.randn(vocab_size, d_model) * 0.1
        
        # Couches Transformer
        self.encoder_layers = [TransformerBlock(d_model, n_heads) 
                              for _ in range(n_layers)]
        self.decoder_layers = [TransformerBlock(d_model, n_heads) 
                              for _ in range(n_layers)]
        
        # Couche de sortie
        self.output_projection = np.random.randn(d_model, vocab_size) * 0.1
        
        print(f"ğŸŒ Transformer Traduction crÃ©Ã©:")
        print(f"   Vocabulaire: {vocab_size} mots")
        print(f"   ModÃ¨le: {d_model}D, {n_heads} tÃªtes, {n_layers} couches")
    
    def encode(self, input_ids):
        """Encoder une sÃ©quence d'entrÃ©e"""
        # Embeddings + encodage positionnel
        seq_len = len(input_ids)
        x = self.embedding[input_ids]  # Lookup embeddings
        x += encodage_positionnel(seq_len, self.d_model)
        
        print(f"\\nğŸ“¥ ENCODAGE")
        print(f"Tokens: {input_ids}")
        print(f"Embeddings shape: {x.shape}")
        
        # Passage dans les couches encoder
        for i, layer in enumerate(self.encoder_layers):
            print(f"\\n--- Couche Encoder {i+1} ---")
            x, _ = layer.forward(x)
        
        return x
    
    def decode_step(self, decoder_input, encoder_output):
        """Un pas de dÃ©codage"""
        # Embeddings + encodage positionnel
        seq_len = len(decoder_input)
        x = self.embedding[decoder_input]
        x += encodage_positionnel(seq_len, self.d_model)
        
        print(f"\\nğŸ“¤ DÃ‰CODAGE")
        print(f"Tokens decoder: {decoder_input}")
        
        # Passage dans les couches decoder (simplifiÃ©)
        for i, layer in enumerate(self.decoder_layers):
            print(f"\\n--- Couche Decoder {i+1} ---")
            x, _ = layer.forward(x)
        
        # Projection vers vocabulaire
        logits = np.matmul(x[-1], self.output_projection)  # Dernier token
        probas = self.softmax(logits)
        
        return probas
    
    def softmax(self, x):
        """Softmax stable"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)

# Simulation de traduction Wolof â†’ FranÃ§ais
print("ğŸŒ SIMULATION TRADUCTION WOLOF â†’ FRANÃ‡AIS")
print("=" * 50)

# Vocabulaires simplifiÃ©s
vocab_wolof = ["<pad>", "<start>", "<end>", "Dakar", "mag", "na", "rafet"]
vocab_francais = ["<pad>", "<start>", "<end>", "Dakar", "est", "trÃ¨s", "belle"]

# Phrase: "Dakar mag na rafet" â†’ "Dakar est trÃ¨s belle"
phrase_wolof = [1, 3, 4, 5, 6]  # <start> Dakar mag na rafet
phrase_francais = [1, 3, 4, 5, 6]  # <start> Dakar est trÃ¨s belle

# CrÃ©ation du modÃ¨le
transformer = TransformerSimple(vocab_size=len(vocab_wolof), 
                               d_model=8, n_heads=2, n_layers=1)

# Encodage de la phrase wolof
encoder_output = transformer.encode(phrase_wolof)

# DÃ©codage (simulation d'un pas)
decoder_input = [1, 3]  # <start> Dakar
probas = transformer.decode_step(decoder_input, encoder_output)

print(f"\\nğŸ¯ PRÃ‰DICTION DU MOT SUIVANT")
print(f"Contexte: {[vocab_francais[i] for i in decoder_input]}")
print(f"ProbabilitÃ©s:")
for i, (mot, proba) in enumerate(zip(vocab_francais, probas)):
    if proba > 0.05:  # Afficher seulement les probas significatives
        print(f"   {mot}: {proba:.3f}")`,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice pratique : calcul manuel Transformer",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Calculez manuellement un bloc Transformer sur 3 mots avec 1 tÃªte d'attention.</p>
                        
                        <p><strong>ğŸ“Š DonnÃ©es :</strong></p>
                        <p>Phrase : "SÃ©nÃ©gal est beau"</p>
                        <p>$$X = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}_{3 \\times 2}$$</p>
                        
                        <p><strong>âš™ï¸ ParamÃ¨tres :</strong></p>
                        <p>$$W^Q = W^K = W^V = I_2 \\quad \\text{(matrices identitÃ©)}$$</p>
                        
                        <p><strong>ğŸ“ Calculez :</strong></p>
                        <ol>
                            <li>Les matrices Q, K, V</li>
                            <li>Les scores d'attention \\(QK^T / \\sqrt{2}\\)</li>
                            <li>Les poids d'attention (softmax par ligne)</li>
                            <li>La sortie de l'attention</li>
                            <li>InterprÃ©tez : quel mot "fait attention" Ã  quels autres mots ?</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('transformer-manual-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="transformer-manual-exercise" style="display: none;">
                        <ol>
                            <li><strong>Q, K, V :</strong> Comme W = I, on a Q = K = V = X</li>
                            <li><strong>Scores :</strong><br>
                                \\(QK^T = XX^T = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix}\\)<br>
                                \\(\\text{Scores} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix} = \\begin{bmatrix} 0.71 & 0 & 0.71 \\\\ 0 & 0.71 & 0.71 \\\\ 0.71 & 0.71 & 1.41 \\end{bmatrix}\\)</li>
                            <li><strong>Poids attention (softmax) :</strong><br>
                                Ligne 1: [0.5, 0.25, 0.5] (normalisÃ©)<br>
                                Ligne 2: [0.25, 0.5, 0.5] (normalisÃ©)<br>
                                Ligne 3: [0.33, 0.33, 0.67] (normalisÃ©)</li>
                            <li><strong>Sortie :</strong> Combinaison pondÃ©rÃ©e des vecteurs V</li>
                            <li><strong>InterprÃ©tation :</strong><br>
                                - "SÃ©nÃ©gal" fait attention Ã  lui-mÃªme et "beau" (similaritÃ©)<br>
                                - "est" fait attention surtout Ã  lui-mÃªme et "beau"<br>
                                - "beau" fait le plus attention Ã  lui-mÃªme (mot le plus riche)</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Masquage causal : empÃªcher de tricher",
            content: `
                        <p><strong>ğŸ¤” Pourquoi le masquage est-il crucial ?</strong></p>
                        <p>Dans la gÃ©nÃ©ration de texte, le modÃ¨le ne doit pas "voir le futur" ! Sinon, il tricherait en connaissant dÃ©jÃ  la rÃ©ponse.</p>
                        
                        <p><strong>ğŸ¯ ProblÃ¨me concret :</strong></p>
                        <p>GÃ©nÃ©rer : "Le SÃ©nÃ©gal est un pays ___"</p>
                        <ul>
                            <li>âŒ <strong>Sans masque</strong> : le modÃ¨le voit dÃ©jÃ  "magnifique" et triche</li>
                            <li>âœ… <strong>Avec masque</strong> : le modÃ¨le ne voit que "Le SÃ©nÃ©gal est un pays" et doit vraiment prÃ©dire</li>
                        </ul>
                        
                        <p><strong>ğŸ­ Masque causal (triangulaire infÃ©rieur) :</strong></p>
                        <p>$$\\text{Mask} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 1 & 1 & 1 & 0 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ” InterprÃ©tation :</strong></p>
                        <ul>
                            <li>Position 0 : ne voit que lui-mÃªme</li>
                            <li>Position 1 : voit positions 0 et 1</li>
                            <li>Position 2 : voit positions 0, 1 et 2</li>
                            <li>Position 3 : voit toutes les positions prÃ©cÃ©dentes</li>
                        </ul>
                        
                        <p><strong>âš™ï¸ Application technique :</strong></p>
                        <p>$$\\text{Scores masquÃ©s} = \\begin{cases} \\text{score}_{ij} & \\text{si mask}_{ij} = 1 \\\\ -\\infty & \\text{si mask}_{ij} = 0 \\end{cases}$$</p>
                        
                        <p><strong>ğŸ’¡ RÃ©sultat :</strong> AprÃ¨s softmax, les positions futures ont une probabilitÃ© de 0.</p>
                    `,
          },
          {
            type: "code",
            title: "ImplÃ©mentation du masquage causal",
            description: "CrÃ©ons un masque causal et testons son effet :",
            code: `def creer_masque_causal(seq_len):
    """CrÃ©e un masque triangulaire infÃ©rieur"""
    mask = np.tril(np.ones((seq_len, seq_len)))
    return mask

def attention_avec_masque(Q, K, V, mask=None):
    """Attention avec masquage causal"""
    # Calcul des scores
    scores = np.matmul(Q, K.transpose()) / np.sqrt(Q.shape[-1])
    
    print(f"Scores avant masque:")
    print(scores.round(3))
    
    # Application du masque
    if mask is not None:
        scores_masques = np.where(mask == 0, -1e9, scores)
        print(f"\\nScores aprÃ¨s masque:")
        print(scores_masques.round(3))
    else:
        scores_masques = scores
    
    # Softmax
    attention_weights = np.exp(scores_masques - np.max(scores_masques, axis=-1, keepdims=True))
    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)
    
    print(f"\\nPoids d'attention finaux:")
    print(attention_weights.round(3))
    
    # Application aux valeurs
    output = np.matmul(attention_weights, V)
    
    return output, attention_weights

# Test avec et sans masque
print("ğŸ­ TEST DU MASQUAGE CAUSAL")
print("=" * 35)

# DonnÃ©es : 4 mots
X_test = np.array([
    [1, 0, 0],  # "Le"
    [0, 1, 0],  # "SÃ©nÃ©gal" 
    [0, 0, 1],  # "est"
    [1, 1, 1]   # "beau"
])

Q = K = V = X_test
seq_len = len(X_test)

print("Phrase: 'Le SÃ©nÃ©gal est beau'")
print(f"DonnÃ©es shape: {X_test.shape}")

# Sans masque (peut voir le futur)
print("\\nğŸš« SANS MASQUE (peut tricher):")
output_sans, attn_sans = attention_avec_masque(Q, K, V, mask=None)

# Avec masque causal
print("\\nâœ… AVEC MASQUE CAUSAL (pas de triche):")
masque = creer_masque_causal(seq_len)
print(f"Masque causal:")
print(masque.astype(int))

output_avec, attn_avec = attention_avec_masque(Q, K, V, mask=masque)

print("\\nğŸ’¡ Observation:")
print("   Avec masque: chaque mot ne voit que les prÃ©cÃ©dents")
print("   â†’ GÃ©nÃ©ration de texte honnÃªte !")`,
          },
          {
            type: "code",
            title: "GÃ©nÃ©ration de texte avec Transformer",
            description: "Simulons la gÃ©nÃ©ration de texte autoregressive :",
            code: `def generer_texte_transformer(transformer, prompt_ids, vocab, max_length=10):
    """
    GÃ©nÃ©ration de texte autoregressive
    Le modÃ¨le gÃ©nÃ¨re un mot Ã  la fois, en utilisant tous les mots prÃ©cÃ©dents
    """
    print(f"ğŸ¤– GÃ‰NÃ‰RATION DE TEXTE AUTOREGRESSIVE")
    print("=" * 45)
    
    # Commencer avec le prompt
    sequence_generee = prompt_ids.copy()
    
    print(f"ğŸ’­ Prompt initial: {[vocab[i] for i in prompt_ids]}")
    
    for step in range(max_length):
        print(f"\\n--- Ã‰tape {step + 1} ---")
        
        # Encoder la sÃ©quence actuelle
        encoder_output = transformer.encode(sequence_generee)
        
        # PrÃ©dire le prochain mot (simulation)
        # En rÃ©alitÃ©, on passerait par le decoder avec masque causal
        derniere_representation = encoder_output[-1]
        logits = np.matmul(derniere_representation, transformer.output_projection)
        probas = transformer.softmax(logits)
        
        # SÃ©lection du mot le plus probable (greedy)
        next_token = np.argmax(probas)
        
        # Affichage des top 3 candidats
        top_indices = np.argsort(probas)[-3:][::-1]
        print(f"Top 3 candidats:")
        for i, idx in enumerate(top_indices):
            if idx < len(vocab):
                print(f"   {i+1}. {vocab[idx]}: {probas[idx]:.3f}")
        
        # Ajouter Ã  la sÃ©quence
        if next_token < len(vocab):
            sequence_generee.append(next_token)
            print(f"âœ… Mot choisi: {vocab[next_token]}")
            print(f"ğŸ“ SÃ©quence: {[vocab[i] for i in sequence_generee]}")
        
        # ArrÃªt si token de fin
        if next_token == 2:  # <end>
            print("ğŸ GÃ©nÃ©ration terminÃ©e (token <end>)")
            break
    
    return sequence_generee

# Test de gÃ©nÃ©ration
vocab_test = ["<pad>", "<start>", "<end>", "Dakar", "est", "une", "ville", "magnifique", "du", "SÃ©nÃ©gal"]
prompt = [1, 3]  # "<start> Dakar"

# Simulation (avec un transformer trÃ¨s simple)
transformer_gen = TransformerSimple(vocab_size=len(vocab_test), d_model=6, n_heads=1, n_layers=1)

print("ğŸ¯ Test de gÃ©nÃ©ration:")
sequence_finale = generer_texte_transformer(transformer_gen, prompt, vocab_test, max_length=5)

print(f"\\nğŸ‰ RÃ‰SULTAT FINAL:")
print(f"Texte gÃ©nÃ©rÃ©: {' '.join([vocab_test[i] for i in sequence_finale])}")`,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Innovations clÃ©s des Transformers",
            content: `
                        <p><strong>ğŸš€ Les Transformers introduisent 6 innovations rÃ©volutionnaires :</strong></p>
                        
                        <p><strong>1ï¸âƒ£ Attention Is All You Need :</strong></p>
                        <ul>
                            <li>âŒ <strong>Fini les RNN/LSTM</strong> : plus de traitement sÃ©quentiel</li>
                            <li>âœ… <strong>Attention pure</strong> : mÃ©canisme unique et puissant</li>
                            <li>âš¡ <strong>ParallÃ©lisation</strong> : tous les mots traitÃ©s simultanÃ©ment</li>
                        </ul>
                        
                        <p><strong>2ï¸âƒ£ Multi-Head Attention :</strong></p>
                        <ul>
                            <li>ğŸ‘ï¸ <strong>Plusieurs perspectives</strong> : 8-16 tÃªtes diffÃ©rentes</li>
                            <li>ğŸ¯ <strong>SpÃ©cialisation</strong> : syntaxe, sÃ©mantique, relations</li>
                            <li>ğŸ§  <strong>Richesse</strong> : capture des nuances complexes</li>
                        </ul>
                        
                        <p><strong>3ï¸âƒ£ Encodage positionnel :</strong></p>
                        <ul>
                            <li>ğŸ“ <strong>Ordre prÃ©servÃ©</strong> : sans rÃ©currence</li>
                            <li>ğŸŒŠ <strong>SinusoÃ¯des</strong> : patterns mathÃ©matiques Ã©lÃ©gants</li>
                            <li>â™¾ï¸ <strong>Extrapolation</strong> : fonctionne sur sÃ©quences plus longues</li>
                        </ul>
                        
                        <p><strong>4ï¸âƒ£ Connexions rÃ©siduelles :</strong></p>
                        <ul>
                            <li>ğŸ›¤ï¸ <strong>Autoroutes d'information</strong> : gradients fluides</li>
                            <li>ğŸ§  <strong>RÃ©seaux profonds</strong> : 12-96 couches possibles</li>
                            <li>âš¡ <strong>EntraÃ®nement stable</strong> : pas de disparition</li>
                        </ul>
                        
                        <p><strong>5ï¸âƒ£ Layer Normalization :</strong></p>
                        <ul>
                            <li>ğŸ“Š <strong>StabilitÃ©</strong> : normalise les activations</li>
                            <li>ğŸ¯ <strong>Convergence</strong> : entraÃ®nement plus rapide</li>
                            <li>ğŸ”§ <strong>Robustesse</strong> : moins sensible Ã  l'initialisation</li>
                        </ul>
                        
                        <p><strong>6ï¸âƒ£ Architecture modulaire :</strong></p>
                        <ul>
                            <li>ğŸ§© <strong>Blocs identiques</strong> : facilite la mise Ã  l'Ã©chelle</li>
                            <li>ğŸ”„ <strong>RÃ©utilisabilitÃ©</strong> : mÃªme bloc pour encoder/decoder</li>
                            <li>ğŸ›ï¸ <strong>FlexibilitÃ©</strong> : adaptation Ã  diffÃ©rents problÃ¨mes</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Layer Normalization : stabilisation mathÃ©matique",
            content: `
                        <p><strong>ğŸ“Š Layer Normalization</strong> normalise les activations pour stabiliser l'entraÃ®nement des rÃ©seaux profonds.</p>
                        
                        <p><strong>ğŸ“ Formule mathÃ©matique :</strong></p>
                        <p>Pour un vecteur \\(\\vec{x} \\in \\mathbb{R}^d\\) :</p>
                        <p>$$\\text{LayerNorm}(\\vec{x}) = \\gamma \\odot \\frac{\\vec{x} - \\mu}{\\sigma} + \\beta$$</p>
                        
                        <p><strong>ğŸ” Composants :</strong></p>
                        <ul>
                            <li>\\(\\mu = \\frac{1}{d} \\sum_{i=1}^d x_i\\) = <strong>moyenne</strong> du vecteur</li>
                            <li>\\(\\sigma = \\sqrt{\\frac{1}{d} \\sum_{i=1}^d (x_i - \\mu)^2}\\) = <strong>Ã©cart-type</strong></li>
                            <li>\\(\\gamma, \\beta \\in \\mathbb{R}^d\\) = <strong>paramÃ¨tres apprenables</strong></li>
                            <li>\\(\\odot\\) = <strong>produit Ã©lÃ©ment par Ã©lÃ©ment</strong></li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Effet de la normalisation :</strong></p>
                        <ul>
                            <li>ğŸ“Š <strong>Centrage</strong> : moyenne = 0</li>
                            <li>ğŸ“ <strong>Mise Ã  l'Ã©chelle</strong> : Ã©cart-type = 1</li>
                            <li>ğŸ›ï¸ <strong>RÃ©ajustement</strong> : Î³ et Î² permettent de rÃ©cupÃ©rer l'expressivitÃ©</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ DiffÃ©rence avec Batch Normalization :</strong></p>
                        <ul>
                            <li><strong>Batch Norm</strong> : normalise sur le batch (axe des exemples)</li>
                            <li><strong>Layer Norm</strong> : normalise sur les features (axe des dimensions)</li>
                            <li><strong>Avantage</strong> : indÃ©pendant de la taille du batch</li>
                        </ul>
                        
                        <p><strong>ğŸ”§ Pourquoi crucial dans les Transformers ?</strong></p>
                        <ul>
                            <li>ğŸ§  <strong>RÃ©seaux profonds</strong> : 12+ couches sans problÃ¨me</li>
                            <li>âš¡ <strong>Gradients stables</strong> : pas d'explosion/disparition</li>
                            <li>ğŸ¯ <strong>Convergence rapide</strong> : entraÃ®nement plus efficace</li>
                        </ul>
                    `,
          },
          {
            type: "code",
            title: "ImplÃ©mentation Layer Normalization",
            description: "ImplÃ©mentons et testons la Layer Normalization :",
            code: `class LayerNormalization:
    def __init__(self, d_model, epsilon=1e-6):
        """
        Layer Normalization pour Transformers
        d_model: dimension des embeddings
        epsilon: terme de stabilitÃ© numÃ©rique
        """
        self.d_model = d_model
        self.epsilon = epsilon
        
        # ParamÃ¨tres apprenables (initialisÃ©s)
        self.gamma = np.ones(d_model)  # Facteur d'Ã©chelle
        self.beta = np.zeros(d_model)  # DÃ©calage
        
        print(f"ğŸ“Š LayerNorm crÃ©Ã©e: d_model={d_model}")
    
    def forward(self, x):
        """Normalisation avant"""
        # Calcul des statistiques par vecteur
        mean = np.mean(x, axis=-1, keepdims=True)
        variance = np.var(x, axis=-1, keepdims=True)
        std = np.sqrt(variance + self.epsilon)
        
        # Normalisation
        x_normalized = (x - mean) / std
        
        # RÃ©ajustement avec paramÃ¨tres apprenables
        output = self.gamma * x_normalized + self.beta
        
        return output, mean, std

# Test de Layer Normalization
print("ğŸ“Š TEST LAYER NORMALIZATION")
print("=" * 35)

# DonnÃ©es d'entrÃ©e (2 mots, 4 dimensions)
X_test = np.array([
    [10.0, 2.0, 0.5, -1.0],   # "Dakar" (valeurs dÃ©sÃ©quilibrÃ©es)
    [0.1, 15.0, -5.0, 3.0]    # "belle" (Ã©chelles diffÃ©rentes)
])

print("DonnÃ©es avant normalisation:")
print(X_test)
print(f"Moyennes par mot: {np.mean(X_test, axis=1)}")
print(f"Ã‰carts-types par mot: {np.std(X_test, axis=1)}")

# Application de LayerNorm
layer_norm = LayerNormalization(d_model=4)
X_normalized, moyennes, stds = layer_norm.forward(X_test)

print(f"\\nDonnÃ©es aprÃ¨s normalisation:")
print(X_normalized.round(3))
print(f"Nouvelles moyennes: {np.mean(X_normalized, axis=1).round(6)}")
print(f"Nouveaux Ã©carts-types: {np.std(X_normalized, axis=1).round(6)}")

print(f"\\nğŸ’¡ Effet: Chaque vecteur a maintenant moyenneâ‰ˆ0 et stdâ‰ˆ1")
print(f"   â†’ StabilitÃ© pour les couches suivantes !")`,
          },
          {
            type: "code",
            title: "Transformer complet avec toutes les innovations",
            description:
              "Assemblons un Transformer complet avec toutes les innovations :",
            code: `class TransformerComplet:
    def __init__(self, vocab_size=50, d_model=8, n_heads=2, n_layers=2, d_ff=16):
        """Transformer complet avec toutes les innovations"""
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.n_heads = n_heads
        self.n_layers = n_layers
        self.d_ff = d_ff
        
        # Embeddings
        self.embedding = np.random.randn(vocab_size, d_model) * 0.1
        
        # Couches
        self.encoder_blocks = []
        self.decoder_blocks = []
        
        for i in range(n_layers):
            # Chaque bloc contient: Attention + LayerNorm + FFN + LayerNorm
            encoder_block = {
                'attention': TransformerBlock(d_model, n_heads, d_ff),
                'layer_norm1': LayerNormalization(d_model),
                'layer_norm2': LayerNormalization(d_model)
            }
            self.encoder_blocks.append(encoder_block)
        
        # Couche de sortie
        self.output_projection = np.random.randn(d_model, vocab_size) * 0.1
        
        print(f"ğŸ—ï¸ TRANSFORMER COMPLET CRÃ‰Ã‰")
        print(f"   ğŸ“š Vocabulaire: {vocab_size} mots")
        print(f"   ğŸ§  ModÃ¨le: {d_model}D, {n_heads} tÃªtes")
        print(f"   ğŸ—ï¸ Architecture: {n_layers} couches")
        print(f"   âš™ï¸ Feed-forward: {d_ff}D")
        
        # Calcul du nombre de paramÃ¨tres
        params_embedding = vocab_size * d_model
        params_attention = n_layers * (4 * d_model * d_model)  # Q,K,V,O
        params_ff = n_layers * (d_model * d_ff + d_ff * d_model)
        params_norm = n_layers * 2 * (2 * d_model)  # gamma, beta
        params_output = d_model * vocab_size
        
        total_params = params_embedding + params_attention + params_ff + params_norm + params_output
        print(f"   ğŸ“Š ParamÃ¨tres totaux: {total_params:,}")
    
    def softmax(self, x):
        """Softmax numÃ©rique stable"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    def forward_encoder(self, input_ids):
        """Passage avant dans l'encoder"""
        seq_len = len(input_ids)
        
        # Embeddings + encodage positionnel
        x = self.embedding[input_ids]
        x += encodage_positionnel(seq_len, self.d_model)
        
        print(f"\\nğŸ”„ ENCODER FORWARD")
        print(f"Input: {input_ids}")
        print(f"Embeddings shape: {x.shape}")
        
        # Passage dans chaque bloc encoder
        for i, block in enumerate(self.encoder_blocks):
            print(f"\\n--- Bloc Encoder {i+1} ---")
            
            # Self-attention + connexion rÃ©siduelle + layer norm
            attn_output, _ = block['attention'].forward(x)
            x = block['layer_norm1'].forward(x + attn_output)[0]
            
            # Feed-forward + connexion rÃ©siduelle + layer norm  
            ff_output = block['attention'].feed_forward(x)
            x = block['layer_norm2'].forward(x + ff_output)[0]
            
            print(f"Sortie bloc {i+1}: {x.round(3)}")
        
        return x

# Test du Transformer complet
print("ğŸš€ TEST TRANSFORMER COMPLET")
print("=" * 40)

# Vocabulaire sÃ©nÃ©galais simplifiÃ©
vocab_senegal = [
    "<pad>", "<start>", "<end>", "Dakar", "SÃ©nÃ©gal", "est", "belle", 
    "ville", "ThiÃ¨s", "magnifique", "pays", "Afrique", "teranga"
]

# Phrase test: "Dakar est belle ville"
phrase_test = [3, 5, 6, 7]  # Indices dans le vocabulaire

# CrÃ©ation et test
transformer_complet = TransformerComplet(
    vocab_size=len(vocab_senegal),
    d_model=8, 
    n_heads=2, 
    n_layers=2,
    d_ff=16
)

# Test de l'encoder
encoder_output = transformer_complet.forward_encoder(phrase_test)

print(f"\\nâœ… ENCODAGE TERMINÃ‰")
print(f"Phrase: {[vocab_senegal[i] for i in phrase_test]}")
print(f"ReprÃ©sentation finale shape: {encoder_output.shape}")
print(f"Le Transformer a crÃ©Ã© une reprÃ©sentation riche de la phrase !")`,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : conception d'architecture Transformer",
            content: `
                        <p><strong>ğŸ¯ Exercice de conception :</strong></p>
                        <p>Vous devez concevoir des architectures Transformer pour 5 applications diffÃ©rentes. Pour chaque cas, justifiez vos choix :</p>
                        
                        <ol>
                            <li><strong>Traduction Wolof â†” FranÃ§ais</strong> (phrases courtes, 20 mots max)</li>
                            <li><strong>RÃ©sumÃ© d'articles</strong> (articles longs, 1000 mots â†’ 100 mots)</li>
                            <li><strong>Chatbot sÃ©nÃ©galais</strong> (conversations, rÃ©ponses rapides)</li>
                            <li><strong>Analyse de sentiment</strong> (tweets, classification binaire)</li>
                            <li><strong>GÃ©nÃ©ration de code Python</strong> (prÃ©cision syntaxique cruciale)</li>
                        </ol>
                        
                        <p><strong>ğŸ”§ ParamÃ¨tres Ã  choisir :</strong></p>
                        <ul>
                            <li>Taille du vocabulaire</li>
                            <li>d_model (dimension des embeddings)</li>
                            <li>Nombre de tÃªtes d'attention</li>
                            <li>Nombre de couches</li>
                            <li>Architecture (encoder-only, decoder-only, encoder-decoder)</li>
                        </ul>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('transformer-architecture-design')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="transformer-architecture-design" style="display: none;">
                        <ol>
                            <li><strong>Traduction Wolof â†” FranÃ§ais :</strong><br>
                                â€¢ Vocab: 10k (langues spÃ©cialisÃ©es)<br>
                                â€¢ d_model: 256 (suffisant pour 2 langues)<br>
                                â€¢ TÃªtes: 8 (relations syntaxiques complexes)<br>
                                â€¢ Couches: 6 (standard pour traduction)<br>
                                â€¢ Architecture: <strong>Encoder-Decoder</strong> (traduction = transformation)</li>
                            <li><strong>RÃ©sumÃ© d'articles :</strong><br>
                                â€¢ Vocab: 30k (vocabulaire riche)<br>
                                â€¢ d_model: 512 (comprendre nuances)<br>
                                â€¢ TÃªtes: 16 (attention fine sur long texte)<br>
                                â€¢ Couches: 12 (compression complexe)<br>
                                â€¢ Architecture: <strong>Encoder-Decoder</strong> (transformation longue â†’ courte)</li>
                            <li><strong>Chatbot sÃ©nÃ©galais :</strong><br>
                                â€¢ Vocab: 15k (conversation + culture)<br>
                                â€¢ d_model: 384 (Ã©quilibre performance/vitesse)<br>
                                â€¢ TÃªtes: 6 (dialogue contextuel)<br>
                                â€¢ Couches: 8 (comprÃ©hension conversationnelle)<br>
                                â€¢ Architecture: <strong>Decoder-only</strong> (gÃ©nÃ©ration pure)</li>
                            <li><strong>Analyse de sentiment :</strong><br>
                                â€¢ Vocab: 20k (expressions variÃ©es)<br>
                                â€¢ d_model: 128 (classification simple)<br>
                                â€¢ TÃªtes: 4 (relations Ã©motionnelles)<br>
                                â€¢ Couches: 4 (pas besoin de profondeur)<br>
                                â€¢ Architecture: <strong>Encoder-only</strong> (classification)</li>
                            <li><strong>GÃ©nÃ©ration de code :</strong><br>
                                â€¢ Vocab: 5k (tokens de code)<br>
                                â€¢ d_model: 768 (prÃ©cision syntaxique)<br>
                                â€¢ TÃªtes: 12 (structure de code complexe)<br>
                                â€¢ Couches: 24 (logique de programmation)<br>
                                â€¢ Architecture: <strong>Decoder-only</strong> (gÃ©nÃ©ration sÃ©quentielle)</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "warning",
            icon: "âš ï¸",
            title: "L'hÃ©ritage rÃ©volutionnaire des Transformers",
            content: `
                        <p><strong>ğŸŒŸ Les Transformers ont crÃ©Ã© l'IA moderne que nous connaissons :</strong></p>
                        
                        <p><strong>ğŸ“ˆ Ã‰volution fulgurante (2017-2024) :</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>2017</strong> : Transformer original (traduction)</li>
                            <li>ğŸ§  <strong>2018</strong> : BERT (comprÃ©hension) + GPT-1 (gÃ©nÃ©ration)</li>
                            <li>ğŸš€ <strong>2019</strong> : GPT-2 (1.5B paramÃ¨tres)</li>
                            <li>ğŸŒŸ <strong>2020</strong> : GPT-3 (175B paramÃ¨tres)</li>
                            <li>ğŸ’¬ <strong>2022</strong> : ChatGPT (rÃ©volution grand public)</li>
                            <li>ğŸ§  <strong>2023</strong> : GPT-4 (multimodal)</li>
                            <li>ğŸ”® <strong>2024</strong> : ModÃ¨les de 1T+ paramÃ¨tres</li>
                        </ul>
                        
                        <p><strong>ğŸŒ Impact sur tous les domaines :</strong></p>
                        <ul>
                            <li>ğŸ’¬ <strong>Langage</strong> : ChatGPT, traduction, rÃ©sumÃ©</li>
                            <li>ğŸ–¼ï¸ <strong>Vision</strong> : Vision Transformer (ViT)</li>
                            <li>ğŸ’» <strong>Code</strong> : GitHub Copilot, CodeT5</li>
                            <li>ğŸ§¬ <strong>Science</strong> : AlphaFold, dÃ©couverte de mÃ©dicaments</li>
                            <li>ğŸ¨ <strong>CrÃ©ativitÃ©</strong> : DALL-E, Midjourney</li>
                            <li>ğŸµ <strong>Musique</strong> : MuseNet, Jukebox</li>
                        </ul>
                        
                        <p><strong>ğŸ‡¸ğŸ‡³ Applications pour le SÃ©nÃ©gal :</strong></p>
                        <ul>
                            <li>ğŸ—£ï¸ <strong>Langues nationales</strong> : traduction automatique Wolof/Pulaar/Serer</li>
                            <li>ğŸ“š <strong>Ã‰ducation</strong> : assistants pÃ©dagogiques en langues locales</li>
                            <li>ğŸ¥ <strong>SantÃ©</strong> : diagnostic automatique adaptÃ© au contexte local</li>
                            <li>ğŸŒ¾ <strong>Agriculture</strong> : conseils personnalisÃ©s selon les rÃ©gions</li>
                            <li>ğŸ’¼ <strong>Administration</strong> : automatisation des services publics</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Point clÃ© :</strong> Les Transformers ne sont pas juste une amÃ©lioration technique - ils ont crÃ©Ã© un <strong>nouveau paradigme</strong> oÃ¹ l'IA peut comprendre et gÃ©nÃ©rer du contenu avec une qualitÃ© quasi-humaine.</p>
                        
                        <p><strong>ğŸ”® Prochaine Ã©tape :</strong> GPT & BERT - les deux familles de modÃ¨les qui ont conquis le monde !</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ğŸ¤” Quelle est l'innovation principale des Transformers par rapport aux RNN/LSTM ?",
          options: [
            "A) Ils utilisent plus de paramÃ¨tres",
            "B) Ils traitent tous les mots en parallÃ¨le grÃ¢ce Ã  l'attention",
            "C) Ils sont plus prÃ©cis sur les tÃ¢ches de classification",
            "D) Ils consomment moins de mÃ©moire",
          ],
          correct: 1,
          explanation:
            "L'innovation rÃ©volutionnaire est le traitement parallÃ¨le : au lieu de traiter les mots un par un (RNN), les Transformers utilisent l'attention pour que chaque mot 'regarde' tous les autres simultanÃ©ment. Cela permet un entraÃ®nement beaucoup plus rapide et une meilleure capture des dÃ©pendances Ã  long terme.",
        },
        prevModule: "attention.html",
        nextModule: "gpt-bert.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
