<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>GPT & BERT | IA4Ndada</title>

    <!-- MathJax pour les formules mathÃ©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">ğŸ  Accueil</a>
          <span>â€º</span>
          <span>ğŸ’¬ LLMs & IA Moderne</span>
          <span>â€º</span>
          <span>GPT & BERT</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>ğŸ¤– GPT & BERT : Les GÃ©ants de l'IA Moderne</h1>
      <p class="subtitle">Module 5.3 - ModÃ¨les de Langage RÃ©volutionnaires</p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>ğŸ¯ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajoutÃ©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajoutÃ© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>ğŸ‰ Checkpoint - GPT & BERT</h3>
        <p>
          FÃ©licitations ! Vous comprenez maintenant les modÃ¨les qui ont
          rÃ©volutionnÃ© l'IA moderne.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme complÃ©tÃ©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="transformers.html" class="nav-link" id="prev-link"
          >â† Module prÃ©cÃ©dent : Transformers</a
        >
        <a href="fine-tuning.html" class="nav-link" id="next-link"
          >Module suivant : Fine-tuning â†’</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module GPT & BERT
      const moduleConfig = {
        id: "llm-gpt-bert",
        title: "GPT & BERT : Les GÃ©ants de l'IA Moderne",
        category: "LLMs & IA Moderne",
        objectives: [
          "Comprendre la rÃ©volution GPT et BERT dans l'histoire de l'IA",
          "MaÃ®triser les diffÃ©rences fondamentales entre les deux approches",
          "Calculer manuellement les prÃ©dictions de chaque modÃ¨le",
          "Comprendre les tÃ¢ches spÃ©cialisÃ©es de chaque famille",
          "ImplÃ©menter des versions simplifiÃ©es des deux architectures",
        ],
        content: [
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "La rÃ©volution 2018-2019 : deux gÃ©ants Ã©mergent",
            content: `
                        <p>En l'espace de 18 mois, <strong>deux familles de modÃ¨les</strong> ont rÃ©volutionnÃ© l'intelligence artificielle et changÃ© le monde pour toujours :</p>
                        
                        <p><strong>ğŸ¤– BERT (2018) :</strong> "Bidirectional Encoder Representations from Transformers"</p>
                        <ul>
                            <li>ğŸ¢ <strong>Google</strong> : rÃ©volutionne la recherche et la comprÃ©hension</li>
                            <li>ğŸ‘ï¸ <strong>Vision bidirectionnelle</strong> : regarde dans les deux sens</li>
                            <li>ğŸ¯ <strong>SpÃ©cialitÃ©</strong> : comprendre et analyser le texte</li>
                        </ul>
                        
                        <p><strong>ğŸ’¬ GPT (2018-2019) :</strong> "Generative Pre-trained Transformer"</p>
                        <ul>
                            <li>ğŸ¢ <strong>OpenAI</strong> : rÃ©volutionne la gÃ©nÃ©ration et la conversation</li>
                            <li>â¡ï¸ <strong>Vision unidirectionnelle</strong> : regarde seulement vers la gauche</li>
                            <li>ğŸ¯ <strong>SpÃ©cialitÃ©</strong> : gÃ©nÃ©rer et converser naturellement</li>
                        </ul>
                        
                        <p><strong>ğŸ“Š Impact quantifiÃ© :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>Recherche Google</strong> : +10% de pertinence grÃ¢ce Ã  BERT</li>
                            <li>ğŸ’¬ <strong>ChatGPT</strong> : 100M utilisateurs en 2 mois (record mondial)</li>
                            <li>ğŸ’° <strong>MarchÃ© IA</strong> : de 50B$ Ã  500B$ en 5 ans</li>
                            <li>ğŸŒ <strong>Langues supportÃ©es</strong> : de 10 Ã  100+ langues</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Point clÃ© :</strong> Ces deux approches complÃ©mentaires ont dÃ©mocratisÃ© l'IA et rendu possible l'intelligence artificielle gÃ©nÃ©rale que nous connaissons aujourd'hui.</p>
                    `,
          },
          {
            type: "intuition",
            icon: "ğŸ§ ",
            title: "L'analogie de la lecture : deux stratÃ©gies opposÃ©es",
            content: `
                        <p>Imaginez deux Ã©tudiants sÃ©nÃ©galais qui prÃ©parent le <strong>baccalaurÃ©at en franÃ§ais</strong> avec des stratÃ©gies opposÃ©es :</p>
                        
                        <p><strong>ğŸ“š Ã‰tudiant BERT (Bidirectionnel) :</strong></p>
                        <ul>
                            <li>ğŸ“– <strong>Lit tout le texte</strong> avant de rÃ©pondre aux questions</li>
                            <li>ğŸ” <strong>Analyse le contexte complet</strong> : dÃ©but, milieu, fin</li>
                            <li>ğŸ¯ <strong>Excellent pour</strong> : comprÃ©hension, analyse, questions prÃ©cises</li>
                            <li>â° <strong>Prend son temps</strong> : rÃ©flÃ©chit avant de parler</li>
                        </ul>
                        
                        <p><strong>ğŸ’¬ Ã‰tudiant GPT (Unidirectionnel) :</strong></p>
                        <ul>
                            <li>ğŸ“ <strong>Lit mot par mot</strong> et prÃ©dit le suivant</li>
                            <li>â¡ï¸ <strong>Ne regarde jamais en arriÃ¨re</strong> : seulement ce qu'il a dÃ©jÃ  lu</li>
                            <li>ğŸ¯ <strong>Excellent pour</strong> : Ã©criture crÃ©ative, conversation, gÃ©nÃ©ration</li>
                            <li>âš¡ <strong>RÃ©pond instantanÃ©ment</strong> : gÃ©nÃ¨re au fur et Ã  mesure</li>
                        </ul>
                        
                        <p><strong>ğŸ­ Exemple concret :</strong></p>
                        <p><strong>Phrase :</strong> "Le prÃ©sident du SÃ©nÃ©gal a annoncÃ© une nouvelle politique Ã©conomique pour ___"</p>
                        
                        <ul>
                            <li>ğŸ¤– <strong>BERT</strong> : "Je vois toute la phrase, le mot manquant est probablement 'stimuler', 'dÃ©velopper' ou 'moderniser'"</li>
                            <li>ğŸ’¬ <strong>GPT</strong> : "BasÃ© sur ce que j'ai lu jusqu'ici, je continue logiquement avec 'stimuler la croissance et crÃ©er des emplois dans les secteurs prioritaires...'"</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ ComplÃ©mentaritÃ© parfaite :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>BERT</strong> = Sherlock Holmes (analyse minutieuse)</li>
                            <li>ğŸ¨ <strong>GPT</strong> = Conteur traditionnel (narration fluide)</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Architectures formelles : Encoder vs Decoder",
            content: `
                        <p><strong>ğŸ“ Formalisation des deux architectures :</strong></p>
                        
                        <p><strong>ğŸ¤– BERT (Encoder-only) :</strong></p>
                        <p>$$\\text{BERT}(\\vec{x}) = \\text{Encoder}^{(L)}(\\text{Encoder}^{(L-1)}(...\\text{Encoder}^{(1)}(\\vec{x} + \\vec{p})))$$</p>
                        
                        <p><strong>ğŸ” Composants :</strong></p>
                        <ul>
                            <li>\\(\\vec{x}\\) = <strong>tokens d'entrÃ©e</strong> (phrase complÃ¨te)</li>
                            <li>\\(\\vec{p}\\) = <strong>encodage positionnel</strong> (voir <a href="attention.html">Module 5.1</a>)</li>
                            <li>\\(\\text{Encoder}^{(l)}\\) = <strong>couche d'encodage</strong> (Self-Attention + FFN)</li>
                            <li>\\(L\\) = <strong>nombre de couches</strong> (12 pour BERT-base, 24 pour BERT-large)</li>
                        </ul>
                        
                        <p><strong>ğŸ’¬ GPT (Decoder-only) :</strong></p>
                        <p>$$\\text{GPT}(\\vec{x}_{<t}) = \\text{Decoder}^{(L)}(\\text{Decoder}^{(L-1)}(...\\text{Decoder}^{(1)}(\\vec{x}_{<t} + \\vec{p})))$$</p>
                        
                        <p><strong>ğŸ” DiffÃ©rence cruciale :</strong></p>
                        <ul>
                            <li>\\(\\vec{x}_{<t}\\) = <strong>tokens jusqu'Ã  la position t</strong> (masquage causal)</li>
                            <li>\\(\\text{Decoder}^{(l)}\\) = <strong>couche de dÃ©codage</strong> (Masked Self-Attention + FFN)</li>
                            <li><strong>Masque causal</strong> : \\(\\text{Attention}_{ij} = -\\infty\\) si \\(j > i\\)</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ DiffÃ©rence fondamentale :</strong></p>
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>BERT :</strong> Attention bidirectionnelle â†’ voit tout le contexte<br>
                            <strong>GPT :</strong> Attention causale â†’ ne voit que le passÃ©
                        </div>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Calcul manuel : BERT vs GPT sur mÃªme phrase",
            content: `
                        <p><strong>ğŸ“ Comparons les deux approches sur la mÃªme phrase :</strong></p>
                        <p><strong>Phrase :</strong> "Dakar est [MASK] capitale"</p>
                        
                        <p><strong>ğŸ¤– Approche BERT (bidirectionnelle) :</strong></p>
                        <p>BERT voit : ["Dakar", "est", "[MASK]", "capitale"]</p>
                        
                        <p><strong>ğŸ” Calcul d'attention pour [MASK] :</strong></p>
                        <p>Le token [MASK] peut regarder dans toutes les directions :</p>
                        <ul>
                            <li>Attention([MASK], "Dakar") = 0.4 â†’ <em>ville importante</em></li>
                            <li>Attention([MASK], "est") = 0.1 â†’ <em>verbe de liaison</em></li>
                            <li>Attention([MASK], "capitale") = 0.5 â†’ <em>indice crucial !</em></li>
                        </ul>
                        <p><strong>ğŸ¯ PrÃ©diction BERT :</strong> "la" (car "Dakar est **la** capitale")</p>
                        
                        <p><strong>ğŸ’¬ Approche GPT (unidirectionnelle) :</strong></p>
                        <p>GPT gÃ©nÃ¨re mot par mot : "Dakar" â†’ "est" â†’ "la" â†’ "capitale"</p>
                        
                        <p><strong>ğŸ” Calcul d'attention pour "la" :</strong></p>
                        <p>Le token "la" ne peut regarder que vers la gauche :</p>
                        <ul>
                            <li>Attention("la", "Dakar") = 0.6 â†’ <em>sujet principal</em></li>
                            <li>Attention("la", "est") = 0.4 â†’ <em>verbe prÃ©cÃ©dent</em></li>
                            <li>Attention("la", "capitale") = 0.0 â†’ <em>interdit ! (futur)</em></li>
                        </ul>
                        <p><strong>ğŸ¯ PrÃ©diction GPT :</strong> "la" (basÃ© uniquement sur "Dakar est")</p>
                        
                        <p><strong>ğŸ’¡ Observation :</strong> MÃªme rÃ©sultat, mais processus complÃ¨tement diffÃ©rent !</p>
                    `,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "PrÃ©-entraÃ®nement : la rÃ©volution de l'apprentissage",
            content: `
                        <p><strong>ğŸ¯ Le prÃ©-entraÃ®nement a rÃ©volutionnÃ© l'IA en crÃ©ant des "cerveaux universels" :</strong></p>
                        
                        <p><strong>ğŸ“š Avant (2017) :</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>ModÃ¨les spÃ©cialisÃ©s</strong> : un modÃ¨le par tÃ¢che</li>
                            <li>ğŸ“Š <strong>DonnÃ©es Ã©tiquetÃ©es</strong> : besoin de millions d'exemples annotÃ©s</li>
                            <li>â° <strong>Temps d'entraÃ®nement</strong> : mois pour chaque nouvelle tÃ¢che</li>
                            <li>ğŸ’° <strong>CoÃ»t prohibitif</strong> : seules les GAFAM pouvaient se le permettre</li>
                        </ul>
                        
                        <p><strong>ğŸš€ AprÃ¨s (2018+) :</strong></p>
                        <ul>
                            <li>ğŸ§  <strong>ModÃ¨les universels</strong> : un cerveau pour toutes les tÃ¢ches</li>
                            <li>ğŸ“– <strong>Texte brut</strong> : apprentissage sur tout Internet</li>
                            <li>âš¡ <strong>Adaptation rapide</strong> : heures pour une nouvelle tÃ¢che</li>
                            <li>ğŸŒ <strong>DÃ©mocratisation</strong> : accessible aux startups et universitÃ©s</li>
                        </ul>
                        
                        <p><strong>ğŸ”‘ StratÃ©gies de prÃ©-entraÃ®nement :</strong></p>
                        
                        <p><strong>ğŸ¤– BERT - Masked Language Modeling (MLM) :</strong></p>
                        <ul>
                            <li>ğŸ­ <strong>Principe</strong> : masquer 15% des mots, deviner les mots cachÃ©s</li>
                            <li>ğŸ“š <strong>Exemple</strong> : "Le [MASK] du SÃ©nÃ©gal est Macky Sall" â†’ prÃ©dire "prÃ©sident"</li>
                            <li>ğŸ§  <strong>Apprentissage</strong> : comprendre le contexte bidirectionnel</li>
                        </ul>
                        
                        <p><strong>ğŸ’¬ GPT - Causal Language Modeling (CLM) :</strong></p>
                        <ul>
                            <li>ğŸ“ <strong>Principe</strong> : prÃ©dire le mot suivant dans une sÃ©quence</li>
                            <li>ğŸ“š <strong>Exemple</strong> : "Le prÃ©sident du SÃ©nÃ©gal" â†’ prÃ©dire "est", "a", "vient"...</li>
                            <li>ğŸ§  <strong>Apprentissage</strong> : gÃ©nÃ©rer du texte cohÃ©rent et naturel</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Fonctions objectif : MLM vs CLM",
            content: `
                        <p><strong>ğŸ“ Formalisation des objectifs d'apprentissage :</strong></p>
                        
                        <p><strong>ğŸ¤– BERT - Masked Language Modeling :</strong></p>
                        <p>$$\\mathcal{L}_{MLM} = -\\sum_{i \\in \\mathcal{M}} \\log P(x_i | \\vec{x}_{\\setminus i})$$</p>
                        
                        <p><strong>ğŸ” DÃ©cryptage :</strong></p>
                        <ul>
                            <li>\\(\\mathcal{M}\\) = <strong>ensemble des positions masquÃ©es</strong></li>
                            <li>\\(x_i\\) = <strong>token masquÃ© Ã  prÃ©dire</strong></li>
                            <li>\\(\\vec{x}_{\\setminus i}\\) = <strong>tous les autres tokens</strong> (contexte bidirectionnel)</li>
                            <li>\\(P(x_i | \\vec{x}_{\\setminus i})\\) = <strong>probabilitÃ© du token</strong> sachant tout le contexte</li>
                        </ul>
                        
                        <p><strong>ğŸ’¬ GPT - Causal Language Modeling :</strong></p>
                        <p>$$\\mathcal{L}_{CLM} = -\\sum_{i=1}^{T} \\log P(x_i | x_1, x_2, ..., x_{i-1})$$</p>
                        
                        <p><strong>ğŸ” DÃ©cryptage :</strong></p>
                        <ul>
                            <li>\\(T\\) = <strong>longueur de la sÃ©quence</strong></li>
                            <li>\\(x_i\\) = <strong>token Ã  la position i</strong></li>
                            <li>\\(x_1, ..., x_{i-1}\\) = <strong>contexte prÃ©cÃ©dent uniquement</strong></li>
                            <li>\\(P(x_i | x_{<i})\\) = <strong>probabilitÃ© du token</strong> sachant seulement le passÃ©</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ DiffÃ©rence cruciale :</strong></p>
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>BERT :</strong> Optimise la comprÃ©hension (contexte complet)<br>
                            <strong>GPT :</strong> Optimise la gÃ©nÃ©ration (prÃ©diction sÃ©quentielle)
                        </div>
                        
                        <p><strong>ğŸ“Š ConsÃ©quences :</strong></p>
                        <ul>
                            <li><strong>BERT</strong> : excellent pour classification, Q&A, analyse</li>
                            <li><strong>GPT</strong> : excellent pour gÃ©nÃ©ration, conversation, crÃ©ativitÃ©</li>
                        </ul>
                    `,
          },
          {
            type: "code",
            title: "Simulation BERT : Masked Language Modeling",
            description: "Simulons comment BERT prÃ©dit les mots masquÃ©s :",
            code: `import numpy as np

class BERTSimple:
    def __init__(self, vocab_size=1000, hidden_size=64):
        """BERT simplifiÃ© pour dÃ©monstration"""
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        
        # Vocabulaire simplifiÃ© (mots sÃ©nÃ©galais + franÃ§ais)
        self.vocab = {
            'dakar': 0, 'senegal': 1, 'president': 2, 'capitale': 3,
            'est': 4, 'la': 5, 'le': 6, 'une': 7, 'pays': 8,
            'afrique': 9, 'ouest': 10, '[MASK]': 11, '[CLS]': 12, '[SEP]': 13
        }
        self.id_to_token = {v: k for k, v in self.vocab.items()}
        
        # Matrices d'embedding (simplifiÃ©es)
        np.random.seed(42)
        self.token_embeddings = np.random.normal(0, 0.1, (vocab_size, hidden_size))
        self.position_embeddings = np.random.normal(0, 0.1, (512, hidden_size))
        
        print("ğŸ¤– BERT SimplifiÃ© initialisÃ©")
        print(f"Vocabulaire : {len(self.vocab)} mots")
        print(f"Dimension cachÃ©e : {hidden_size}")
    
    def tokenize(self, phrase):
        """Tokenisation simple"""
        mots = phrase.lower().split()
        tokens = []
        for mot in mots:
            if mot in self.vocab:
                tokens.append(self.vocab[mot])
            else:
                tokens.append(self.vocab.get('[MASK]', 0))  # Token inconnu
        return tokens
    
    def embed(self, token_ids):
        """CrÃ©ation des embeddings"""
        seq_len = len(token_ids)
        
        # Embeddings de tokens
        token_emb = self.token_embeddings[token_ids]
        
        # Embeddings positionnels
        pos_emb = self.position_embeddings[:seq_len]
        
        # Somme (comme dans BERT rÃ©el)
        embeddings = token_emb + pos_emb
        
        return embeddings
    
    def attention_bidirectionnelle(self, embeddings):
        """Attention bidirectionnelle simplifiÃ©e"""
        seq_len, hidden_size = embeddings.shape
        
        # Matrices Q, K, V simplifiÃ©es
        W_q = np.random.normal(0, 0.1, (hidden_size, hidden_size))
        W_k = np.random.normal(0, 0.1, (hidden_size, hidden_size))
        W_v = np.random.normal(0, 0.1, (hidden_size, hidden_size))
        
        Q = embeddings @ W_q
        K = embeddings @ W_k  
        V = embeddings @ W_v
        
        # Scores d'attention
        scores = Q @ K.T / np.sqrt(hidden_size)
        
        # Softmax (pas de masque - bidirectionnel !)
        attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)
        
        # Sortie pondÃ©rÃ©e
        output = attention_weights @ V
        
        return output, attention_weights
    
    def predire_masque(self, phrase_avec_masque):
        """PrÃ©dire le token masquÃ©"""
        print(f"ğŸ­ Phrase avec masque : '{phrase_avec_masque}'")
        
        # Tokenisation
        tokens = self.tokenize(phrase_avec_masque)
        print(f"Tokens : {[self.id_to_token[t] for t in tokens]}")
        
        # Position du masque
        mask_pos = tokens.index(self.vocab['[MASK]'])
        print(f"Position du masque : {mask_pos}")
        
        # Embeddings
        embeddings = self.embed(tokens)
        
        # Attention bidirectionnelle
        output, attention_weights = self.attention_bidirectionnelle(embeddings)
        
        # Analyse de l'attention pour le token masquÃ©
        print(f"\\nğŸ” Attention du token masquÃ© :")
        for i, token_id in enumerate(tokens):
            token = self.id_to_token[token_id]
            attention_score = attention_weights[mask_pos, i]
            print(f"  {token}: {attention_score:.3f}")
        
        # Simulation de prÃ©diction (simplifiÃ©e)
        candidats = ['la', 'une', 'notre', 'cette']
        scores = np.random.random(len(candidats))
        scores = scores / scores.sum()  # Normalisation
        
        print(f"\\nğŸ¯ PrÃ©dictions possibles :")
        for candidat, score in zip(candidats, scores):
            print(f"  '{candidat}': {score:.3f}")
        
        meilleur = candidats[np.argmax(scores)]
        print(f"\\nâœ… PrÃ©diction finale : '{meilleur}'")
        
        return meilleur, attention_weights

# Test BERT
bert = BERTSimple()
print("ğŸ¤– TEST BERT - MASKED LANGUAGE MODELING")
print("=" * 50)

prediction, attention = bert.predire_masque("dakar est [MASK] capitale")`,
          },
          {
            type: "code",
            title: "Simulation GPT : gÃ©nÃ©ration autoregressive",
            description: "Simulons comment GPT gÃ©nÃ¨re du texte mot par mot :",
            code: `class GPTSimple:
    def __init__(self, vocab_size=1000, hidden_size=64):
        """GPT simplifiÃ© pour dÃ©monstration"""
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        
        # MÃªme vocabulaire que BERT
        self.vocab = {
            'dakar': 0, 'senegal': 1, 'president': 2, 'capitale': 3,
            'est': 4, 'la': 5, 'le': 6, 'une': 7, 'pays': 8,
            'afrique': 9, 'ouest': 10, 'belle': 11, 'ville': 12,
            'moderne': 13, 'economique': 14, 'culturel': 15
        }
        self.id_to_token = {v: k for k, v in self.vocab.items()}
        
        # Matrices d'embedding
        np.random.seed(42)
        self.token_embeddings = np.random.normal(0, 0.1, (vocab_size, hidden_size))
        self.position_embeddings = np.random.normal(0, 0.1, (512, hidden_size))
        
        print("ğŸ’¬ GPT SimplifiÃ© initialisÃ©")
    
    def attention_causale(self, embeddings):
        """Attention avec masque causal"""
        seq_len, hidden_size = embeddings.shape
        
        # Matrices Q, K, V
        W_q = np.random.normal(0, 0.1, (hidden_size, hidden_size))
        W_k = np.random.normal(0, 0.1, (hidden_size, hidden_size))
        W_v = np.random.normal(0, 0.1, (hidden_size, hidden_size))
        
        Q = embeddings @ W_q
        K = embeddings @ W_k
        V = embeddings @ W_v
        
        # Scores d'attention
        scores = Q @ K.T / np.sqrt(hidden_size)
        
        # MASQUE CAUSAL : interdire de regarder le futur
        mask = np.triu(np.ones((seq_len, seq_len)), k=1) * -1e9
        scores_masques = scores + mask
        
        # Softmax avec masque
        attention_weights = np.exp(scores_masques) / np.sum(np.exp(scores_masques), axis=1, keepdims=True)
        
        # Sortie pondÃ©rÃ©e
        output = attention_weights @ V
        
        return output, attention_weights
    
    def generer_mot_suivant(self, contexte):
        """GÃ©nÃ¨re le mot suivant basÃ© sur le contexte"""
        print(f"ğŸ“ Contexte : '{contexte}'")
        
        # Tokenisation
        tokens = self.tokenize(contexte)
        print(f"Tokens : {[self.id_to_token[t] for t in tokens]}")
        
        # Embeddings
        embeddings = self.embed(tokens)
        
        # Attention causale
        output, attention_weights = self.attention_causale(embeddings)
        
        # Analyse de l'attention pour le dernier token
        last_pos = len(tokens) - 1
        print(f"\\nğŸ” Attention du dernier token '{self.id_to_token[tokens[-1]]}' :")
        for i, token_id in enumerate(tokens):
            token = self.id_to_token[token_id]
            attention_score = attention_weights[last_pos, i]
            print(f"  {token}: {attention_score:.3f}")
        
        # Simulation de prÃ©diction du mot suivant
        candidats = ['est', 'la', 'une', 'belle', 'moderne']
        scores = np.random.random(len(candidats))
        scores = scores / scores.sum()
        
        print(f"\\nğŸ¯ Mots suivants possibles :")
        for candidat, score in zip(candidats, scores):
            print(f"  '{candidat}': {score:.3f}")
        
        meilleur = candidats[np.argmax(scores)]
        print(f"\\nâœ… Mot gÃ©nÃ©rÃ© : '{meilleur}'")
        
        return meilleur, attention_weights
    
    def tokenize(self, phrase):
        """Tokenisation simple"""
        mots = phrase.lower().split()
        tokens = []
        for mot in mots:
            if mot in self.vocab:
                tokens.append(self.vocab[mot])
            else:
                tokens.append(0)  # Token par dÃ©faut
        return tokens
    
    def embed(self, token_ids):
        """CrÃ©ation des embeddings"""
        seq_len = len(token_ids)
        token_emb = self.token_embeddings[token_ids]
        pos_emb = self.position_embeddings[:seq_len]
        return token_emb + pos_emb

# Test GPT
gpt = GPTSimple()
print("\\nğŸ’¬ TEST GPT - GÃ‰NÃ‰RATION AUTOREGRESSIVE")
print("=" * 50)

mot_suivant, attention = gpt.generer_mot_suivant("dakar est")`,
          },
          {
            type: "code",
            title: "GÃ©nÃ©ration complÃ¨te avec GPT",
            description: "Simulons une gÃ©nÃ©ration complÃ¨te de phrase :",
            code: `def generation_complete(gpt_model, prompt_initial, max_tokens=5):
    """GÃ©nÃ©ration autoregressive complÃ¨te"""
    print(f"ğŸš€ GÃ‰NÃ‰RATION AUTOREGRESSIVE")
    print(f"Prompt initial : '{prompt_initial}'")
    print("=" * 40)
    
    phrase_actuelle = prompt_initial
    
    for step in range(max_tokens):
        print(f"\\nÃ‰tape {step + 1}:")
        print(f"Contexte actuel : '{phrase_actuelle}'")
        
        # GÃ©nÃ©rer le mot suivant
        mot_suivant, _ = gpt_model.generer_mot_suivant(phrase_actuelle)
        
        # Ajouter Ã  la phrase
        phrase_actuelle += " " + mot_suivant
        print(f"â†’ Phrase Ã©tendue : '{phrase_actuelle}'")
        
        # Condition d'arrÃªt (simulation)
        if mot_suivant in ['fin', 'stop', '.']:
            print("ğŸ›‘ GÃ©nÃ©ration terminÃ©e (token de fin)")
            break
    
    return phrase_actuelle

# Test de gÃ©nÃ©ration
phrase_finale = generation_complete(gpt, "le senegal", max_tokens=4)
print(f"\\nğŸ¯ PHRASE FINALE GÃ‰NÃ‰RÃ‰E :")
print(f"'{phrase_finale}'")

# Comparaison des masques d'attention
print(f"\\nğŸ” COMPARAISON DES MASQUES D'ATTENTION")
print("=" * 45)

# Masque BERT (bidirectionnel - tout visible)
masque_bert = np.ones((4, 4))
print("ğŸ¤– Masque BERT (bidirectionnel) :")
print("   0 1 2 3")
for i in range(4):
    print(f"{i}: {masque_bert[i].astype(int)}")

print()

# Masque GPT (causal - triangulaire infÃ©rieur)
masque_gpt = np.tril(np.ones((4, 4)))
print("ğŸ’¬ Masque GPT (causal) :")
print("   0 1 2 3")
for i in range(4):
    print(f"{i}: {masque_gpt[i].astype(int)}")

print("\\nğŸ’¡ 0 = masquÃ© (invisible), 1 = visible")`,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice pratique : calcul manuel des deux approches",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Phrase : "Le SÃ©nÃ©gal [MASK] un pays"</p>
                        <p>Tokens : [Le=0, SÃ©nÃ©gal=1, [MASK]=2, un=3, pays=4]</p>
                        
                        <p><strong>ğŸ“ Calculez manuellement :</strong></p>
                        <ol>
                            <li><strong>BERT :</strong> Quels tokens le [MASK] peut-il voir ?</li>
                            <li><strong>GPT :</strong> Si on gÃ©nÃ¨re Ã  la position 2, quels tokens peut-on voir ?</li>
                            <li><strong>Attention BERT :</strong> Calculez les scores d'attention pour [MASK]</li>
                            <li><strong>Attention GPT :</strong> Calculez les scores pour la position 2</li>
                            <li><strong>PrÃ©dictions :</strong> Quel mot chaque modÃ¨le prÃ©dirait-il ?</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('bert-gpt-comparison-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="bert-gpt-comparison-exercise" style="display: none;">
                        <ol>
                            <li><strong>BERT - Vision bidirectionnelle :</strong><br>
                                [MASK] peut voir : "Le" (0), "SÃ©nÃ©gal" (1), "un" (3), "pays" (4)<br>
                                â†’ Contexte complet disponible</li>
                            <li><strong>GPT - Vision causale :</strong><br>
                                Position 2 peut voir : "Le" (0), "SÃ©nÃ©gal" (1)<br>
                                â†’ Seulement le contexte prÃ©cÃ©dent</li>
                            <li><strong>Attention BERT :</strong><br>
                                Attention([MASK], "Le") = 0.1<br>
                                Attention([MASK], "SÃ©nÃ©gal") = 0.4<br>
                                Attention([MASK], "un") = 0.2<br>
                                Attention([MASK], "pays") = 0.3</li>
                            <li><strong>Attention GPT :</strong><br>
                                Attention(pos2, "Le") = 0.3<br>
                                Attention(pos2, "SÃ©nÃ©gal") = 0.7<br>
                                Attention(pos2, "un") = 0.0 (masquÃ©)<br>
                                Attention(pos2, "pays") = 0.0 (masquÃ©)</li>
                            <li><strong>PrÃ©dictions :</strong><br>
                                <strong>BERT :</strong> "est" (grÃ¢ce au contexte "un pays")<br>
                                <strong>GPT :</strong> "est" (basÃ© sur "Le SÃ©nÃ©gal" uniquement)</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "SpÃ©cialisations : tÃ¢ches de chaque famille",
            content: `
                        <p><strong>ğŸ¯ Chaque famille excelle dans des domaines spÃ©cifiques :</strong></p>
                        
                        <p><strong>ğŸ¤– Famille BERT (ComprÃ©hension) :</strong></p>
                        <ul>
                            <li>ğŸ“Š <strong>Classification de texte</strong> : sentiment, spam, catÃ©gories</li>
                            <li>â“ <strong>Question-RÃ©ponse</strong> : extraire des rÃ©ponses de documents</li>
                            <li>ğŸ·ï¸ <strong>Reconnaissance d'entitÃ©s</strong> : noms, lieux, organisations</li>
                            <li>ğŸ” <strong>Recherche sÃ©mantique</strong> : trouver des documents pertinents</li>
                            <li>ğŸ“ <strong>RÃ©sumÃ© extractif</strong> : sÃ©lectionner les phrases importantes</li>
                            <li>ğŸŒ <strong>Traduction</strong> : avec architecture encoder-decoder</li>
                        </ul>
                        
                        <p><strong>ğŸ’¬ Famille GPT (GÃ©nÃ©ration) :</strong></p>
                        <ul>
                            <li>âœï¸ <strong>GÃ©nÃ©ration de texte</strong> : articles, histoires, poÃ¨mes</li>
                            <li>ğŸ’¬ <strong>Conversation</strong> : chatbots, assistants virtuels</li>
                            <li>ğŸ”„ <strong>ComplÃ©tion de code</strong> : GitHub Copilot</li>
                            <li>ğŸ“ <strong>RÃ©sumÃ© gÃ©nÃ©ratif</strong> : rÃ©Ã©crire avec ses propres mots</li>
                            <li>ğŸ¨ <strong>CrÃ©ativitÃ©</strong> : brainstorming, idÃ©es, scÃ©narios</li>
                            <li>ğŸ§  <strong>Raisonnement</strong> : rÃ©solution de problÃ¨mes Ã©tape par Ã©tape</li>
                        </ul>
                        
                        <p><strong>ğŸ‡¸ğŸ‡³ Applications sÃ©nÃ©galaises :</strong></p>
                        <ul>
                            <li>ğŸ¤– <strong>BERT-Wolof</strong> : comprendre les textes en langues nationales</li>
                            <li>ğŸ’¬ <strong>GPT-SÃ©nÃ©gal</strong> : assistant conversationnel culturellement adaptÃ©</li>
                            <li>ğŸ“š <strong>Ã‰ducation</strong> : tuteur IA pour l'apprentissage du franÃ§ais</li>
                            <li>ğŸ›ï¸ <strong>Administration</strong> : traitement automatique de documents officiels</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Ã‰volution des tailles : de BERT-base Ã  GPT-4",
            content: `
                        <p><strong>ğŸ“Š Ã‰volution spectaculaire des modÃ¨les (2018-2024) :</strong></p>
                        
                        <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">ModÃ¨le</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">AnnÃ©e</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">ParamÃ¨tres</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Couches</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Dimension</th>
                            </tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;"><strong>BERT-base</strong></td><td style="padding: 0.5rem; border: 1px solid #ddd;">2018</td><td style="padding: 0.5rem; border: 1px solid #ddd;">110M</td><td style="padding: 0.5rem; border: 1px solid #ddd;">12</td><td style="padding: 0.5rem; border: 1px solid #ddd;">768</td></tr>
                            <tr style="background: #f8f9fa;"><td style="padding: 0.5rem; border: 1px solid #ddd;"><strong>GPT-1</strong></td><td style="padding: 0.5rem; border: 1px solid #ddd;">2018</td><td style="padding: 0.5rem; border: 1px solid #ddd;">117M</td><td style="padding: 0.5rem; border: 1px solid #ddd;">12</td><td style="padding: 0.5rem; border: 1px solid #ddd;">768</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;"><strong>BERT-large</strong></td><td style="padding: 0.5rem; border: 1px solid #ddd;">2018</td><td style="padding: 0.5rem; border: 1px solid #ddd;">340M</td><td style="padding: 0.5rem; border: 1px solid #ddd;">24</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1024</td></tr>
                            <tr style="background: #f8f9fa;"><td style="padding: 0.5rem; border: 1px solid #ddd;"><strong>GPT-2</strong></td><td style="padding: 0.5rem; border: 1px solid #ddd;">2019</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1.5B</td><td style="padding: 0.5rem; border: 1px solid #ddd;">48</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1600</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;"><strong>GPT-3</strong></td><td style="padding: 0.5rem; border: 1px solid #ddd;">2020</td><td style="padding: 0.5rem; border: 1px solid #ddd;">175B</td><td style="padding: 0.5rem; border: 1px solid #ddd;">96</td><td style="padding: 0.5rem; border: 1px solid #ddd;">12288</td></tr>
                            <tr style="background: #f8f9fa;"><td style="padding: 0.5rem; border: 1px solid #ddd;"><strong>GPT-4</strong></td><td style="padding: 0.5rem; border: 1px solid #ddd;">2023</td><td style="padding: 0.5rem; border: 1px solid #ddd;">~1.7T</td><td style="padding: 0.5rem; border: 1px solid #ddd;">~120</td><td style="padding: 0.5rem; border: 1px solid #ddd;">~20000</td></tr>
                        </table>
                        
                        <p><strong>ğŸ“ˆ Croissance exponentielle :</strong></p>
                        <ul>
                            <li>ğŸš€ <strong>ParamÃ¨tres</strong> : Ã—15 000 en 5 ans (110M â†’ 1.7T)</li>
                            <li>ğŸ§  <strong>CapacitÃ©s</strong> : de la classification simple au raisonnement complexe</li>
                            <li>ğŸ’° <strong>CoÃ»t d'entraÃ®nement</strong> : de 1000$ Ã  100M$</li>
                            <li>âš¡ <strong>Puissance de calcul</strong> : de 1 GPU Ã  25 000 GPU</li>
                        </ul>
                        
                        <p><strong>ğŸ” Lois d'Ã©chelle (Scaling Laws) :</strong></p>
                        <p>$$\\text{Performance} \\propto \\text{ParamÃ¨tres}^{\\alpha} \\times \\text{DonnÃ©es}^{\\beta} \\times \\text{Calcul}^{\\gamma}$$</p>
                        
                        <p><strong>ğŸ’¡ DÃ©couverte clÃ© :</strong> Plus c'est gros, plus c'est intelligent ! (jusqu'Ã  un certain point)</p>
                    `,
          },
          {
            type: "code",
            title: "Calcul de paramÃ¨tres : anatomie d'un Transformer",
            description: "Calculons le nombre de paramÃ¨tres d'un modÃ¨le :",
            code: `def calculer_parametres_transformer(vocab_size, seq_len, hidden_size, num_layers, num_heads):
    """Calcule le nombre total de paramÃ¨tres d'un Transformer"""
    
    print(f"ğŸ§® CALCUL DE PARAMÃˆTRES - TRANSFORMER")
    print("=" * 45)
    print(f"Vocabulaire : {vocab_size:,}")
    print(f"SÃ©quence max : {seq_len}")
    print(f"Dimension cachÃ©e : {hidden_size}")
    print(f"Nombre de couches : {num_layers}")
    print(f"TÃªtes d'attention : {num_heads}")
    print()
    
    # 1. Embeddings
    token_emb = vocab_size * hidden_size
    pos_emb = seq_len * hidden_size
    total_emb = token_emb + pos_emb
    
    print(f"ğŸ“Š EMBEDDINGS :")
    print(f"  Token embeddings : {vocab_size:,} Ã— {hidden_size} = {token_emb:,}")
    print(f"  Position embeddings : {seq_len} Ã— {hidden_size} = {pos_emb:,}")
    print(f"  Total embeddings : {total_emb:,}")
    print()
    
    # 2. Une couche Transformer
    # Attention multi-tÃªtes
    d_k = hidden_size // num_heads
    attention_params = 4 * hidden_size * hidden_size  # Q, K, V, O
    
    # Feed-Forward Network (gÃ©nÃ©ralement 4x plus large)
    ffn_size = 4 * hidden_size
    ffn_params = hidden_size * ffn_size + ffn_size * hidden_size
    
    # Layer Normalization (2 par couche)
    ln_params = 2 * 2 * hidden_size  # 2 LN Ã— (scale + bias)
    
    params_par_couche = attention_params + ffn_params + ln_params
    
    print(f"ğŸ”§ UNE COUCHE TRANSFORMER :")
    print(f"  Multi-Head Attention : 4 Ã— {hidden_size} Ã— {hidden_size} = {attention_params:,}")
    print(f"  Feed-Forward : {hidden_size} Ã— {ffn_size} Ã— 2 = {ffn_params:,}")
    print(f"  Layer Normalization : 2 Ã— 2 Ã— {hidden_size} = {ln_params:,}")
    print(f"  Total par couche : {params_par_couche:,}")
    print()
    
    # 3. Total du modÃ¨le
    total_layers = num_layers * params_par_couche
    
    # 4. TÃªte de sortie (classification/gÃ©nÃ©ration)
    output_head = hidden_size * vocab_size
    
    total_params = total_emb + total_layers + output_head
    
    print(f"ğŸ¯ TOTAL DU MODÃˆLE :")
    print(f"  {num_layers} couches : {total_layers:,}")
    print(f"  TÃªte de sortie : {hidden_size} Ã— {vocab_size:,} = {output_head:,}")
    print(f"  TOTAL GÃ‰NÃ‰RAL : {total_params:,} paramÃ¨tres")
    print()
    
    # Comparaison avec modÃ¨les rÃ©els
    print(f"ğŸ“Š COMPARAISON :")
    if total_params < 1e9:
        print(f"  Votre modÃ¨le : {total_params/1e6:.1f}M paramÃ¨tres")
        print(f"  CatÃ©gorie : ModÃ¨le lÃ©ger (mobile/edge)")
    elif total_params < 10e9:
        print(f"  Votre modÃ¨le : {total_params/1e9:.1f}B paramÃ¨tres")
        print(f"  CatÃ©gorie : ModÃ¨le moyen (serveur)")
    else:
        print(f"  Votre modÃ¨le : {total_params/1e9:.1f}B paramÃ¨tres")
        print(f"  CatÃ©gorie : ModÃ¨le gÃ©ant (datacenter)")
    
    return total_params

# Calculs pour diffÃ©rentes tailles
print("ğŸ” EXEMPLES DE CONFIGURATIONS")
print("=" * 40)

# Configuration BERT-base
params_bert_base = calculer_parametres_transformer(
    vocab_size=30000, seq_len=512, hidden_size=768, 
    num_layers=12, num_heads=12
)

print("\\n" + "="*50)

# Configuration GPT-2 small
params_gpt2_small = calculer_parametres_transformer(
    vocab_size=50000, seq_len=1024, hidden_size=768,
    num_layers=12, num_heads=12
)`,
          },
          {
            type: "code",
            title: "Comparaison pratique : mÃªme tÃ¢che, deux approches",
            description: "Comparons BERT et GPT sur une tÃ¢che de sentiment :",
            code: `class AnalyseSentiment:
    def __init__(self):
        # Phrases de test sÃ©nÃ©galaises
        self.phrases_test = [
            "Dakar est une belle ville moderne",
            "Les embouteillages Ã  Dakar sont terribles",
            "J'adore la cuisine sÃ©nÃ©galaise",
            "Le systÃ¨me de santÃ© doit Ãªtre amÃ©liorÃ©",
            "Les plages de Saly sont magnifiques"
        ]
        
        # Sentiments rÃ©els (pour comparaison)
        self.sentiments_reels = ['positif', 'nÃ©gatif', 'positif', 'nÃ©gatif', 'positif']
    
    def approche_bert(self, phrase):
        """Simulation de l'approche BERT pour sentiment"""
        print(f"ğŸ¤– BERT analyse : '{phrase}'")
        
        # BERT voit toute la phrase d'un coup
        mots_positifs = ['belle', 'moderne', 'adore', 'magnifiques', 'excellent']
        mots_negatifs = ['terribles', 'mauvais', 'problÃ¨me', 'amÃ©liorer']
        
        score_positif = sum(1 for mot in mots_positifs if mot in phrase.lower())
        score_negatif = sum(1 for mot in mots_negatifs if mot in phrase.lower())
        
        print(f"  Mots positifs dÃ©tectÃ©s : {score_positif}")
        print(f"  Mots nÃ©gatifs dÃ©tectÃ©s : {score_negatif}")
        
        if score_positif > score_negatif:
            prediction = 'positif'
            confiance = 0.8 + 0.1 * (score_positif - score_negatif)
        elif score_negatif > score_positif:
            prediction = 'nÃ©gatif'
            confiance = 0.8 + 0.1 * (score_negatif - score_positif)
        else:
            prediction = 'neutre'
            confiance = 0.5
        
        print(f"  ğŸ¯ PrÃ©diction : {prediction} (confiance: {confiance:.2f})")
        return prediction, confiance
    
    def approche_gpt(self, phrase):
        """Simulation de l'approche GPT pour sentiment"""
        print(f"ğŸ’¬ GPT gÃ©nÃ¨re : '{phrase}' â†’ ?")
        
        # GPT gÃ©nÃ¨re une continuation qui rÃ©vÃ¨le le sentiment
        continuations = {
            'positif': [" C'est vraiment formidable", " Je recommande vivement", " Quelle merveille"],
            'nÃ©gatif': [" C'est dÃ©cevant", " Il faut amÃ©liorer", " Quel problÃ¨me"],
            'neutre': [" C'est normal", " Rien d'exceptionnel", " Comme d'habitude"]
        }
        
        # Simulation de gÃ©nÃ©ration
        mots = phrase.lower().split()
        if any(mot in ['belle', 'adore', 'magnifiques'] for mot in mots):
            sentiment = 'positif'
            continuation = continuations['positif'][0]
            confiance = 0.85
        elif any(mot in ['terribles', 'amÃ©liorer'] for mot in mots):
            sentiment = 'nÃ©gatif'
            continuation = continuations['nÃ©gatif'][0]
            confiance = 0.80
        else:
            sentiment = 'neutre'
            continuation = continuations['neutre'][0]
            confiance = 0.60
        
        print(f"  Continuation gÃ©nÃ©rÃ©e : '{continuation}'")
        print(f"  ğŸ¯ Sentiment infÃ©rÃ© : {sentiment} (confiance: {confiance:.2f})")
        return sentiment, confiance

# Test comparatif
analyseur = AnalyseSentiment()

print("ğŸ” COMPARAISON BERT vs GPT - ANALYSE DE SENTIMENT")
print("=" * 60)

for i, phrase in enumerate(analyseur.phrases_test):
    print(f"\\nğŸ“ Phrase {i+1}: '{phrase}'")
    print(f"âœ… Sentiment rÃ©el : {analyseur.sentiments_reels[i]}")
    print()
    
    # Test BERT
    pred_bert, conf_bert = analyseur.approche_bert(phrase)
    
    print()
    
    # Test GPT
    pred_gpt, conf_gpt = analyseur.approche_gpt(phrase)
    
    # Comparaison
    print(f"\\nğŸ“Š Comparaison :")
    print(f"  BERT : {pred_bert} ({conf_bert:.2f})")
    print(f"  GPT  : {pred_gpt} ({conf_gpt:.2f})")
    print(f"  RÃ©el : {analyseur.sentiments_reels[i]}")
    print("-" * 40)`,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Variantes et Ã©volutions : l'arbre gÃ©nÃ©alogique",
            content: `
                        <p><strong>ğŸŒ³ L'arbre gÃ©nÃ©alogique des modÃ¨les de langage :</strong></p>
                        
                        <p><strong>ğŸ¤– Famille BERT (Encodeurs) :</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>BERT</strong> (2018) : l'ancÃªtre rÃ©volutionnaire</li>
                            <li>âš¡ <strong>RoBERTa</strong> (2019) : BERT optimisÃ© (Facebook)</li>
                            <li>ğŸ”¬ <strong>DeBERTa</strong> (2020) : attention dÃ©couplÃ©e (Microsoft)</li>
                            <li>âš¡ <strong>ELECTRA</strong> (2020) : entraÃ®nement plus efficace (Google)</li>
                            <li>ğŸŒ <strong>mBERT</strong> : multilingue (104 langues)</li>
                        </ul>
                        
                        <p><strong>ğŸ’¬ Famille GPT (DÃ©codeurs) :</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>GPT-1</strong> (2018) : preuve de concept</li>
                            <li>ğŸ“ˆ <strong>GPT-2</strong> (2019) : "trop dangereux Ã  publier"</li>
                            <li>ğŸš€ <strong>GPT-3</strong> (2020) : Ã©mergence de l'intelligence</li>
                            <li>ğŸ’¬ <strong>ChatGPT</strong> (2022) : rÃ©volution conversationnelle</li>
                            <li>ğŸ§  <strong>GPT-4</strong> (2023) : intelligence quasi-humaine</li>
                        </ul>
                        
                        <p><strong>ğŸ”„ Hybrides et innovations :</strong></p>
                        <ul>
                            <li>ğŸ­ <strong>T5</strong> : "Text-to-Text Transfer Transformer" (Google)</li>
                            <li>ğŸŒŸ <strong>BART</strong> : BERT + GPT combinÃ©s (Facebook)</li>
                            <li>ğŸ¯ <strong>PaLM</strong> : Pathways Language Model (Google)</li>
                            <li>ğŸ¤– <strong>LaMDA</strong> : conversation spÃ©cialisÃ©e (Google)</li>
                            <li>ğŸ¦™ <strong>LLaMA</strong> : efficacitÃ© optimisÃ©e (Meta)</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Tendance actuelle :</strong> Convergence vers des modÃ¨les multimodaux (texte + images + code + audio)</p>
                    `,
          },
          {
            type: "code",
            title: "ImplÃ©mentation : BERT vs GPT head-to-head",
            description: "Comparons les deux architectures cÃ´te Ã  cÃ´te :",
            code: `class BERTvsGPT:
    def __init__(self, vocab_size=100, hidden_size=32):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        
        # Vocabulaire partagÃ©
        self.vocab = {
            'le': 0, 'senegal': 1, 'est': 2, 'un': 3, 'pays': 4,
            'afrique': 5, 'ouest': 6, 'dakar': 7, 'capitale': 8,
            '[MASK]': 9, '[CLS]': 10, '[SEP]': 11
        }
        self.id_to_token = {v: k for k, v in self.vocab.items()}
        
        print("ğŸ¤– Comparateur BERT vs GPT initialisÃ©")
    
    def attention_bert(self, embeddings):
        """Attention bidirectionnelle (BERT)"""
        seq_len = embeddings.shape[0]
        
        # Pas de masque - attention complÃ¨te
        scores = np.random.random((seq_len, seq_len))
        attention_weights = scores / scores.sum(axis=1, keepdims=True)
        
        print("ğŸ¤– BERT - Attention bidirectionnelle :")
        print("Matrice d'attention (chaque ligne = un token) :")
        for i in range(seq_len):
            print(f"  Token {i}: {attention_weights[i].round(2)}")
        
        return attention_weights
    
    def attention_gpt(self, embeddings):
        """Attention causale (GPT)"""
        seq_len = embeddings.shape[0]
        
        # Masque causal - triangulaire infÃ©rieur
        scores = np.random.random((seq_len, seq_len))
        
        # Application du masque causal
        mask = np.triu(np.ones((seq_len, seq_len)), k=1)
        scores_masques = np.where(mask == 1, -np.inf, scores)
        
        # Softmax avec masque
        attention_weights = np.exp(scores_masques) / np.sum(np.exp(scores_masques), axis=1, keepdims=True)
        
        print("ğŸ’¬ GPT - Attention causale :")
        print("Matrice d'attention (triangulaire infÃ©rieure) :")
        for i in range(seq_len):
            print(f"  Token {i}: {attention_weights[i].round(2)}")
        
        return attention_weights
    
    def comparer_sur_phrase(self, phrase):
        """Compare BERT et GPT sur la mÃªme phrase"""
        print(f"\\nğŸ“ Phrase d'analyse : '{phrase}'")
        print("=" * 50)
        
        # Tokenisation
        tokens = phrase.lower().split()
        token_ids = [self.vocab.get(token, 0) for token in tokens]
        
        print(f"Tokens : {tokens}")
        print(f"IDs : {token_ids}")
        
        # Embeddings simulÃ©s
        embeddings = np.random.normal(0, 0.1, (len(tokens), self.hidden_size))
        
        print(f"\\nğŸ” COMPARAISON DES ATTENTIONS :")
        print("-" * 30)
        
        # BERT
        attention_bert = self.attention_bert(embeddings)
        
        print()
        
        # GPT
        attention_gpt = self.attention_gpt(embeddings)
        
        return attention_bert, attention_gpt

# Test comparatif
comparateur = BERTvsGPT()
att_bert, att_gpt = comparateur.comparer_sur_phrase("le senegal est un pays")

print(f"\\nğŸ’¡ OBSERVATIONS :")
print(f"ğŸ¤– BERT : Chaque token voit tous les autres (matrice pleine)")
print(f"ğŸ’¬ GPT : Chaque token ne voit que les prÃ©cÃ©dents (triangulaire)")`,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : conception d'architectures spÃ©cialisÃ©es",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Pour chaque application, choisissez BERT ou GPT et justifiez votre choix :</p>
                        
                        <ol>
                            <li><strong>DÃ©tection de fake news</strong> en wolof</li>
                            <li><strong>Assistant conversationnel</strong> pour l'administration sÃ©nÃ©galaise</li>
                            <li><strong>Traduction automatique</strong> franÃ§ais â†” wolof</li>
                            <li><strong>GÃ©nÃ©ration d'articles</strong> de presse automatique</li>
                            <li><strong>Classification de documents</strong> juridiques</li>
                            <li><strong>Chatbot Ã©ducatif</strong> pour l'apprentissage du franÃ§ais</li>
                            <li><strong>Analyse de sentiment</strong> sur les rÃ©seaux sociaux</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('architecture-choice-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="architecture-choice-exercise" style="display: none;">
                        <ol>
                            <li><strong>DÃ©tection fake news :</strong> <strong>BERT</strong><br>
                                <em>Justification :</em> Besoin d'analyser tout le contexte pour dÃ©tecter les incohÃ©rences</li>
                            <li><strong>Assistant conversationnel :</strong> <strong>GPT</strong><br>
                                <em>Justification :</em> GÃ©nÃ©ration de rÃ©ponses naturelles et fluides</li>
                            <li><strong>Traduction :</strong> <strong>Hybride (Encoder-Decoder)</strong><br>
                                <em>Justification :</em> BERT pour comprendre + GPT pour gÃ©nÃ©rer</li>
                            <li><strong>GÃ©nÃ©ration d'articles :</strong> <strong>GPT</strong><br>
                                <em>Justification :</em> CrÃ©ation de contenu long et cohÃ©rent</li>
                            <li><strong>Classification juridique :</strong> <strong>BERT</strong><br>
                                <em>Justification :</em> Analyse prÃ©cise de documents complets</li>
                            <li><strong>Chatbot Ã©ducatif :</strong> <strong>GPT</strong><br>
                                <em>Justification :</em> Interaction naturelle et gÃ©nÃ©ration d'explications</li>
                            <li><strong>Analyse sentiment :</strong> <strong>BERT</strong><br>
                                <em>Justification :</em> Classification prÃ©cise avec contexte complet</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "code",
            title: "Simulation d'entraÃ®nement : MLM vs CLM",
            description: "Simulons les deux stratÃ©gies d'entraÃ®nement :",
            code: `def simuler_entrainement_mlm():
    """Simule l'entraÃ®nement BERT avec Masked Language Modeling"""
    print("ğŸ¤– SIMULATION ENTRAÃNEMENT BERT (MLM)")
    print("=" * 45)
    
    phrases = [
        "dakar est la capitale du senegal",
        "le senegal est en afrique ouest",
        "la cuisine senegalaise est delicieuse"
    ]
    
    for i, phrase in enumerate(phrases):
        print(f"\\nÃ‰poque {i+1} - Phrase : '{phrase}'")
        
        # Masquage alÃ©atoire (15% des tokens)
        mots = phrase.split()
        nb_masques = max(1, len(mots) // 4)  # ~25% pour la dÃ©mo
        positions_masquees = np.random.choice(len(mots), nb_masques, replace=False)
        
        phrase_masquee = mots.copy()
        mots_cibles = []
        
        for pos in positions_masquees:
            mots_cibles.append(mots[pos])
            phrase_masquee[pos] = '[MASK]'
        
        print(f"  Phrase masquÃ©e : '{' '.join(phrase_masquee)}'")
        print(f"  Cibles Ã  prÃ©dire : {mots_cibles}")
        
        # Simulation de prÃ©diction
        predictions = []
        for j, mot_cible in enumerate(mots_cibles):
            # BERT utilise le contexte complet pour prÃ©dire
            if mot_cible in ['dakar', 'senegal']:
                prediction = mot_cible  # Bonne prÃ©diction
                loss = 0.1
            else:
                prediction = 'autre'  # PrÃ©diction approximative
                loss = 0.8
            
            predictions.append(prediction)
            print(f"    [MASK] â†’ '{prediction}' (loss: {loss:.1f})")
        
        loss_moyenne = np.mean([0.1 if p in mots_cibles else 0.8 for p in predictions])
        print(f"  ğŸ“Š Loss moyenne : {loss_moyenne:.2f}")

def simuler_entrainement_clm():
    """Simule l'entraÃ®nement GPT avec Causal Language Modeling"""
    print("\\nğŸ’¬ SIMULATION ENTRAÃNEMENT GPT (CLM)")
    print("=" * 45)
    
    phrases = [
        "dakar est la capitale du senegal",
        "le senegal est en afrique ouest", 
        "la cuisine senegalaise est delicieuse"
    ]
    
    for i, phrase in enumerate(phrases):
        print(f"\\nÃ‰poque {i+1} - Phrase : '{phrase}'")
        
        mots = phrase.split()
        
        # PrÃ©diction sÃ©quentielle
        for j in range(1, len(mots)):
            contexte = ' '.join(mots[:j])
            mot_cible = mots[j]
            
            # GPT prÃ©dit basÃ© seulement sur le contexte prÃ©cÃ©dent
            if j == 1:  # AprÃ¨s "dakar"
                prediction = 'est'
                loss = 0.2
            elif 'capitale' in contexte:
                prediction = 'du'
                loss = 0.1
            else:
                prediction = 'autre'
                loss = 0.7
            
            print(f"  '{contexte}' â†’ '{mot_cible}' (prÃ©dit: '{prediction}', loss: {loss:.1f})")
        
        loss_moyenne = 0.3  # Simulation
        print(f"  ğŸ“Š Loss moyenne : {loss_moyenne:.2f}")

# ExÃ©cution des simulations
simuler_entrainement_mlm()
simuler_entrainement_clm()

print(f"\\nğŸ’¡ DIFFÃ‰RENCES CLÃ‰S :")
print(f"ğŸ¤– BERT : Apprend Ã  'comprendre' en devinant les mots cachÃ©s")
print(f"ğŸ’¬ GPT : Apprend Ã  'gÃ©nÃ©rer' en prÃ©disant la suite logique")`,
          },
          {
            type: "warning",
            icon: "âš ï¸",
            title: "L'hÃ©ritage rÃ©volutionnaire : de 2018 Ã  aujourd'hui",
            content: `
                        <p><strong>ğŸŒŸ BERT et GPT ont crÃ©Ã© l'IA moderne que nous connaissons :</strong></p>
                        
                        <p><strong>ğŸ¤– HÃ©ritage de BERT :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>Google Search</strong> : comprÃ©hension des requÃªtes complexes</li>
                            <li>ğŸ“§ <strong>Gmail</strong> : classification automatique des emails</li>
                            <li>ğŸŒ <strong>Traduction</strong> : Google Translate plus prÃ©cis</li>
                            <li>ğŸ“Š <strong>Analyse de donnÃ©es</strong> : extraction d'insights textuels</li>
                            <li>ğŸ¥ <strong>MÃ©decine</strong> : analyse de dossiers mÃ©dicaux</li>
                        </ul>
                        
                        <p><strong>ğŸ’¬ HÃ©ritage de GPT :</strong></p>
                        <ul>
                            <li>ğŸ¤– <strong>ChatGPT</strong> : rÃ©volution conversationnelle mondiale</li>
                            <li>ğŸ’» <strong>GitHub Copilot</strong> : programmation assistÃ©e par IA</li>
                            <li>âœï¸ <strong>Jasper, Copy.ai</strong> : gÃ©nÃ©ration de contenu marketing</li>
                            <li>ğŸ¨ <strong>CrÃ©ativitÃ©</strong> : Ã©criture, brainstorming, scÃ©narios</li>
                            <li>ğŸ“š <strong>Ã‰ducation</strong> : tuteurs IA personnalisÃ©s</li>
                        </ul>
                        
                        <p><strong>ğŸŒ Impact sociÃ©tal quantifiÃ© :</strong></p>
                        <ul>
                            <li>ğŸ’° <strong>Ã‰conomie</strong> : 4.4T$ de valeur ajoutÃ©e prÃ©vue d'ici 2030</li>
                            <li>ğŸ‘¥ <strong>Emplois</strong> : 300M d'emplois transformÃ©s</li>
                            <li>ğŸ“ <strong>Ã‰ducation</strong> : 1B d'Ã©tudiants avec accÃ¨s Ã  des tuteurs IA</li>
                            <li>ğŸ¥ <strong>SantÃ©</strong> : diagnostic IA dans 50% des hÃ´pitaux</li>
                            <li>ğŸŒ <strong>Langues</strong> : prÃ©servation de 1000+ langues menacÃ©es</li>
                        </ul>
                        
                        <p><strong>ğŸ‡¸ğŸ‡³ OpportunitÃ©s pour le SÃ©nÃ©gal :</strong></p>
                        <ul>
                            <li>ğŸ—£ï¸ <strong>Langues nationales</strong> : BERT-Wolof, GPT-Pulaar</li>
                            <li>ğŸ“š <strong>Ã‰ducation</strong> : tuteurs IA adaptÃ©s au contexte local</li>
                            <li>ğŸ›ï¸ <strong>Administration</strong> : automatisation des services publics</li>
                            <li>ğŸŒ¾ <strong>Agriculture</strong> : conseils IA pour les agriculteurs</li>
                            <li>ğŸ’° <strong>Finance</strong> : inclusion financiÃ¨re par l'IA</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Point clÃ© :</strong> BERT et GPT ne sont pas juste des modÃ¨les - ils ont crÃ©Ã© un <strong>nouveau paradigme</strong> oÃ¹ l'IA comprend et gÃ©nÃ¨re du langage naturel. Ils sont les ancÃªtres de toute l'IA conversationnelle moderne !</p>
                        
                        <p><strong>ğŸ”® Prochaine Ã©tape :</strong> Fine-tuning - comment adapter ces gÃ©ants Ã  vos besoins spÃ©cifiques !</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ğŸ¤” Quelle est la diffÃ©rence principale entre l'entraÃ®nement de BERT et de GPT ?",
          options: [
            "A) BERT utilise plus de donnÃ©es",
            "B) BERT prÃ©dit des mots masquÃ©s, GPT prÃ©dit le mot suivant",
            "C) GPT est plus rapide Ã  entraÃ®ner",
            "D) BERT a plus de paramÃ¨tres",
          ],
          correct: 1,
          explanation:
            "BERT utilise le Masked Language Modeling (prÃ©dire des mots cachÃ©s avec contexte bidirectionnel) tandis que GPT utilise le Causal Language Modeling (prÃ©dire le mot suivant avec contexte unidirectionnel). Cette diffÃ©rence fondamentale dÃ©termine leurs capacitÃ©s respectives.",
        },
        prevModule: "transformers.html",
        nextModule: "fine-tuning.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
