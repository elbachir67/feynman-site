<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Les Gradients | IA4Ndada</title>

    <!-- MathJax pour les formules math√©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">üè† Accueil</a>
          <span>‚Ä∫</span>
          <span>üßÆ Math√©matiques</span>
          <span>‚Ä∫</span>
          <span>Les Gradients</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>üèîÔ∏è Les Gradients</h1>
      <p class="subtitle">Module 1.5 - La boussole de l'IA pour trouver le chemin optimal</p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>üéØ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajout√©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajout√© dynamiquement -->
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="derivatives.html" class="nav-link" id="prev-link"
          >‚Üê Module pr√©c√©dent : D√©riv√©es</a
        >
        <a href="probability.html" class="nav-link" id="next-link"
          >Module suivant : Probabilit√©s ‚Üí</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module Gradients
      const moduleConfig = {
        id: "math-gradients",
        title: "Les Gradients",
        category: "Math√©matiques",
        objectives: [
          "Comprendre les d√©riv√©es partielles",
          "Calculer le gradient d'une fonction √† plusieurs variables",
          "Comprendre la descente de gradient",
          "Ma√Ætriser le taux d'apprentissage",
          "Impl√©menter la descente de gradient en Python",
          "Visualiser l'optimisation en 3D",
        ],
        content: [
          // ===== SECTION 1: DES D√âRIV√âES AUX GRADIENTS =====
          {
            type: "concept",
            icon: "üí°",
            title: "Des d√©riv√©es aux gradients",
            content: `
              <p>Dans le module pr√©c√©dent, nous avons vu les d√©riv√©es pour des fonctions d'<strong>une seule variable</strong>. Mais en IA, les mod√®les ont souvent <strong>des millions de param√®tres</strong> !</p>

              <p><strong>üéØ Le probl√®me :</strong></p>
              <p>Comment trouver la direction optimale quand on a plusieurs variables √† ajuster simultan√©ment ?</p>

              <p><strong>üí° La solution : le gradient !</strong></p>
              <p>Le <strong>gradient</strong> est un vecteur qui rassemble toutes les d√©riv√©es partielles d'une fonction. Il pointe vers la direction de la plus forte mont√©e.</p>

              <p><strong>üìê Notation :</strong></p>
              <p>Pour une fonction \\(f(x, y)\\) :</p>
              <p>$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$</p>

              <p><strong>üîë Point cl√© :</strong></p>
              <ul>
                <li>Le gradient est un <strong>vecteur</strong>, pas un nombre</li>
                <li>Sa direction indique o√π la fonction augmente le plus vite</li>
                <li>Sa norme (longueur) indique la "raideur" de la pente</li>
              </ul>
            `,
          },
          {
            type: "exercise",
            icon: "‚úèÔ∏è",
            title: "Exercice 1 : Comprendre le gradient",
            content: `
              <p>Si le gradient d'une fonction au point \\((2, 3)\\) est \\(\\nabla f = [4, -2]\\), dans quelle direction la fonction augmente-t-elle le plus rapidement ?</p>
            `,
            exerciseType: "mcq",
            options: [
              "Dans la direction [4, -2] (vers la droite et vers le bas)",
              "Dans la direction [-4, 2] (vers la gauche et vers le haut)",
              "Dans la direction [2, 3] (vers le point lui-m√™me)",
              "La fonction n'augmente pas"
            ],
            correctAnswer: 0,
            explanation: "Le gradient \\([4, -2]\\) pointe directement vers la direction de la plus forte mont√©e. Ici, la fonction augmente le plus vite en allant vers la droite (\\(+4\\)) et vers le bas (\\(-2\\)).",
            completesObjective: 0,
          },

          // ===== SECTION 2: D√âRIV√âES PARTIELLES =====
          {
            type: "mathematique",
            icon: "‚àÇ",
            title: "Les d√©riv√©es partielles",
            content: `
              <p>Une <strong>d√©riv√©e partielle</strong> mesure comment une fonction varie quand on change <strong>une seule variable</strong>, en gardant les autres fixes.</p>

              <p><strong>üìù M√©thode de calcul :</strong></p>
              <ol>
                <li>Choisir la variable par rapport √† laquelle on d√©rive</li>
                <li>Traiter toutes les autres variables comme des <strong>constantes</strong></li>
                <li>Appliquer les r√®gles de d√©rivation habituelles</li>
              </ol>

              <p><strong>üìê Exemple d√©taill√© :</strong></p>
              <p>Soit \\(f(x, y) = x^2 + 3xy + y^2\\)</p>

              <p><strong>D√©riv√©e par rapport √† \\(x\\)</strong> (on traite \\(y\\) comme une constante) :</p>
              <p>$$\\frac{\\partial f}{\\partial x} = 2x + 3y + 0 = 2x + 3y$$</p>

              <p><strong>D√©riv√©e par rapport √† \\(y\\)</strong> (on traite \\(x\\) comme une constante) :</p>
              <p>$$\\frac{\\partial f}{\\partial y} = 0 + 3x + 2y = 3x + 2y$$</p>

              <p><strong>Le gradient complet :</strong></p>
              <p>$$\\nabla f = \\begin{bmatrix} 2x + 3y \\\\ 3x + 2y \\end{bmatrix}$$</p>
            `,
          },
          {
            type: "exercise",
            icon: "‚úèÔ∏è",
            title: "Exercice 2 : Calculer une d√©riv√©e partielle",
            content: `
              <p>Soit \\(f(x, y) = 2x^2 + 5y\\).</p>
              <p>Calculez \\(\\frac{\\partial f}{\\partial x}\\) (la d√©riv√©e partielle par rapport √† \\(x\\)).</p>
              <p><em>Quel est le coefficient devant \\(x\\) dans le r√©sultat ?</em></p>
            `,
            exerciseType: "numeric",
            correctAnswer: 4,
            tolerance: 0,
            explanation: "On traite \\(y\\) comme une constante. La d√©riv√©e de \\(2x^2\\) est \\(4x\\), et la d√©riv√©e de \\(5y\\) (constante par rapport √† \\(x\\)) est \\(0\\). Donc \\(\\frac{\\partial f}{\\partial x} = 4x\\).",
            completesObjective: 0,
          },
          {
            type: "exercise",
            icon: "‚úèÔ∏è",
            title: "Exercice 3 : Calculer un gradient complet",
            content: `
              <p>Soit \\(f(x, y) = x^2 + y^2\\).</p>
              <p>Calculez le gradient \\(\\nabla f\\) au point \\((3, 4)\\).</p>
              <p><em>Quelle est la norme (longueur) du gradient ? Utilisez \\(||v|| = \\sqrt{v_x^2 + v_y^2}\\)</em></p>
            `,
            exerciseType: "numeric",
            correctAnswer: 10,
            tolerance: 0.01,
            explanation: "Le gradient est \\(\\nabla f = [2x, 2y]\\). Au point \\((3, 4)\\) : \\(\\nabla f = [6, 8]\\). La norme est \\(\\sqrt{6^2 + 8^2} = \\sqrt{36 + 64} = \\sqrt{100} = 10\\).",
            completesObjective: 1,
          },

          // ===== SECTION 3: LA DESCENTE DE GRADIENT =====
          {
            type: "concept",
            icon: "üìâ",
            title: "La descente de gradient : l'algorithme fondamental",
            content: `
              <p>La <strong>descente de gradient</strong> est l'algorithme le plus utilis√© en IA pour optimiser les mod√®les.</p>

              <p><strong>üéØ Objectif :</strong> Trouver les param√®tres qui <strong>minimisent</strong> la fonction de co√ªt.</p>

              <p><strong>üìã Algorithme :</strong></p>
              <ol>
                <li>Partir d'un point initial (souvent al√©atoire)</li>
                <li>Calculer le gradient √† ce point</li>
                <li>Se d√©placer dans la direction <strong>oppos√©e</strong> au gradient</li>
                <li>R√©p√©ter jusqu'√† convergence</li>
              </ol>

              <p><strong>üìê Formule de mise √† jour :</strong></p>
              <p>$$\\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla J(\\theta)$$</p>

              <p><strong>üîë Param√®tres :</strong></p>
              <ul>
                <li>\\(\\theta\\) = les param√®tres du mod√®le (un vecteur)</li>
                <li>\\(\\alpha\\) = le <strong>taux d'apprentissage</strong> (learning rate)</li>
                <li>\\(\\nabla J(\\theta)\\) = le gradient de la fonction de co√ªt</li>
              </ul>

              <p><strong>üí° Pourquoi le signe moins ?</strong></p>
              <p>Le gradient pointe vers la mont√©e. Pour descendre (minimiser), on va dans le sens oppos√© !</p>
            `,
          },
          {
            type: "exercise",
            icon: "‚úèÔ∏è",
            title: "Exercice 4 : Une √©tape de descente de gradient",
            content: `
              <p>On a les param√®tres \\(\\theta = [2, 3]\\) et le gradient \\(\\nabla J = [4, -2]\\).</p>
              <p>Avec un taux d'apprentissage \\(\\alpha = 0.5\\), calculez les nouveaux param√®tres.</p>
              <p><em>Quelle est la premi√®re composante de \\(\\theta_{new}\\) ?</em></p>
            `,
            exerciseType: "numeric",
            correctAnswer: 0,
            tolerance: 0.01,
            explanation: "\\(\\theta_{new} = [2, 3] - 0.5 \\times [4, -2] = [2 - 2, 3 + 1] = [0, 4]\\). La premi√®re composante est donc 0.",
            completesObjective: 2,
          },

          // ===== SECTION 4: LE TAUX D'APPRENTISSAGE =====
          {
            type: "concept",
            icon: "‚öôÔ∏è",
            title: "Le taux d'apprentissage : un param√®tre crucial",
            content: `
              <p>Le <strong>taux d'apprentissage</strong> (\\(\\alpha\\)) contr√¥le la taille des pas qu'on fait √† chaque it√©ration.</p>

              <p><strong>‚ö†Ô∏è Les risques :</strong></p>

              <p><strong>Si \\(\\alpha\\) est trop petit :</strong></p>
              <ul>
                <li>Les pas sont minuscules</li>
                <li>La convergence est tr√®s lente</li>
                <li>On peut rester bloqu√© dans un minimum local</li>
              </ul>

              <p><strong>Si \\(\\alpha\\) est trop grand :</strong></p>
              <ul>
                <li>Les pas sont trop grands</li>
                <li>On peut "sauter" par-dessus le minimum</li>
                <li>L'algorithme peut diverger (exploser) !</li>
              </ul>

              <p><strong>üéØ Valeurs typiques :</strong></p>
              <ul>
                <li>\\(\\alpha = 0.001\\) √† \\(0.1\\) sont courants</li>
                <li>On commence souvent par \\(\\alpha = 0.01\\)</li>
                <li>On peut diminuer \\(\\alpha\\) au fil de l'entra√Ænement</li>
              </ul>

              <p><strong>üí° Conseil pratique :</strong> Si votre mod√®le "explose" (valeurs NaN ou tr√®s grandes), r√©duisez le taux d'apprentissage !</p>
            `,
          },
          {
            type: "exercise",
            icon: "‚úèÔ∏è",
            title: "Exercice 5 : Impact du taux d'apprentissage",
            content: `
              <p>On minimise \\(f(x) = x^2\\) avec la descente de gradient. On part de \\(x = 10\\).</p>
              <p>La d√©riv√©e est \\(f'(x) = 2x\\), donc au point initial \\(f'(10) = 20\\).</p>
              <p>Avec \\(\\alpha = 0.1\\), quelle est la valeur de \\(x\\) apr√®s une it√©ration ?</p>
            `,
            exerciseType: "numeric",
            correctAnswer: 8,
            tolerance: 0.01,
            explanation: "\\(x_{new} = x - \\alpha \\cdot f'(x) = 10 - 0.1 \\times 20 = 10 - 2 = 8\\). On s'est rapproch√© du minimum (\\(x = 0\\)).",
            completesObjective: 3,
          },
          {
            type: "exercise",
            icon: "‚úèÔ∏è",
            title: "Exercice 6 : Taux d'apprentissage trop grand",
            content: `
              <p>M√™me situation : \\(f(x) = x^2\\), \\(x = 10\\), \\(f'(10) = 20\\).</p>
              <p>Mais cette fois avec \\(\\alpha = 0.6\\). Quelle est la valeur de \\(x\\) apr√®s une it√©ration ?</p>
              <p><em>Observez : on s'√©loigne du minimum !</em></p>
            `,
            exerciseType: "numeric",
            correctAnswer: -2,
            tolerance: 0.01,
            explanation: "\\(x_{new} = 10 - 0.6 \\times 20 = 10 - 12 = -2\\). On a saut√© de l'autre c√¥t√© du minimum ! Avec un \\(\\alpha\\) encore plus grand, l'algorithme divergerait.",
            completesObjective: 3,
          },

          // ===== SECTION 5: VISUALISATION PYTHON =====
          {
            type: "code",
            icon: "üêç",
            title: "Visualiser la descente de gradient en 2D",
            content: `
              <p>Ce code montre la descente de gradient sur une fonction √† deux variables :</p>
            `,
            code: `import numpy as np
import matplotlib.pyplot as plt

# Fonction √† minimiser : f(x,y) = x¬≤ + y¬≤
def f(x, y):
    return x**2 + y**2

# Gradient : ‚àáf = [2x, 2y]
def gradient(x, y):
    return np.array([2*x, 2*y])

# Param√®tres
alpha = 0.1  # Taux d'apprentissage
n_iterations = 20
start = np.array([4.0, 3.0])  # Point de d√©part

# Descente de gradient
path = [start.copy()]
point = start.copy()

for i in range(n_iterations):
    grad = gradient(point[0], point[1])
    point = point - alpha * grad
    path.append(point.copy())

path = np.array(path)

# Visualisation
fig, ax = plt.subplots(figsize=(10, 8))

# Contours de la fonction
x = np.linspace(-5, 5, 100)
y = np.linspace(-5, 5, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

contours = ax.contour(X, Y, Z, levels=20, cmap='coolwarm')
ax.clabel(contours, inline=True, fontsize=8)

# Chemin de la descente
ax.plot(path[:, 0], path[:, 1], 'go-', linewidth=2, markersize=8, label='Chemin de descente')
ax.scatter(path[0, 0], path[0, 1], color='red', s=200, zorder=5, label='D√©part')
ax.scatter(path[-1, 0], path[-1, 1], color='blue', s=200, zorder=5, marker='*', label='Arriv√©e')
ax.scatter(0, 0, color='purple', s=300, marker='x', linewidths=3, label='Minimum global')

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title(f'Descente de gradient sur f(x,y) = x¬≤ + y¬≤\\nŒ± = {alpha}, {n_iterations} it√©rations')
ax.legend()
ax.grid(True, alpha=0.3)
ax.set_aspect('equal')

plt.tight_layout()
plt.savefig('gradient_2d.png', dpi=100, bbox_inches='tight')
plt.show()

print(f"üéØ Point de d√©part : ({path[0, 0]:.2f}, {path[0, 1]:.2f})")
print(f"üìç Point d'arriv√©e : ({path[-1, 0]:.4f}, {path[-1, 1]:.4f})")
print(f"üìâ Valeur finale : f = {f(path[-1, 0], path[-1, 1]):.6f}")`,
          },
          {
            type: "exercise-code",
            icon: "üíª",
            title: "Exercice 7 : Impl√©menter la descente de gradient",
            content: `
              <p>Impl√©mentez la descente de gradient pour minimiser \\(f(x, y) = (x-2)^2 + (y-1)^2\\).</p>
              <p>Le minimum est en \\((2, 1)\\). Partez de \\((0, 0)\\) et v√©rifiez la convergence.</p>
            `,
            starterCode: `import numpy as np

# Fonction √† minimiser : f(x,y) = (x-2)¬≤ + (y-1)¬≤
# Le minimum est en (2, 1) o√π f = 0

# TODO: Calculer le gradient de f
# Rappel : gradient = [d√©riv√©e par rapport √† x, d√©riv√©e par rapport √† y]
def gradient(x, y):
    df_dx = ___  # D√©riv√©e de (x-2)¬≤ + (y-1)¬≤ par rapport √† x
    df_dy = ___  # D√©riv√©e de (x-2)¬≤ + (y-1)¬≤ par rapport √† y
    return np.array([df_dx, df_dy])

# Param√®tres
alpha = 0.1
n_iterations = 50
x, y = 0.0, 0.0  # Point de d√©part

# TODO: Boucle de descente de gradient
for i in range(n_iterations):
    grad = gradient(x, y)
    x = ___  # Mise √† jour de x
    y = ___  # Mise √† jour de y

print(f"Position finale : ({x:.4f}, {y:.4f})")
print(f"Position attendue : (2.0, 1.0)")`,
            solution: `import numpy as np
import matplotlib.pyplot as plt

# Fonction √† minimiser : f(x,y) = (x-2)¬≤ + (y-1)¬≤
def f(x, y):
    return (x-2)**2 + (y-1)**2

# Gradient de f
def gradient(x, y):
    df_dx = 2*(x-2)  # D√©riv√©e de (x-2)¬≤ par rapport √† x
    df_dy = 2*(y-1)  # D√©riv√©e de (y-1)¬≤ par rapport √† y
    return np.array([df_dx, df_dy])

# Param√®tres
alpha = 0.1
n_iterations = 50
x, y = 0.0, 0.0  # Point de d√©part

# Historique pour visualisation
history = [(x, y)]

# Boucle de descente de gradient
for i in range(n_iterations):
    grad = gradient(x, y)
    x = x - alpha * grad[0]  # Mise √† jour de x
    y = y - alpha * grad[1]  # Mise √† jour de y
    history.append((x, y))

print(f"Position finale : ({x:.4f}, {y:.4f})")
print(f"Position attendue : (2.0, 1.0)")
print(f"Erreur : {np.sqrt((x-2)**2 + (y-1)**2):.6f}")

# Visualisation
history = np.array(history)
fig, ax = plt.subplots(figsize=(8, 8))

# Contours
xg = np.linspace(-1, 4, 100)
yg = np.linspace(-1, 3, 100)
X, Y = np.meshgrid(xg, yg)
Z = f(X, Y)
ax.contour(X, Y, Z, levels=15, cmap='coolwarm')

# Chemin
ax.plot(history[:, 0], history[:, 1], 'go-', markersize=6, label='Chemin')
ax.scatter(0, 0, color='red', s=150, zorder=5, label='D√©part (0,0)')
ax.scatter(2, 1, color='purple', s=200, marker='*', zorder=5, label='Minimum (2,1)')

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('Descente de gradient vers (2, 1)')
ax.legend()
ax.grid(True, alpha=0.3)
ax.set_aspect('equal')
plt.tight_layout()
plt.savefig('gradient_exercise.png', dpi=100, bbox_inches='tight')
plt.show()`,
            expectedOutput: "Position finale : (1.99",
            completesObjective: 4,
          },

          // ===== SECTION 6: SURFACE 3D =====
          {
            type: "code",
            icon: "üé®",
            title: "Visualisation 3D de la surface d'erreur",
            content: `
              <p>Cette visualisation en 3D montre comment l'algorithme "descend" sur la surface de la fonction de co√ªt :</p>
            `,
            code: `import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Fonction : f(x,y) = x¬≤ + y¬≤
def f(x, y):
    return x**2 + y**2

def gradient(x, y):
    return np.array([2*x, 2*y])

# Descente de gradient
alpha = 0.15
start = np.array([4.0, 3.0])
point = start.copy()
path = [start.copy()]

for _ in range(25):
    point = point - alpha * gradient(point[0], point[1])
    path.append(point.copy())
path = np.array(path)

# Cr√©ation de la figure 3D
fig = plt.figure(figsize=(12, 5))

# Vue 3D
ax1 = fig.add_subplot(121, projection='3d')
x = np.linspace(-5, 5, 50)
y = np.linspace(-5, 5, 50)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

ax1.plot_surface(X, Y, Z, cmap='coolwarm', alpha=0.7, edgecolor='none')
ax1.plot(path[:, 0], path[:, 1], f(path[:, 0], path[:, 1]),
         'ko-', linewidth=2, markersize=5, label='Descente')
ax1.scatter(path[0, 0], path[0, 1], f(path[0, 0], path[0, 1]),
            color='red', s=100, label='D√©part')
ax1.scatter(0, 0, 0, color='green', s=200, marker='*', label='Minimum')

ax1.set_xlabel('x')
ax1.set_ylabel('y')
ax1.set_zlabel('f(x,y)')
ax1.set_title('Vue 3D de la descente de gradient')
ax1.legend()

# Vue de dessus (contours)
ax2 = fig.add_subplot(122)
contours = ax2.contourf(X, Y, Z, levels=20, cmap='coolwarm', alpha=0.8)
plt.colorbar(contours, ax=ax2, label='f(x,y)')
ax2.plot(path[:, 0], path[:, 1], 'ko-', linewidth=2, markersize=5)
ax2.scatter(path[0, 0], path[0, 1], color='red', s=100, zorder=5, label='D√©part')
ax2.scatter(0, 0, color='green', s=200, marker='*', zorder=5, label='Minimum')
ax2.set_xlabel('x')
ax2.set_ylabel('y')
ax2.set_title('Vue de dessus (lignes de niveau)')
ax2.legend()
ax2.set_aspect('equal')

plt.tight_layout()
plt.savefig('gradient_3d.png', dpi=100, bbox_inches='tight')
plt.show()

print("üèîÔ∏è La surface 3D repr√©sente la fonction de co√ªt")
print("üìâ L'algorithme descend naturellement vers le minimum (point vert)")`,
          },
          {
            type: "exercise-code",
            icon: "üíª",
            title: "Exercice 8 : Comparer diff√©rents taux d'apprentissage",
            content: `
              <p>Comparez l'effet de diff√©rents taux d'apprentissage sur la convergence.</p>
              <p>Observez comment un \\(\\alpha\\) trop grand fait "rebondir" l'algorithme.</p>
            `,
            starterCode: `import numpy as np
import matplotlib.pyplot as plt

def f(x):
    return x**2

def gradient(x):
    return 2*x

# TODO: Tester 3 taux d'apprentissage diff√©rents
alphas = [0.1, 0.5, 0.9]  # Petit, moyen, grand
colors = ['green', 'blue', 'red']
x_start = 4.0
n_iterations = 20

plt.figure(figsize=(10, 6))

for alpha, color in zip(alphas, colors):
    x = x_start
    history = [x]

    # TODO: Impl√©menter la descente de gradient
    for i in range(n_iterations):
        x = ___  # Formule de mise √† jour
        history.append(x)

    plt.plot(history, 'o-', color=color, label=f'Œ± = {alpha}')

plt.axhline(y=0, color='black', linestyle='--', label='Minimum (x=0)')
plt.xlabel('It√©ration')
plt.ylabel('Valeur de x')
plt.title('Effet du taux d apprentissage')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()`,
            solution: `import numpy as np
import matplotlib.pyplot as plt

def f(x):
    return x**2

def gradient(x):
    return 2*x

# Tester 3 taux d'apprentissage diff√©rents
alphas = [0.1, 0.5, 0.9]
colors = ['green', 'blue', 'red']
x_start = 4.0
n_iterations = 20

plt.figure(figsize=(10, 6))

for alpha, color in zip(alphas, colors):
    x = x_start
    history = [x]

    for i in range(n_iterations):
        x = x - alpha * gradient(x)  # Formule de mise √† jour
        history.append(x)

    plt.plot(history, 'o-', color=color, label=f'Œ± = {alpha}', linewidth=2, markersize=4)

plt.axhline(y=0, color='black', linestyle='--', label='Minimum (x=0)')
plt.xlabel('It√©ration')
plt.ylabel('Valeur de x')
plt.title('Effet du taux d\\'apprentissage sur la convergence')
plt.legend()
plt.grid(True, alpha=0.3)
plt.ylim(-5, 5)
plt.tight_layout()
plt.savefig('learning_rates.png', dpi=100, bbox_inches='tight')
plt.show()

print("üìä Observations :")
print("‚Ä¢ Œ± = 0.1 (vert) : Convergence lente mais stable")
print("‚Ä¢ Œ± = 0.5 (bleu) : Convergence rapide et stable")
print("‚Ä¢ Œ± = 0.9 (rouge) : Oscillations ! Presque trop grand")`,
            expectedOutput: "Observations",
            completesObjective: 5,
          },

          // ===== SECTION 7: QUIZ FINAL =====
          {
            type: "quiz",
            icon: "üéØ",
            title: "Quiz final : Les gradients",
            question: "Dans l'entra√Ænement d'un r√©seau de neurones avec des millions de param√®tres, que repr√©sente le gradient ?",
            options: [
              "Un nombre unique indiquant l'erreur totale",
              "Un vecteur avec autant de composantes que de param√®tres, indiquant comment ajuster chacun",
              "La valeur finale des poids apr√®s entra√Ænement",
              "Le nombre d'it√©rations n√©cessaires pour converger"
            ],
            correctAnswer: 1,
            explanation: "Le gradient est un VECTEUR qui contient une d√©riv√©e partielle pour chaque param√®tre du mod√®le. Si on a 1 million de param√®tres, le gradient a 1 million de composantes ! Chaque composante indique comment ajuster le param√®tre correspondant pour r√©duire l'erreur.",
          },
        ],
        prevModule: "derivatives.html",
        nextModule: "probability.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
