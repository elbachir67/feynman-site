<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Les Gradients | IA4Ndada</title>

    <!-- MathJax pour les formules mathÃ©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">ğŸ  Accueil</a>
          <span>â€º</span>
          <span>ğŸ§® MathÃ©matiques</span>
          <span>â€º</span>
          <span>Les Gradients</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>ğŸ”ï¸ Les Gradients</h1>
      <p class="subtitle">Module 1.5 - Analyse multivariable pour l'IA</p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>ğŸ¯ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajoutÃ©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajoutÃ© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>ğŸ‰ Checkpoint - Les Gradients</h3>
        <p>
          FÃ©licitations ! Vous comprenez maintenant comment l'IA trouve
          automatiquement les meilleures solutions.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme complÃ©tÃ©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="derivatives.html" class="nav-link" id="prev-link"
          >â† Module prÃ©cÃ©dent : DÃ©rivÃ©es</a
        >
        <a href="probability.html" class="nav-link" id="next-link"
          >Module suivant : ProbabilitÃ©s â†’</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module Gradients
      const moduleConfig = {
        id: "math-gradients",
        title: "Les Gradients",
        category: "MathÃ©matiques",
        objectives: [
          "Comprendre les gradients comme extension des dÃ©rivÃ©es",
          "Calculer des gradients de fonctions simples",
          "Comprendre la descente de gradient",
          "Voir le lien avec l'apprentissage automatique",
        ],
        content: [
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Concept fondamental",
            content: `
                        <p>Le <strong>gradient</strong> est l'extension des dÃ©rivÃ©es aux fonctions de plusieurs variables. Si la dÃ©rivÃ©e indique la pente d'une courbe, le gradient indique la <strong>direction de la plus forte montÃ©e</strong> sur une surface.</p>
                        <p><strong>ğŸ”‘ IdÃ©e simple :</strong></p>
                        <p>Imaginez une fonction qui dÃ©pend de deux variables : \\(f(x, y)\\). Le gradient nous dit dans quelle direction nous devons nous dÃ©placer pour que la fonction augmente le plus rapidement.</p>
                        <p><strong>ğŸ“ Notation :</strong></p>
                        <p>$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$</p>
                        <p><strong>ğŸ¤– Applications en IA :</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>Descente de gradient</strong> : algorithme d'optimisation fondamental</li>
                            <li>ğŸ§  <strong>RÃ©seaux de neurones</strong> : mise Ã  jour des poids automatique</li>
                            <li>ğŸ“Š <strong>Machine Learning</strong> : minimisation des erreurs</li>
                            <li>ğŸ’¬ <strong>LLMs</strong> : entraÃ®nement de modÃ¨les gÃ©ants comme ChatGPT</li>
                            <li>ğŸ¨ <strong>IA gÃ©nÃ©rative</strong> : amÃ©lioration continue des rÃ©sultats</li>
                        </ul>
                    `,
          },
          {
            type: "intuition",
            icon: "ğŸ§ ",
            title: "L'analogie de la montagne",
            content: `
                        <p>Imaginez que vous Ãªtes <strong>perdu dans le brouillard</strong> sur une montagne et que vous voulez atteindre le sommet le plus rapidement possible :</p>
                        <ul>
                            <li>ğŸ”ï¸ <strong>La montagne</strong> = fonction \\(f(x, y)\\) (altitude en chaque point)</li>
                            <li>ğŸ“ <strong>Votre position</strong> = point \\((x, y)\\)</li>
                            <li>ğŸ§­ <strong>Le gradient</strong> = direction de la pente la plus raide vers le haut</li>
                        </ul>
                        <p><strong>ğŸ¯ StratÃ©gie optimale :</strong></p>
                        <ul>
                            <li>ğŸ‘€ <strong>Regarder autour de vous</strong> : calculer le gradient</li>
                            <li>ğŸš¶ <strong>Faire un pas</strong> dans la direction du gradient</li>
                            <li>ğŸ”„ <strong>RÃ©pÃ©ter</strong> jusqu'Ã  atteindre le sommet</li>
                        </ul>
                        <p><strong>ğŸ’¡ C'est exactement ce que fait l'IA :</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>Objectif</strong> : minimiser les erreurs (descendre vers le creux)</li>
                            <li>ğŸ§­ <strong>Gradient</strong> : direction pour rÃ©duire l'erreur le plus vite</li>
                            <li>ğŸ”„ <strong>ItÃ©ration</strong> : amÃ©lioration continue automatique</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Calcul manuel de gradients",
            content: `
                        <p><strong>Prenons une fonction simple Ã  deux variables :</strong></p>
                        <p>$$f(x, y) = x^2 + y^2$$</p>
                        
                        <p><strong>ğŸ”¢ Calcul du gradient :</strong></p>
                        <p><strong>Ã‰tape 1 :</strong> DÃ©rivÃ©e partielle par rapport Ã  \\(x\\)</p>
                        <p>$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x}(x^2 + y^2) = 2x$$</p>
                        
                        <p><strong>Ã‰tape 2 :</strong> DÃ©rivÃ©e partielle par rapport Ã  \\(y\\)</p>
                        <p>$$\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y}(x^2 + y^2) = 2y$$</p>
                        
                        <p><strong>Ã‰tape 3 :</strong> Assemblage du gradient</p>
                        <p>$$\\nabla f = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ“ Exemple numÃ©rique :</strong> Au point \\((3, 4)\\)</p>
                        <p>$$\\nabla f(3, 4) = \\begin{bmatrix} 2 \\times 3 \\\\ 2 \\times 4 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 8 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ” InterprÃ©tation :</strong> Au point \\((3, 4)\\), pour augmenter \\(f\\) le plus rapidement, il faut se dÃ©placer dans la direction \\([6, 8]\\).</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "DÃ©rivÃ©es partielles",
            content: `
                        <p>Les <strong>dÃ©rivÃ©es partielles</strong> sont les composantes du gradient. Pour une fonction \\(f(x, y)\\) :</p>
                        <ul style="list-style: none; padding-left: 0">
                            <li><strong>â€¢ DÃ©rivÃ©e partielle par rapport Ã  \\(x\\) :</strong> $$\\frac{\\partial f}{\\partial x}$$ (on traite \\(y\\) comme une constante)</li>
                            <li style="margin-top: 1rem"><strong>â€¢ DÃ©rivÃ©e partielle par rapport Ã  \\(y\\) :</strong> $$\\frac{\\partial f}{\\partial y}$$ (on traite \\(x\\) comme une constante)</li>
                        </ul>
                        <p><strong>ğŸ”‘ RÃ¨gles de calcul :</strong></p>
                        <ul>
                            <li>MÃªmes rÃ¨gles que les dÃ©rivÃ©es ordinaires</li>
                            <li>On "ignore" les autres variables (les traiter comme des constantes)</li>
                            <li>Le gradient combine toutes les dÃ©rivÃ©es partielles</li>
                        </ul>
                        <p><strong>ğŸ“ Pour une fonction Ã  \\(n\\) variables :</strong></p>
                        <p>$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$</p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice pratique : calculs manuels",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Calculez les gradients des fonctions suivantes :</p>
                        
                        <ol>
                            <li>\\(f(x, y) = 3x^2 + 2y\\)</li>
                            <li>\\(g(x, y) = x^2 + y^2 + xy\\)</li>
                            <li>\\(h(x, y) = 2x^3 + y^2 - 4xy\\)</li>
                            <li>Pour \\(f(x, y) = x^2 + y^2\\), calculez \\(\\nabla f(1, 2)\\)</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('gradients-exercise-solution')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="gradients-exercise-solution" style="display: none;">
                        <ol>
                            <li>\\(\\nabla f = \\begin{bmatrix} 6x \\\\ 2 \\end{bmatrix}\\)</li>
                            <li>\\(\\nabla g = \\begin{bmatrix} 2x + y \\\\ 2y + x \\end{bmatrix}\\)</li>
                            <li>\\(\\nabla h = \\begin{bmatrix} 6x^2 - 4y \\\\ 2y - 4x \\end{bmatrix}\\)</li>
                            <li>\\(\\nabla f = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix}\\), donc \\(\\nabla f(1, 2) = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\\)<br>
                                <strong>InterprÃ©tation :</strong> Au point \\((1, 2)\\), la direction de plus forte croissance est \\([2, 4]\\).</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "La descente de gradient",
            content: `
                        <p><strong>ğŸ¯ Algorithme fondamental de l'IA :</strong></p>
                        <p>Pour <strong>minimiser</strong> une fonction (rÃ©duire l'erreur), on utilise la descente de gradient :</p>
                        
                        <p><strong>ğŸ“‹ Algorithme :</strong></p>
                        <ol>
                            <li><strong>Partir</strong> d'un point initial \\((x_0, y_0)\\)</li>
                            <li><strong>Calculer</strong> le gradient \\(\\nabla f(x, y)\\)</li>
                            <li><strong>Se dÃ©placer</strong> dans la direction opposÃ©e : \\((x, y) \\leftarrow (x, y) - \\alpha \\nabla f(x, y)\\)</li>
                            <li><strong>RÃ©pÃ©ter</strong> jusqu'Ã  convergence</li>
                        </ol>
                        
                        <p><strong>ğŸ”‘ ParamÃ¨tres :</strong></p>
                        <ul>
                            <li>\\(\\alpha\\) = <strong>taux d'apprentissage</strong> (taille du pas)</li>
                            <li><strong>Direction opposÃ©e</strong> car on veut descendre (minimiser)</li>
                        </ul>
                        
                        <p><strong>ğŸ“ Exemple :</strong> Minimiser \\(f(x, y) = x^2 + y^2\\)</p>
                        <ul>
                            <li><strong>Gradient :</strong> \\(\\nabla f = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix}\\)</li>
                            <li><strong>Mise Ã  jour :</strong> \\(\\begin{bmatrix} x \\\\ y \\end{bmatrix} \\leftarrow \\begin{bmatrix} x \\\\ y \\end{bmatrix} - \\alpha \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix}\\)</li>
                            <li><strong>RÃ©sultat :</strong> Convergence vers \\((0, 0)\\) = minimum global</li>
                        </ul>
                    `,
          },
          {
            type: "warning",
            icon: "âš ï¸",
            title: "Pourquoi les gradients sont-ils le cÅ“ur de l'IA ?",
            content: `
                        <p><strong>ğŸ§  Les gradients permettent l'apprentissage automatique :</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>Optimisation automatique</strong> : l'IA trouve les meilleures solutions seule</li>
                            <li>ğŸ§  <strong>RÃ©seaux de neurones</strong> : mise Ã  jour de millions de paramÃ¨tres</li>
                            <li>ğŸ“Š <strong>Machine Learning</strong> : minimisation des erreurs de prÃ©diction</li>
                            <li>ğŸ’¬ <strong>LLMs (ChatGPT)</strong> : entraÃ®nement sur des milliards de paramÃ¨tres</li>
                            <li>ğŸ¨ <strong>IA gÃ©nÃ©rative</strong> : amÃ©lioration continue des crÃ©ations</li>
                        </ul>
                        <p><strong>ğŸ’¡ Point clÃ© :</strong> Sans les gradients, l'IA moderne n'existerait pas ! Ils permettent aux machines d'apprendre automatiquement en "descendant" vers les meilleures solutions.</p>
                        <p><strong>ğŸ”® Prochaine Ã©tape :</strong> Nous verrons comment les probabilitÃ©s permettent Ã  l'IA de gÃ©rer l'incertitude et de prendre des dÃ©cisions intelligentes !</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ğŸ¤” Si le gradient d'une fonction au point (2, 3) est [4, -6], dans quelle direction faut-il se dÃ©placer pour MINIMISER la fonction ?",
          options: [
            "A) Direction [4, -6]",
            "B) Direction [-4, 6]",
            "C) Direction [6, 4]",
            "D) Rester sur place",
          ],
          correct: 1,
          explanation:
            "Pour minimiser, on va dans la direction OPPOSÃ‰E au gradient. Si le gradient est [4, -6], on se dÃ©place dans la direction [-4, 6] pour descendre vers le minimum.",
        },
        prevModule: "derivatives.html",
        nextModule: "probability.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
