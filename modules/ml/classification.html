<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Classification | IA4Ndada</title>

    <!-- MathJax pour les formules math√©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">üè† Accueil</a>
          <span>‚Ä∫</span>
          <span>ü§ñ Machine Learning</span>
          <span>‚Ä∫</span>
          <span>Classification</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>üéØ Classification</h1>
      <p class="subtitle">Module 3.3 - Machine Learning</p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>üéØ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajout√©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajout√© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>üéâ Checkpoint - Classification</h3>
        <p>
          Vous ma√Ætrisez maintenant la r√©gression logistique et ses extensions.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme compl√©t√©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="linear-regression.html" class="nav-link" id="prev-link"
          >‚Üê Module pr√©c√©dent : R√©gression Lin√©aire</a
        >
        <a href="clustering.html" class="nav-link" id="next-link"
          >Module suivant : Clustering ‚Üí</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module Classification
      const moduleConfig = {
        id: "ml-classification",
        title: "Classification",
        category: "Machine Learning",
        objectives: [
          "Comprendre pourquoi on ne peut pas utiliser la r√©gression directement",
          "Ma√Ætriser la fonction sigmo√Øde et son r√¥le crucial",
          "D√©river et comprendre l'entropie crois√©e",
          "Comprendre la r√©gularisation et son importance",
          "G√©n√©raliser au cas multi-classe avec softmax",
        ],
        content: [
          {
            type: "concept",
            icon: "üí°",
            title: "Le probl√®me : pr√©dire des cat√©gories",
            content: `
                        <p><strong>La classification pr√©dit une classe discr√®te, pas une valeur continue.</strong></p>
                        
                        <p>Exemple m√©dical : une tumeur est maligne (1) ou b√©nigne (0). Pas 0.7 maligne.</p>
                        
                        <p><strong>Pourquoi pas la r√©gression lin√©aire ?</strong> Testons avec y ‚àà {0, 1} :</p>
                        
                        <div style="background: #ffebee; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>Probl√®me fatal :</strong> y = ax + b peut donner n'importe quelle valeur !<br><br>
                            ‚Ä¢ Tumeur 1mm ‚Üí y = -0.2 (probabilit√© n√©gative ?)<br>
                            ‚Ä¢ Tumeur 100mm ‚Üí y = 1.8 (probabilit√© > 100% ?)<br><br>
                            
                            On a besoin d'une fonction born√©e entre 0 et 1.
                        </div>
                        
                        <p><strong>L'id√©e :</strong> Transformer la sortie lin√©aire en probabilit√© P(y=1|x).</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "La sigmo√Øde : le c≈ìur de la solution",
            content: `
                        <p><strong>La fonction sigmo√Øde transforme ‚Ñù ‚Üí (0,1) :</strong></p>
                        
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$
                            
                            <strong>Propri√©t√©s essentielles :</strong><br>
                            ‚Ä¢ \\(\\lim_{z \\to -\\infty} \\sigma(z) = 0\\) et \\(\\lim_{z \\to +\\infty} \\sigma(z) = 1\\)<br>
                            ‚Ä¢ \\(\\sigma(0) = 0.5\\) (point d'inflexion)<br>
                            ‚Ä¢ Strictement croissante (pr√©serve l'ordre)<br>
                            ‚Ä¢ Sym√©trique : \\(\\sigma(-z) = 1 - \\sigma(z)\\)
                        </div>
                        
                        <p><strong>LA propri√©t√© cruciale (d√©riv√©e) :</strong></p>
                        
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            $$\\frac{d\\sigma}{dz} = \\sigma(z) \\cdot (1 - \\sigma(z))$$
                            
                            <strong>D√©monstration :</strong> Posons \\(\\sigma = (1 + e^{-z})^{-1}\\)<br><br>
                            
                            Par la r√®gle de cha√Æne (<a href="../math/derivatives.html">Module 1.4</a>) :<br>
                            $$\\frac{d\\sigma}{dz} = -(1 + e^{-z})^{-2} \\cdot (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2}$$
                            
                            Or : \\(\\sigma = \\frac{1}{1 + e^{-z}}\\) donc \\(1 - \\sigma = \\frac{e^{-z}}{1 + e^{-z}}\\)<br><br>
                            
                            Ainsi : \\(\\sigma(1 - \\sigma) = \\frac{1}{1 + e^{-z}} \\cdot \\frac{e^{-z}}{1 + e^{-z}} = \\frac{e^{-z}}{(1 + e^{-z})^2}\\) ‚úì
                        </div>
                        
                        <p>Cette forme √©l√©gante va simplifier tous nos calculs d'optimisation !</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "L'entropie crois√©e : pourquoi cette fonction de co√ªt",
            content: `
                        <p><strong>Le mod√®le complet (r√©gression logistique) :</strong></p>
                        <p>$$P(y=1|\\vec{x}) = \\sigma(\\vec{w}^T\\vec{x} + b)$$</p>
                        
                        <p><strong>Pourquoi pas l'erreur quadratique ?</strong></p>
                        <p>Avec \\(J = \\sum(y_i - \\sigma(\\vec{w}^T\\vec{x}_i + b))^2\\), la fonction n'est pas convexe ‚Üí minima locaux !</p>
                        
                        <p><strong>L'approche par maximum de vraisemblance :</strong></p>
                        
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            Pour un exemple \\((\\vec{x}_i, y_i)\\), la probabilit√© d'observer \\(y_i\\) est :<br><br>
                            
                            $$P(y_i|\\vec{x}_i) = p_i^{y_i}(1-p_i)^{1-y_i}$$<br>
                            o√π \\(p_i = \\sigma(\\vec{w}^T\\vec{x}_i + b)\\)<br><br>
                            
                            <strong>V√©rification :</strong><br>
                            ‚Ä¢ Si \\(y_i = 1\\) : \\(P = p_i\\) ‚úì<br>
                            ‚Ä¢ Si \\(y_i = 0\\) : \\(P = 1 - p_i\\) ‚úì
                        </div>
                        
                        <p><strong>La fonction de co√ªt (entropie crois√©e binaire) :</strong></p>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            En prenant le -log de la vraisemblance et en moyennant :<br><br>
                            
                            $$J(\\vec{w}, b) = -\\frac{1}{n}\\sum_{i=1}^{n} [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]$$<br><br>
                            
                            <strong>Cette fonction est convexe !</strong> (Un seul minimum global)<br><br>
                            
                            <strong>Gradient (apr√®s calcul avec r√®gle de cha√Æne) :</strong><br>
                            $$\\nabla_{\\vec{w}} J = \\frac{1}{n}\\sum_{i=1}^{n} (p_i - y_i) \\vec{x}_i$$<br>
                            $$\\frac{\\partial J}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n} (p_i - y_i)$$<br><br>
                            
                            Forme identique √† la r√©gression, mais avec \\(p_i = \\sigma(\\vec{w}^T\\vec{x}_i + b)\\) !
                        </div>
                    `,
          },
          {
            type: "concept",
            icon: "üí°",
            title: "La r√©gularisation : √©viter le sur-apprentissage",
            content: `
                        <p><strong>Le probl√®me du sur-apprentissage :</strong></p>
                        
                        <p>Avec beaucoup de features, le mod√®le peut "m√©moriser" les donn√©es d'entra√Ænement au lieu de g√©n√©raliser.</p>
                        
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>La solution : p√©naliser les poids trop grands</strong><br><br>
                            
                            On ajoute un terme de r√©gularisation √† J :<br>
                            $$J_{reg}(\\vec{w}, b) = J(\\vec{w}, b) + \\lambda \\cdot R(\\vec{w})$$<br><br>
                            
                            o√π \\(\\lambda > 0\\) contr√¥le la force de r√©gularisation.
                        </div>
                        
                        <p><strong>Deux types principaux :</strong></p>
                        
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>L2 (Ridge) :</strong> \\(R(\\vec{w}) = ||\\vec{w}||_2^2 = \\sum_{j} w_j^2\\)<br>
                            ‚Ä¢ R√©duit uniform√©ment tous les poids<br>
                            ‚Ä¢ Garde toutes les features<br>
                            ‚Ä¢ Solution analytique modifi√©e : \\((X^TX + \\lambda I)^{-1}X^Ty\\)<br><br>
                            
                            <strong>L1 (Lasso) :</strong> \\(R(\\vec{w}) = ||\\vec{w}||_1 = \\sum_{j} |w_j|\\)<br>
                            ‚Ä¢ Force certains poids √† exactement 0<br>
                            ‚Ä¢ S√©lection automatique de features<br>
                            ‚Ä¢ Pas de solution analytique
                        </div>
                        
                        <p><strong>Interpr√©tation bay√©sienne :</strong> La r√©gularisation correspond √† un prior sur les poids (L2 = gaussien, L1 = laplacien).</p>
                        
                        <p><strong>Impact sur le gradient :</strong><br>
                        L2 : \\(\\nabla_{\\vec{w}} J_{reg} = \\nabla_{\\vec{w}} J + 2\\lambda\\vec{w}\\)<br>
                        L1 : \\(\\nabla_{\\vec{w}} J_{reg} = \\nabla_{\\vec{w}} J + \\lambda \\cdot \\text{sign}(\\vec{w})\\)</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "Extension multi-classe : softmax",
            content: `
                        <p><strong>Pour K classes (K > 2), on g√©n√©ralise avec softmax :</strong></p>
                        
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>Un vecteur de poids par classe :</strong><br>
                            \\(\\vec{w}_1, \\vec{w}_2, ..., \\vec{w}_K\\) et \\(b_1, b_2, ..., b_K\\)<br><br>
                            
                            <strong>Scores (logits) :</strong><br>
                            $$z_k = \\vec{w}_k^T\\vec{x} + b_k \\quad \\text{pour } k = 1, ..., K$$<br><br>
                            
                            <strong>Fonction softmax :</strong><br>
                            $$P(y=k|\\vec{x}) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$$
                        </div>
                        
                        <p><strong>Propri√©t√©s du softmax :</strong></p>
                        <ul>
                            <li>\\(\\sum_{k=1}^{K} P(y=k|\\vec{x}) = 1\\) (vraie distribution)</li>
                            <li>\\(0 < P(y=k|\\vec{x}) < 1\\) pour tout k</li>
                            <li>Invariant par translation : ajouter c √† tous les \\(z_k\\) ne change rien</li>
                            <li>Si K=2, √©quivalent √† la sigmo√Øde</li>
                        </ul>
                        
                        <p><strong>Fonction de co√ªt (entropie crois√©e cat√©gorielle) :</strong></p>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            Avec encodage one-hot : \\(y_i \\in \\{0,1\\}^K\\) o√π \\(\\sum_k y_{ik} = 1\\)<br><br>
                            
                            $$J = -\\frac{1}{n}\\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{ik} \\log(p_{ik})$$<br>
                            o√π \\(p_{ik} = P(y=k|\\vec{x}_i)\\)<br><br>
                            
                            <strong>Gradient (forme vectorielle) :</strong><br>
                            $$\\nabla_{\\vec{w}_k} J = \\frac{1}{n}\\sum_{i=1}^{n} (p_{ik} - y_{ik}) \\vec{x}_i$$<br><br>
                            
                            M√™me forme √©l√©gante qu'en binaire !
                        </div>
                        
                        <p><strong>Astuce num√©rique :</strong> Pour √©viter overflow, calculer :<br>
                        \\(\\log P(y=k|\\vec{x}) = z_k - \\log\\sum_j e^{z_j} = z_k - \\log\\sum_j e^{z_j - \\max_j z_j} - \\max_j z_j\\)</p>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Application : classification binaire r√©gularis√©e",
            content: `
                        <p><strong>Classifions avec r√©gularisation L2 :</strong></p>
                        
                        <table style="margin: 1rem auto; text-align: center;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.5rem;">x‚ÇÅ</th>
                                <th style="padding: 0.5rem;">x‚ÇÇ</th>
                                <th style="padding: 0.5rem;">y</th>
                            </tr>
                            <tr><td>1</td><td>2</td><td>0</td></tr>
                            <tr><td>2</td><td>3</td><td>0</td></tr>
                            <tr><td>3</td><td>1</td><td>1</td></tr>
                            <tr><td>4</td><td>2</td><td>1</td></tr>
                        </table>
                        
                        <p><strong>Sans r√©gularisation (Œª=0) :</strong></p>
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            Apr√®s optimisation : \\(w_1 = 2.5, w_2 = -1.8, b = -1.2\\)<br>
                            Norme des poids : \\(||\\vec{w}||_2 = \\sqrt{2.5^2 + 1.8^2} \\approx 3.08\\)
                        </div>
                        
                        <p><strong>Avec r√©gularisation L2 (Œª=0.5) :</strong></p>
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            Le gradient devient :<br>
                            $$\\nabla_{\\vec{w}} J_{reg} = \\frac{1}{n}\\sum (p_i - y_i)\\vec{x}_i + \\lambda\\vec{w}$$<br><br>
                            
                            Apr√®s optimisation : \\(w_1 = 1.2, w_2 = -0.8, b = -0.7\\)<br>
                            Norme des poids : \\(||\\vec{w}||_2 \\approx 1.44\\) (r√©duite de 53% !)<br><br>
                            
                            <strong>Impact :</strong> Mod√®le plus simple, moins sensible au bruit
                        </div>
                        
                        <p><strong>Interpr√©tation des coefficients :</strong></p>
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            ‚Ä¢ \\(w_1 = 1.2 > 0\\) : x‚ÇÅ augmente ‚Üí probabilit√© classe 1 augmente<br>
                            ‚Ä¢ \\(w_2 = -0.8 < 0\\) : x‚ÇÇ augmente ‚Üí probabilit√© classe 1 diminue<br>
                            ‚Ä¢ \\(|w_1| > |w_2|\\) : x‚ÇÅ plus important que x‚ÇÇ<br><br>
                            
                            <strong>Fronti√®re de d√©cision :</strong> \\(1.2x_1 - 0.8x_2 - 0.7 = 0\\)<br>
                            Soit : \\(x_2 = 1.5x_1 - 0.875\\)
                        </div>
                    `,
          },
          {
            type: "warning",
            icon: "‚ö†Ô∏è",
            title: "L'essentiel √† retenir",
            content: `
                        <p><strong>Les concepts fondamentaux :</strong></p>
                        
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>1. La sigmo√Øde transforme lin√©aire ‚Üí probabilit√©</strong><br>
                            Sans elle, pas de classification propre.<br><br>
                            
                            <strong>2. L'entropie crois√©e garantit la convexit√©</strong><br>
                            Contrairement √† MSE, un seul minimum global.<br><br>
                            
                            <strong>3. La r√©gularisation contr√¥le la complexit√©</strong><br>
                            √âquilibre biais-variance via Œª.<br><br>
                            
                            <strong>4. Softmax g√©n√©ralise naturellement</strong><br>
                            De 2 √† K classes, m√™me principe.
                        </div>
                        
                        <p><strong>Les pi√®ges √† √©viter :</strong></p>
                        <ul>
                            <li>Classes d√©s√©quilibr√©es ‚Üí pond√©rer les exemples</li>
                            <li>Features √† √©chelles diff√©rentes ‚Üí normaliser</li>
                            <li>Trop de features ‚Üí r√©gulariser</li>
                            <li>Non-lin√©arit√© ‚Üí ajouter features polynomiales ou kernel</li>
                        </ul>
                        
                        <p><strong>Pour aller plus loin :</strong> SVM (marges maximales), arbres de d√©cision (non-lin√©aire), r√©seaux de neurones (compositions de sigmo√Ødes).</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ü§î Pourquoi la r√©gularisation L1 peut-elle mettre des poids √† exactement z√©ro, mais pas L2 ?",
          options: [
            "A) L1 est plus forte que L2",
            "B) La d√©riv√©e de |w| a une discontinuit√© en 0 qui 'attrape' les poids",
            "C) C'est un bug de l'algorithme",
            "D) L2 met aussi des poids √† z√©ro",
          ],
          correct: 1,
          explanation:
            "La d√©riv√©e de L2 (w¬≤) est 2w, qui tend vers 0 quand w‚Üí0 : le gradient diminue progressivement. La d√©riv√©e de L1 (|w|) est sign(w) = ¬±1, constante jusqu'√† w=0 : le gradient 'pousse' avec force constante jusqu'√† z√©ro. G√©om√©triquement, L1 cr√©e une contrainte en losange avec des 'coins' sur les axes (w=0), tandis que L2 cr√©e une contrainte circulaire sans points privil√©gi√©s.",
        },
        prevModule: "linear-regression.html",
        nextModule: "clustering.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
