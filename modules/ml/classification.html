<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Classification | IA4Ndada</title>

    <!-- MathJax pour les formules math√©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">üè† Accueil</a>
          <span>‚Ä∫</span>
          <span>ü§ñ Machine Learning</span>
          <span>‚Ä∫</span>
          <span>Classification</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>üéØ Classification</h1>
      <p class="subtitle">
        Module 3.3 - Pr√©dire des cat√©gories avec la r√©gression logistique
      </p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>üéØ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajout√©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajout√© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>üéâ Checkpoint - Classification</h3>
        <p>
          F√©licitations ! Vous ma√Ætrisez maintenant la r√©gression logistique et
          la classification.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme compl√©t√©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="linear-regression.html" class="nav-link" id="prev-link"
          >‚Üê Module pr√©c√©dent : R√©gression Lin√©aire</a
        >
        <a href="clustering.html" class="nav-link" id="next-link"
          >Module suivant : Clustering ‚Üí</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module Classification
      const moduleConfig = {
        id: "ml-classification",
        title: "Classification",
        category: "Machine Learning",
        objectives: [
          "Comprendre pourquoi la r√©gression lin√©aire ne fonctionne pas pour les cat√©gories",
          "Ma√Ætriser la fonction sigmo√Øde et son r√¥le crucial",
          "D√©river compl√®tement la r√©gression logistique",
          "Comprendre l'entropie crois√©e et pourquoi elle est convexe",
          "Appliquer la r√©gularisation pour √©viter l'overfitting",
          "G√©n√©raliser au multi-classe avec softmax",
        ],
        content: [
          {
            type: "concept",
            icon: "üí°",
            title: "Le probl√®me fondamental : pr√©dire des cat√©gories",
            content: `
                        <p>La <strong>classification</strong> pr√©dit des <strong>cat√©gories discr√®tes</strong>, pas des valeurs continues. C'est un changement de paradigme par rapport √† la r√©gression !</p>
                        
                        <p><strong>üîë Diff√©rence cruciale :</strong></p>
                        <ul>
                            <li>üìà <strong>R√©gression</strong> : "Cette maison co√ªte 45 millions FCFA" (nombre)</li>
                            <li>üéØ <strong>Classification</strong> : "Cet email est un spam" (cat√©gorie)</li>
                        </ul>
                        
                        <p><strong>ü§ñ Applications concr√®tes :</strong></p>
                        <ul>
                            <li>üìß <strong>D√©tection de spam</strong> : spam ou pas spam</li>
                            <li>üè• <strong>Diagnostic m√©dical</strong> : maladie A, B ou C</li>
                            <li>üé¨ <strong>Recommandations</strong> : aimera ou n'aimera pas</li>
                            <li>üñºÔ∏è <strong>Vision par ordinateur</strong> : chat, chien, oiseau</li>
                            <li>üí¨ <strong>Analyse de sentiment</strong> : positif, n√©gatif, neutre</li>
                        </ul>
                        
                        <p><strong>‚ö†Ô∏è Probl√®me avec la r√©gression lin√©aire :</strong></p>
                        <p>Si on code les cat√©gories en nombres (spam=1, pas spam=0), la r√©gression lin√©aire peut pr√©dire 0.7, 1.3, -0.2... Que signifie "0.7 spam" ?</p>
                        
                        <p><strong>üí° Solution :</strong> Transformer la sortie en <strong>probabilit√©</strong> entre 0 et 1 !</p>
                    `,
          },
          {
            type: "intuition",
            icon: "üß†",
            title: "L'analogie du m√©decin et du diagnostic",
            content: `
                        <p>Imaginez un <strong>m√©decin exp√©riment√©</strong> qui examine un patient :</p>
                        
                        <p><strong>ü©∫ Le processus mental :</strong></p>
                        <ol>
                            <li>üìã <strong>Collecte des sympt√¥mes</strong> : fi√®vre, toux, fatigue...</li>
                            <li>üß† <strong>Calcul mental</strong> : "Ces sympt√¥mes correspondent √†..."</li>
                            <li>‚öñÔ∏è <strong>√âvaluation des probabilit√©s</strong> : "85% grippe, 10% rhume, 5% autre"</li>
                            <li>üéØ <strong>D√©cision finale</strong> : "C'est probablement une grippe"</li>
                        </ol>
                        
                        <p><strong>ü§ñ C'est exactement ce que fait la classification :</strong></p>
                        <ul>
                            <li>üìä <strong>Donn√©es d'entr√©e</strong> = sympt√¥mes du patient</li>
                            <li>üßÆ <strong>Calcul lin√©aire</strong> = combinaison pond√©r√©e des sympt√¥mes</li>
                            <li>üéØ <strong>Fonction sigmo√Øde</strong> = transformation en probabilit√©</li>
                            <li>‚öñÔ∏è <strong>Seuil de d√©cision</strong> = "si probabilit√© > 50%, alors grippe"</li>
                        </ul>
                        
                        <p><strong>üí° La diff√©rence :</strong></p>
                        <p>Le m√©decin utilise son exp√©rience, l'IA utilise les <strong>math√©matiques</strong> pour arriver au m√™me type de raisonnement probabiliste !</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "Pourquoi la r√©gression lin√©aire √©choue",
            content: `
                        <p><strong>üîç Testons la r√©gression lin√©aire sur un probl√®me de classification :</strong></p>
                        
                        <p><strong>Probl√®me :</strong> Pr√©dire si un √©tudiant r√©ussit (y=1) ou √©choue (y=0) selon ses heures d'√©tude.</p>
                        
                        <p><strong>Donn√©es d'exemple :</strong></p>
                        <table style="margin: 1rem auto; text-align: center; border-collapse: collapse;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.5rem; border: 1px solid #ddd;">Heures d'√©tude</th>
                                <th style="padding: 0.5rem; border: 1px solid #ddd;">R√©sultat</th>
                            </tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0 (√©chec)</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">4</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0 (√©chec)</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">6</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1 (r√©ussite)</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">8</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1 (r√©ussite)</td></tr>
                        </table>
                        
                        <p><strong>‚ùå Probl√®mes de la r√©gression lin√©aire :</strong></p>
                        
                        <div style="background: #ffebee; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>1. Pr√©dictions impossibles :</strong><br>
                            ‚Ä¢ Pour 0 heure : ≈∑ = -0.4 (probabilit√© n√©gative ?)<br>
                            ‚Ä¢ Pour 12 heures : ≈∑ = 1.6 (probabilit√© > 100% ?)<br><br>
                            
                            <strong>2. Seuil arbitraire :</strong><br>
                            ‚Ä¢ Si ≈∑ > 0.5 ‚Üí r√©ussite, sinon √©chec<br>
                            ‚Ä¢ Mais pourquoi 0.5 ? Pourquoi pas 0.3 ou 0.7 ?<br><br>
                            
                            <strong>3. Sensibilit√© aux outliers :</strong><br>
                            ‚Ä¢ Un √©tudiant exceptionnel (20h, r√©ussite) d√©cale toute la droite<br>
                            ‚Ä¢ Fausses pr√©dictions pour les cas normaux
                        </div>
                        
                        <p><strong>‚úÖ Solution :</strong> Utiliser une fonction qui transforme ‚Ñù ‚Üí (0,1) : la <strong>sigmo√Øde</strong> !</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "La sigmo√Øde : transformation magique",
            content: `
                        <p><strong>La fonction sigmo√Øde r√©sout √©l√©gamment tous les probl√®mes :</strong></p>
                        
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$
                            
                            <strong>Propri√©t√©s remarquables :</strong><br>
                            ‚Ä¢ \\(\\lim_{z \\to -\\infty} \\sigma(z) = 0\\) (jamais n√©gatif)<br>
                            ‚Ä¢ \\(\\lim_{z \\to +\\infty} \\sigma(z) = 1\\) (jamais > 1)<br>
                            ‚Ä¢ \\(\\sigma(0) = 0.5\\) (point d'√©quilibre)<br>
                            ‚Ä¢ Strictement croissante (pr√©serve l'ordre)<br>
                            ‚Ä¢ Forme de "S" lisse (pas de saut brutal)
                        </div>
                        
                        <p><strong>üîç Valeurs remarquables :</strong></p>
                        <ul style="list-style: none; padding-left: 0">
                            <li>‚Ä¢ \\(\\sigma(-2) ‚âà 0.12\\) ‚Üí 12% de chance</li>
                            <li>‚Ä¢ \\(\\sigma(-1) ‚âà 0.27\\) ‚Üí 27% de chance</li>
                            <li>‚Ä¢ \\(\\sigma(0) = 0.50\\) ‚Üí 50% de chance</li>
                            <li>‚Ä¢ \\(\\sigma(1) ‚âà 0.73\\) ‚Üí 73% de chance</li>
                            <li>‚Ä¢ \\(\\sigma(2) ‚âà 0.88\\) ‚Üí 88% de chance</li>
                        </ul>
                        
                        <p><strong>üîë Propri√©t√© cruciale (d√©riv√©e) :</strong></p>
                        <p>$$\\frac{d\\sigma}{dz} = \\sigma(z) \\cdot (1 - \\sigma(z))$$</p>
                        
                        <p><strong>üí° Pourquoi cette d√©riv√©e est g√©niale :</strong></p>
                        <ul>
                            <li>üßÆ <strong>Calcul simple</strong> : pas besoin de recalculer l'exponentielle</li>
                            <li>‚ö° <strong>Efficacit√©</strong> : optimisation rapide des r√©seaux de neurones</li>
                            <li>üéØ <strong>Stabilit√© num√©rique</strong> : √©vite les d√©bordements</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Exercice : calculs manuels avec la sigmo√Øde",
            content: `
                        <p><strong>üéØ Exercice √† r√©soudre :</strong></p>
                        <p>Un mod√®le de classification pr√©dit la r√©ussite d'un √©tudiant avec :</p>
                        <p>$$z = 0.5 \\times \\text{heures_√©tude} - 2$$</p>
                        <p>$$P(\\text{r√©ussite}) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$</p>
                        
                        <p><strong>üìù Calculez √† la main :</strong></p>
                        <ol>
                            <li>Probabilit√© de r√©ussite pour 2 heures d'√©tude</li>
                            <li>Probabilit√© de r√©ussite pour 4 heures d'√©tude</li>
                            <li>Probabilit√© de r√©ussite pour 6 heures d'√©tude</li>
                            <li>Combien d'heures faut-il pour avoir 50% de chance ?</li>
                            <li>Combien d'heures faut-il pour avoir 90% de chance ?</li>
                        </ol>
                        
                        <p><strong>‚úÖ Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('sigmoid-exercise-solution')" style="margin-bottom: 1rem;">
                            üëÅÔ∏è Voir la solution
                        </button>
                        <div id="sigmoid-exercise-solution" style="display: none;">
                        <ol>
                            <li><strong>2 heures :</strong> z = 0.5√ó2 - 2 = -1 ‚Üí œÉ(-1) = 1/(1+e) ‚âà 0.27 ‚Üí <strong>27%</strong></li>
                            <li><strong>4 heures :</strong> z = 0.5√ó4 - 2 = 0 ‚Üí œÉ(0) = 0.5 ‚Üí <strong>50%</strong></li>
                            <li><strong>6 heures :</strong> z = 0.5√ó6 - 2 = 1 ‚Üí œÉ(1) = 1/(1+e‚Åª¬π) ‚âà 0.73 ‚Üí <strong>73%</strong></li>
                            <li><strong>50% de chance :</strong> œÉ(z) = 0.5 ‚ü∫ z = 0 ‚ü∫ 0.5h - 2 = 0 ‚ü∫ <strong>h = 4 heures</strong></li>
                            <li><strong>90% de chance :</strong> œÉ(z) = 0.9 ‚ü∫ z ‚âà 2.2 ‚ü∫ 0.5h - 2 = 2.2 ‚ü∫ <strong>h = 8.4 heures</strong></li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "concept",
            icon: "üí°",
            title: "Construction du mod√®le : r√©gression logistique",
            content: `
                        <p><strong>üéØ Mod√®le complet de r√©gression logistique :</strong></p>
                        
                        <p><strong>√âtape 1 :</strong> Combinaison lin√©aire des features</p>
                        <p>$$z = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n = \\vec{w}^T \\vec{x} + b$$</p>
                        
                        <p><strong>√âtape 2 :</strong> Transformation en probabilit√©</p>
                        <p>$$P(y=1|\\vec{x}) = \\sigma(z) = \\frac{1}{1 + e^{-(\\vec{w}^T \\vec{x} + b)}}$$</p>
                        
                        <p><strong>√âtape 3 :</strong> R√®gle de d√©cision</p>
                        <p>$$\\hat{y} = \\begin{cases} 1 & \\text{si } P(y=1|\\vec{x}) \\geq 0.5 \\\\ 0 & \\text{sinon} \\end{cases}$$</p>
                        
                        <p><strong>üîë Interpr√©tation des param√®tres :</strong></p>
                        <ul>
                            <li>\\(w_i > 0\\) : feature \\(x_i\\) <strong>augmente</strong> la probabilit√© de classe 1</li>
                            <li>\\(w_i < 0\\) : feature \\(x_i\\) <strong>diminue</strong> la probabilit√© de classe 1</li>
                            <li>\\(|w_i|\\) grand : feature \\(x_i\\) tr√®s <strong>influente</strong></li>
                            <li>\\(b\\) : <strong>biais</strong> (probabilit√© quand toutes les features = 0)</li>
                        </ul>
                        
                        <p><strong>üìê Fronti√®re de d√©cision :</strong></p>
                        <p>La fronti√®re o√π P(y=1) = 0.5 correspond √† \\(\\vec{w}^T \\vec{x} + b = 0\\)</p>
                        <p>C'est une <strong>droite</strong> (2D), un <strong>plan</strong> (3D), ou un <strong>hyperplan</strong> (nD) !</p>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Probl√®me concret : admission universitaire",
            content: `
                        <p><strong>üéì Pr√©disons l'admission √† l'UCAD selon 2 crit√®res :</strong></p>
                        
                        <table style="margin: 1rem auto; text-align: center; border-collapse: collapse;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.5rem; border: 1px solid #ddd;">Note Bac</th>
                                <th style="padding: 0.5rem; border: 1px solid #ddd;">Heures √©tude/sem</th>
                                <th style="padding: 0.5rem; border: 1px solid #ddd;">Admis</th>
                            </tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">12</td><td style="padding: 0.5rem; border: 1px solid #ddd;">10</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">14</td><td style="padding: 0.5rem; border: 1px solid #ddd;">15</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">16</td><td style="padding: 0.5rem; border: 1px solid #ddd;">20</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">18</td><td style="padding: 0.5rem; border: 1px solid #ddd;">25</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td></tr>
                        </table>
                        
                        <p><strong>üéØ Mod√®le propos√© :</strong></p>
                        <p>$$P(\\text{admis}) = \\sigma(w_1 \\times \\text{note} + w_2 \\times \\text{heures} + b)$$</p>
                        
                        <p><strong>üîç Avec r√©gression lin√©aire (MAUVAIS) :</strong></p>
                        <p>Droite : ≈∑ = 0.1√ónote + 0.05√óheures - 2</p>
                        <ul>
                            <li>Note=10, heures=5 ‚Üí ≈∑ = -0.75 (probabilit√© n√©gative !)</li>
                            <li>Note=20, heures=30 ‚Üí ≈∑ = 1.5 (probabilit√© > 100% !)</li>
                        </ul>
                        
                        <p><strong>‚úÖ Avec r√©gression logistique (BON) :</strong></p>
                        <p>P(admis) = œÉ(0.1√ónote + 0.05√óheures - 2)</p>
                        <ul>
                            <li>Note=10, heures=5 ‚Üí P = œÉ(-0.75) ‚âà 0.32 ‚Üí <strong>32% de chance</strong></li>
                            <li>Note=20, heures=30 ‚Üí P = œÉ(1.5) ‚âà 0.82 ‚Üí <strong>82% de chance</strong></li>
                        </ul>
                        
                        <p><strong>üí° R√©sultat :</strong> Toujours des probabilit√©s valides entre 0 et 1 !</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "D√©rivation de l'entropie crois√©e",
            content: `
                        <p><strong>ü§î Pourquoi pas l'erreur quadratique (MSE) ?</strong></p>
                        
                        <p>Avec MSE : \\(J = \\frac{1}{2n}\\sum_{i=1}^{n}(y_i - \\sigma(\\vec{w}^T\\vec{x}_i + b))^2\\)</p>
                        
                        <div style="background: #ffebee; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>‚ùå Probl√®me fatal :</strong> Cette fonction n'est PAS convexe !<br><br>
                            
                            La sigmo√Øde cr√©e des "plateaux" o√π le gradient devient tr√®s petit.<br>
                            ‚Üí Minima locaux multiples<br>
                            ‚Üí Optimisation difficile et instable<br>
                            ‚Üí Pas de garantie de convergence
                        </div>
                        
                        <p><strong>‚úÖ Solution : Maximum de vraisemblance</strong></p>
                        
                        <p><strong>√âtape 1 :</strong> Mod√©lisation probabiliste</p>
                        <p>Pour un exemple \\((\\vec{x}_i, y_i)\\), la probabilit√© d'observer \\(y_i\\) est :</p>
                        <p>$$P(y_i|\\vec{x}_i) = p_i^{y_i}(1-p_i)^{1-y_i}$$</p>
                        <p>o√π \\(p_i = \\sigma(\\vec{w}^T\\vec{x}_i + b)\\)</p>
                        
                        <p><strong>V√©rification :</strong></p>
                        <ul>
                            <li>Si \\(y_i = 1\\) : \\(P = p_i^1 \\times (1-p_i)^0 = p_i\\) ‚úì</li>
                            <li>Si \\(y_i = 0\\) : \\(P = p_i^0 \\times (1-p_i)^1 = 1-p_i\\) ‚úì</li>
                        </ul>
                        
                        <p><strong>√âtape 2 :</strong> Vraisemblance totale</p>
                        <p>$$L(\\vec{w}, b) = \\prod_{i=1}^{n} P(y_i|\\vec{x}_i) = \\prod_{i=1}^{n} p_i^{y_i}(1-p_i)^{1-y_i}$$</p>
                        
                        <p><strong>√âtape 3 :</strong> Log-vraisemblance (plus pratique)</p>
                        <p>$$\\ell(\\vec{w}, b) = \\log L = \\sum_{i=1}^{n} [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]$$</p>
                        
                        <p><strong>√âtape 4 :</strong> Fonction de co√ªt (entropie crois√©e)</p>
                        <p>$$J(\\vec{w}, b) = -\\frac{1}{n} \\ell(\\vec{w}, b) = -\\frac{1}{n}\\sum_{i=1}^{n} [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)]$$</p>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>üéâ Cette fonction est convexe !</strong><br>
                            ‚Üí Un seul minimum global<br>
                            ‚Üí Convergence garantie<br>
                            ‚Üí Optimisation stable et efficace
                        </div>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Exercice : construction de la fonction de co√ªt",
            content: `
                        <p><strong>üéØ Exercice √† r√©soudre :</strong></p>
                        <p>Avec nos 4 √©tudiants et le mod√®le \\(P(\\text{admis}) = \\sigma(w_1 \\times \\text{note} + w_2 \\times \\text{heures} + b)\\)</p>
                        
                        <p><strong>üìù Supposons :</strong> \\(w_1 = 0.2\\), \\(w_2 = 0.1\\), \\(b = -3\\)</p>
                        
                        <p><strong>Calculez :</strong></p>
                        <ol>
                            <li>Les pr√©dictions \\(p_i\\) pour chaque √©tudiant</li>
                            <li>La log-vraisemblance totale</li>
                            <li>L'entropie crois√©e (fonction de co√ªt)</li>
                            <li>Interpr√©tez : ce mod√®le est-il bon ?</li>
                        </ol>
                        
                        <p><strong>‚úÖ Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('cost-function-exercise')" style="margin-bottom: 1rem;">
                            üëÅÔ∏è Voir la solution
                        </button>
                        <div id="cost-function-exercise" style="display: none;">
                        <ol>
                            <li><strong>Pr√©dictions :</strong>
                                <br>‚Ä¢ √âtudiant 1 : z = 0.2√ó12 + 0.1√ó10 - 3 = 0.4 ‚Üí p‚ÇÅ = œÉ(0.4) ‚âà 0.60
                                <br>‚Ä¢ √âtudiant 2 : z = 0.2√ó14 + 0.1√ó15 - 3 = 1.3 ‚Üí p‚ÇÇ = œÉ(1.3) ‚âà 0.79
                                <br>‚Ä¢ √âtudiant 3 : z = 0.2√ó16 + 0.1√ó20 - 3 = 2.2 ‚Üí p‚ÇÉ = œÉ(2.2) ‚âà 0.90
                                <br>‚Ä¢ √âtudiant 4 : z = 0.2√ó18 + 0.1√ó25 - 3 = 3.1 ‚Üí p‚ÇÑ = œÉ(3.1) ‚âà 0.96</li>
                            <li><strong>Log-vraisemblance :</strong>
                                <br>‚Ñì = 0√óln(0.60) + (1-0)√óln(0.40) + 0√óln(0.79) + (1-0)√óln(0.21) + 1√óln(0.90) + 0√óln(0.10) + 1√óln(0.96) + 0√óln(0.04)
                                <br>‚Ñì = ln(0.40) + ln(0.21) + ln(0.90) + ln(0.96) ‚âà -0.92 - 1.56 - 0.11 - 0.04 = <strong>-2.63</strong></li>
                            <li><strong>Entropie crois√©e :</strong>
                                <br>J = -‚Ñì/n = -(-2.63)/4 = <strong>0.66</strong></li>
                            <li><strong>Interpr√©tation :</strong>
                                <br>J = 0.66 est mod√©r√©. Un mod√®le parfait aurait J ‚âà 0.
                                <br>Le mod√®le fait quelques erreurs : pr√©dit 60% pour un √©chec, 79% pour un √©chec.</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "Calcul du gradient : optimisation automatique",
            content: `
                        <p><strong>üéØ Pour optimiser, nous devons calculer :</strong></p>
                        <p>$$\\nabla_{\\vec{w}} J \\quad \\text{et} \\quad \\frac{\\partial J}{\\partial b}$$</p>
                        
                        <p><strong>üìê D√©rivation √©tape par √©tape :</strong></p>
                        
                        <p><strong>√âtape 1 :</strong> D√©riv√©e de l'entropie crois√©e</p>
                        <p>$$\\frac{\\partial J}{\\partial p_i} = -\\frac{1}{n}\\left[\\frac{y_i}{p_i} - \\frac{1-y_i}{1-p_i}\\right] = -\\frac{1}{n} \\cdot \\frac{y_i - p_i}{p_i(1-p_i)}$$</p>
                        
                        <p><strong>√âtape 2 :</strong> D√©riv√©e de la sigmo√Øde</p>
                        <p>$$\\frac{\\partial p_i}{\\partial z_i} = \\frac{\\partial \\sigma(z_i)}{\\partial z_i} = \\sigma(z_i)(1-\\sigma(z_i)) = p_i(1-p_i)$$</p>
                        
                        <p><strong>√âtape 3 :</strong> R√®gle de cha√Æne</p>
                        <p>$$\\frac{\\partial J}{\\partial z_i} = \\frac{\\partial J}{\\partial p_i} \\cdot \\frac{\\partial p_i}{\\partial z_i} = -\\frac{1}{n} \\cdot \\frac{y_i - p_i}{p_i(1-p_i)} \\cdot p_i(1-p_i) = \\frac{p_i - y_i}{n}$$</p>
                        
                        <p><strong>√âtape 4 :</strong> Gradients finaux</p>
                        <p>$$\\frac{\\partial z_i}{\\partial w_j} = x_{ij} \\quad \\text{et} \\quad \\frac{\\partial z_i}{\\partial b} = 1$$</p>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>üéâ R√©sultat final (forme √©l√©gante) :</strong><br><br>
                            
                            $$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{n}\\sum_{i=1}^{n} (p_i - y_i) x_{ij}$$<br><br>
                            
                            $$\\frac{\\partial J}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n} (p_i - y_i)$$<br><br>
                            
                            <strong>Forme vectorielle :</strong><br>
                            $$\\nabla_{\\vec{w}} J = \\frac{1}{n} X^T (\\vec{p} - \\vec{y})$$<br><br>
                            
                            Identique √† la r√©gression lin√©aire, mais avec \\(\\vec{p} = \\sigma(X\\vec{w} + b)\\) !
                        </div>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Exercice : calcul de gradient manuel",
            content: `
                        <p><strong>üéØ Exercice √† r√©soudre :</strong></p>
                        <p>Avec nos donn√©es d'admission et \\(w_1 = 0.2\\), \\(w_2 = 0.1\\), \\(b = -3\\) :</p>
                        
                        <p><strong>üìù Calculez le gradient :</strong></p>
                        <ol>
                            <li>Calculez \\(p_i - y_i\\) pour chaque √©tudiant</li>
                            <li>Calculez \\(\\frac{\\partial J}{\\partial w_1}\\)</li>
                            <li>Calculez \\(\\frac{\\partial J}{\\partial w_2}\\)</li>
                            <li>Calculez \\(\\frac{\\partial J}{\\partial b}\\)</li>
                            <li>Dans quelle direction faut-il modifier les poids pour r√©duire l'erreur ?</li>
                        </ol>
                        
                        <p><strong>‚úÖ Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('gradient-calculation-exercise')" style="margin-bottom: 1rem;">
                            üëÅÔ∏è Voir la solution
                        </button>
                        <div id="gradient-calculation-exercise" style="display: none;">
                        <ol>
                            <li><strong>Erreurs de pr√©diction :</strong>
                                <br>‚Ä¢ p‚ÇÅ - y‚ÇÅ = 0.60 - 0 = +0.60 (sur-estimation)
                                <br>‚Ä¢ p‚ÇÇ - y‚ÇÇ = 0.79 - 0 = +0.79 (sur-estimation)
                                <br>‚Ä¢ p‚ÇÉ - y‚ÇÉ = 0.90 - 1 = -0.10 (sous-estimation)
                                <br>‚Ä¢ p‚ÇÑ - y‚ÇÑ = 0.96 - 1 = -0.04 (sous-estimation)</li>
                            <li><strong>‚àÇJ/‚àÇw‚ÇÅ :</strong>
                                <br>= ¬º[(0.60√ó12) + (0.79√ó14) + (-0.10√ó16) + (-0.04√ó18)]
                                <br>= ¬º[7.2 + 11.06 - 1.6 - 0.72] = ¬º √ó 15.94 = <strong>+3.99</strong></li>
                            <li><strong>‚àÇJ/‚àÇw‚ÇÇ :</strong>
                                <br>= ¬º[(0.60√ó10) + (0.79√ó15) + (-0.10√ó20) + (-0.04√ó25)]
                                <br>= ¬º[6 + 11.85 - 2 - 1] = ¬º √ó 14.85 = <strong>+3.71</strong></li>
                            <li><strong>‚àÇJ/‚àÇb :</strong>
                                <br>= ¬º[0.60 + 0.79 + (-0.10) + (-0.04)] = ¬º √ó 1.25 = <strong>+0.31</strong></li>
                            <li><strong>Direction d'optimisation :</strong>
                                <br>Tous les gradients sont positifs ‚Üí il faut <strong>diminuer</strong> w‚ÇÅ, w‚ÇÇ et b
                                <br>Mise √† jour : w‚ÇÅ ‚Üê w‚ÇÅ - Œ±√ó3.99, w‚ÇÇ ‚Üê w‚ÇÇ - Œ±√ó3.71, b ‚Üê b - Œ±√ó0.31</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "code",
            title: "Impl√©mentation compl√®te from scratch",
            description: "Impl√©mentons la r√©gression logistique de A √† Z :",
            code: `import numpy as np
import matplotlib.pyplot as plt

class RegressionLogistique:
    """R√©gression logistique impl√©ment√©e from scratch"""
    
    def __init__(self, learning_rate=0.01, max_iterations=1000):
        self.lr = learning_rate
        self.max_iter = max_iterations
        self.weights = None
        self.bias = None
        self.costs = []
    
    def sigmoid(self, z):
        """Fonction sigmo√Øde avec protection contre overflow"""
        # Clip z pour √©viter overflow
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y):
        """Entra√Ænement du mod√®le"""
        n_samples, n_features = X.shape
        
        # Initialisation des param√®tres
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        print(f"üéØ ENTRA√éNEMENT R√âGRESSION LOGISTIQUE")
        print(f"Donn√©es : {n_samples} exemples, {n_features} features")
        print(f"Param√®tres : lr={self.lr}, max_iter={self.max_iter}")
        print("-" * 50)
        
        # Descente de gradient
        for i in range(self.max_iter):
            # Pr√©dictions
            z = X.dot(self.weights) + self.bias
            predictions = self.sigmoid(z)
            
            # Fonction de co√ªt (entropie crois√©e)
            cost = self.compute_cost(y, predictions)
            self.costs.append(cost)
            
            # Gradients
            dw = (1/n_samples) * X.T.dot(predictions - y)
            db = (1/n_samples) * np.sum(predictions - y)
            
            # Mise √† jour des param√®tres
            self.weights -= self.lr * dw
            self.bias -= self.lr * db
            
            # Affichage p√©riodique
            if i % 200 == 0:
                accuracy = self.accuracy(y, predictions > 0.5)
                print(f"It√©ration {i:4d}: Co√ªt={cost:.4f}, Pr√©cision={accuracy:.1%}")
        
        print(f"‚úÖ Entra√Ænement termin√© !")
        print(f"Poids finaux : {self.weights}")
        print(f"Biais final : {self.bias:.4f}")
    
    def compute_cost(self, y_true, y_pred):
        """Calcul de l'entropie crois√©e"""
        # Protection contre log(0)
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    
    def predict_proba(self, X):
        """Pr√©dictions probabilistes"""
        z = X.dot(self.weights) + self.bias
        return self.sigmoid(z)
    
    def predict(self, X):
        """Pr√©dictions binaires"""
        return (self.predict_proba(X) > 0.5).astype(int)
    
    def accuracy(self, y_true, y_pred):
        """Calcul de la pr√©cision"""
        return np.mean(y_true == y_pred)

# Donn√©es d'admission UCAD
X = np.array([[12, 10],  # Note bac, heures/semaine
              [14, 15],
              [16, 20], 
              [18, 25],
              [13, 12],
              [15, 18],
              [17, 22],
              [19, 28]])

y = np.array([0, 0, 1, 1, 0, 1, 1, 1])  # Admis (1) ou refus√© (0)

print("üìä DONN√âES D'ADMISSION UCAD")
print("Note Bac | Heures/sem | Admis")
print("-" * 30)
for i in range(len(X)):
    status = "‚úÖ Oui" if y[i] == 1 else "‚ùå Non"
    print(f"{X[i,0]:8.0f} | {X[i,1]:9.0f} | {status}")

# Entra√Ænement
model = RegressionLogistique(learning_rate=0.1, max_iterations=1000)
model.fit(X, y)`,
          },
          {
            type: "code",
            title: "Visualisation et analyse",
            description:
              "Visualisons la fronti√®re de d√©cision et les r√©sultats :",
            code: `# √âvaluation finale
predictions_proba = model.predict_proba(X)
predictions_binaires = model.predict(X)
accuracy_finale = model.accuracy(y, predictions_binaires)

print(f"\\nüìä R√âSULTATS FINAUX")
print(f"Pr√©cision : {accuracy_finale:.1%}")
print("\\nPr√©dictions d√©taill√©es :")
print("√âtudiant | R√©el | Proba | Pr√©diction")
print("-" * 40)
for i in range(len(X)):
    reel = "Admis" if y[i] == 1 else "Refus√©"
    pred = "Admis" if predictions_binaires[i] == 1 else "Refus√©"
    print(f"{i+1:8d} | {reel:5s} | {predictions_proba[i]:5.1%} | {pred}")

# Visualisation de la fronti√®re de d√©cision
plt.figure(figsize=(12, 8))

# Points de donn√©es
colors = ['red' if label == 0 else 'green' for label in y]
plt.scatter(X[:, 0], X[:, 1], c=colors, s=100, alpha=0.7)

# Fronti√®re de d√©cision : w1*x1 + w2*x2 + b = 0
# Donc x2 = -(w1*x1 + b) / w2
x1_range = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100)
x2_boundary = -(model.weights[0] * x1_range + model.bias) / model.weights[1]

plt.plot(x1_range, x2_boundary, 'b-', linewidth=2, 
         label=f'Fronti√®re: {model.weights[0]:.2f}√ónote + {model.weights[1]:.2f}√óheures + {model.bias:.2f} = 0')

plt.xlabel('Note au Bac')
plt.ylabel('Heures d\'√©tude par semaine')
plt.title('Classification : Admission UCAD')
plt.legend()
plt.grid(True, alpha=0.3)

# Ajouter les √©tiquettes
for i in range(len(X)):
    plt.annotate(f'{predictions_proba[i]:.0%}', 
                (X[i, 0], X[i, 1]), 
                xytext=(5, 5), textcoords='offset points', fontsize=8)

plt.show()

# Courbe d'apprentissage
plt.figure(figsize=(10, 6))
plt.plot(model.costs, 'b-', linewidth=2)
plt.title('Courbe d\'Apprentissage - Entropie Crois√©e')
plt.xlabel('It√©rations')
plt.ylabel('Co√ªt (Entropie Crois√©e)')
plt.grid(True, alpha=0.3)
plt.show()

print(f"\\nüéØ Interpr√©tation des coefficients :")
print(f"Note Bac (w‚ÇÅ={model.weights[0]:.3f}) : +1 point ‚Üí +{model.weights[0]:.1%} de chance")
print(f"Heures (w‚ÇÇ={model.weights[1]:.3f}) : +1h/sem ‚Üí +{model.weights[1]:.1%} de chance")`,
          },
          {
            type: "concept",
            icon: "üí°",
            title: "R√©gularisation : contr√¥ler la complexit√©",
            content: `
                        <p><strong>ü§î Probl√®me du sur-apprentissage :</strong></p>
                        <p>Avec beaucoup de features, le mod√®le peut "m√©moriser" les donn√©es d'entra√Ænement au lieu d'apprendre les vrais patterns.</p>
                        
                        <p><strong>üí° Solution :</strong> P√©naliser les poids trop grands avec la r√©gularisation !</p>
                        
                        <p><strong>üìê Fonction de co√ªt r√©gularis√©e :</strong></p>
                        <p>$$J_{reg}(\\vec{w}, b) = J(\\vec{w}, b) + \\lambda \\cdot R(\\vec{w})$$</p>
                        
                        <p><strong>üéØ Deux types principaux :</strong></p>
                        
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>R√©gularisation L2 (Ridge) :</strong><br>
                            $$R(\\vec{w}) = ||\\vec{w}||_2^2 = \\sum_{j=1}^{n} w_j^2$$<br><br>
                            
                            <strong>Effet :</strong> R√©duit uniform√©ment tous les poids<br>
                            <strong>Gradient modifi√© :</strong> \\(\\nabla_{\\vec{w}} J_{reg} = \\nabla_{\\vec{w}} J + 2\\lambda\\vec{w}\\)<br>
                            <strong>Interpr√©tation :</strong> "Tire" tous les poids vers z√©ro
                        </div>
                        
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>R√©gularisation L1 (Lasso) :</strong><br>
                            $$R(\\vec{w}) = ||\\vec{w}||_1 = \\sum_{j=1}^{n} |w_j|$$<br><br>
                            
                            <strong>Effet :</strong> Force certains poids √† exactement 0<br>
                            <strong>Gradient modifi√© :</strong> \\(\\nabla_{\\vec{w}} J_{reg} = \\nabla_{\\vec{w}} J + \\lambda \\cdot \\text{sign}(\\vec{w})\\)<br>
                            <strong>Interpr√©tation :</strong> S√©lection automatique de features
                        </div>
                        
                        <p><strong>‚öñÔ∏è Choix du param√®tre Œª :</strong></p>
                        <ul>
                            <li>Œª = 0 : pas de r√©gularisation (risque d'overfitting)</li>
                            <li>Œª petit : r√©gularisation l√©g√®re</li>
                            <li>Œª grand : r√©gularisation forte (risque d'underfitting)</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Exercice : impact de la r√©gularisation",
            content: `
                        <p><strong>üéØ Exercice √† r√©soudre :</strong></p>
                        <p>Avec nos donn√©es d'admission, comparons 3 mod√®les :</p>
                        <ul>
                            <li><strong>Mod√®le A :</strong> Œª = 0 (pas de r√©gularisation)</li>
                            <li><strong>Mod√®le B :</strong> Œª = 0.1 (r√©gularisation L2 l√©g√®re)</li>
                            <li><strong>Mod√®le C :</strong> Œª = 1.0 (r√©gularisation L2 forte)</li>
                        </ul>
                        
                        <p><strong>üìù Pr√©disez :</strong></p>
                        <ol>
                            <li>Quel mod√®le aura les poids les plus grands ?</li>
                            <li>Quel mod√®le aura la meilleure pr√©cision sur les donn√©es d'entra√Ænement ?</li>
                            <li>Quel mod√®le g√©n√©ralisera le mieux sur de nouvelles donn√©es ?</li>
                            <li>Comment √©volue la norme des poids avec Œª ?</li>
                        </ol>
                        
                        <p><strong>‚úÖ Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('regularization-exercise')" style="margin-bottom: 1rem;">
                            üëÅÔ∏è Voir la solution
                        </button>
                        <div id="regularization-exercise" style="display: none;">
                        <ol>
                            <li><strong>Poids les plus grands :</strong> Mod√®le A (Œª=0)
                                <br>Sans r√©gularisation, rien ne limite la croissance des poids</li>
                            <li><strong>Meilleure pr√©cision train :</strong> Mod√®le A (Œª=0)
                                <br>Il peut "coller" parfaitement aux donn√©es d'entra√Ænement</li>
                            <li><strong>Meilleure g√©n√©ralisation :</strong> Mod√®le B (Œª=0.1)
                                <br>√âquilibre optimal entre ajustement et simplicit√©</li>
                            <li><strong>√âvolution de ||w|| :</strong>
                                <br>||w|| d√©cro√Æt quand Œª augmente
                                <br>Œª=0 : ||w|| ‚âà 3.2
                                <br>Œª=0.1 : ||w|| ‚âà 2.1
                                <br>Œª=1.0 : ||w|| ‚âà 0.8</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "Extension multi-classe : softmax",
            content: `
                        <p><strong>üéØ Pour K classes (K > 2), on g√©n√©ralise avec softmax :</strong></p>
                        
                        <p><strong>üìê Architecture multi-classe :</strong></p>
                        <ul>
                            <li>Un vecteur de poids par classe : \\(\\vec{w}_1, \\vec{w}_2, ..., \\vec{w}_K\\)</li>
                            <li>Un biais par classe : \\(b_1, b_2, ..., b_K\\)</li>
                        </ul>
                        
                        <p><strong>√âtape 1 :</strong> Calcul des scores (logits)</p>
                        <p>$$z_k = \\vec{w}_k^T\\vec{x} + b_k \\quad \\text{pour } k = 1, ..., K$$</p>
                        
                        <p><strong>√âtape 2 :</strong> Transformation softmax</p>
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            $$P(y=k|\\vec{x}) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$$<br><br>
                            
                            <strong>Propri√©t√©s du softmax :</strong><br>
                            ‚Ä¢ \\(\\sum_{k=1}^{K} P(y=k|\\vec{x}) = 1\\) (vraie distribution de probabilit√©)<br>
                            ‚Ä¢ \\(0 < P(y=k|\\vec{x}) < 1\\) pour tout k<br>
                            ‚Ä¢ Si K=2, √©quivalent √† la sigmo√Øde<br>
                            ‚Ä¢ Invariant par translation : ajouter c √† tous les \\(z_k\\) ne change rien
                        </div>
                        
                        <p><strong>√âtape 3 :</strong> Fonction de co√ªt (entropie crois√©e cat√©gorielle)</p>
                        <p>Avec encodage one-hot : \\(\\vec{y}_i \\in \\{0,1\\}^K\\) o√π \\(\\sum_k y_{ik} = 1\\)</p>
                        <p>$$J = -\\frac{1}{n}\\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{ik} \\log(p_{ik})$$</p>
                        
                        <p><strong>üéâ Gradient (forme √©l√©gante) :</strong></p>
                        <p>$$\\nabla_{\\vec{w}_k} J = \\frac{1}{n}\\sum_{i=1}^{n} (p_{ik} - y_{ik}) \\vec{x}_i$$</p>
                        
                        <p><strong>üí° M√™me forme qu'en binaire !</strong> La beaut√© des math√©matiques.</p>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Exercice : classification 3 classes",
            content: `
                        <p><strong>üéØ Exercice √† r√©soudre :</strong></p>
                        <p>Classifions les √©tudiants en 3 cat√©gories selon leurs r√©sultats :</p>
                        <ul>
                            <li><strong>Classe 0 :</strong> √âchec (< 10/20)</li>
                            <li><strong>Classe 1 :</strong> Passable (10-14/20)</li>
                            <li><strong>Classe 2 :</strong> Bien (‚â• 15/20)</li>
                        </ul>
                        
                        <p><strong>Donn√©es :</strong> Note = 13, Heures = 18</p>
                        <p><strong>Scores calcul√©s :</strong> z‚ÇÄ = -1.2, z‚ÇÅ = 0.8, z‚ÇÇ = -0.3</p>
                        
                        <p><strong>üìù Calculez :</strong></p>
                        <ol>
                            <li>Les probabilit√©s softmax pour chaque classe</li>
                            <li>La pr√©diction finale (classe la plus probable)</li>
                            <li>La confiance du mod√®le (probabilit√© max)</li>
                            <li>Si la vraie classe est 1, calculez la perte</li>
                        </ol>
                        
                        <p><strong>‚úÖ Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('multiclass-exercise')" style="margin-bottom: 1rem;">
                            üëÅÔ∏è Voir la solution
                        </button>
                        <div id="multiclass-exercise" style="display: none;">
                        <ol>
                            <li><strong>Probabilit√©s softmax :</strong>
                                <br>D√©nominateur : e‚Åª¬π¬∑¬≤ + e‚Å∞¬∑‚Å∏ + e‚Åª‚Å∞¬∑¬≥ ‚âà 0.30 + 2.23 + 0.74 = 3.27
                                <br>P(classe 0) = 0.30/3.27 ‚âà <strong>0.09 (9%)</strong>
                                <br>P(classe 1) = 2.23/3.27 ‚âà <strong>0.68 (68%)</strong>
                                <br>P(classe 2) = 0.74/3.27 ‚âà <strong>0.23 (23%)</strong></li>
                            <li><strong>Pr√©diction :</strong> Classe 1 (probabilit√© maximale = 68%)</li>
                            <li><strong>Confiance :</strong> 68% (mod√©r√©ment confiant)</li>
                            <li><strong>Perte :</strong> Si vraie classe = 1, alors y = [0, 1, 0]
                                <br>Perte = -log(0.68) ‚âà <strong>0.39</strong></li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "code",
            title: "Impl√©mentation multi-classe compl√®te",
            description:
              "Impl√©mentons la classification multi-classe avec softmax :",
            code: `class ClassificationMultiClasse:
    """Classification multi-classe avec softmax"""
    
    def __init__(self, n_classes, learning_rate=0.01, max_iterations=1000):
        self.n_classes = n_classes
        self.lr = learning_rate
        self.max_iter = max_iterations
        self.weights = None
        self.bias = None
        self.costs = []
    
    def softmax(self, Z):
        """Fonction softmax avec stabilit√© num√©rique"""
        # Soustraction du max pour stabilit√©
        Z_stable = Z - np.max(Z, axis=1, keepdims=True)
        exp_Z = np.exp(Z_stable)
        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)
    
    def one_hot_encode(self, y):
        """Encodage one-hot"""
        encoded = np.zeros((len(y), self.n_classes))
        encoded[np.arange(len(y)), y] = 1
        return encoded
    
    def fit(self, X, y):
        """Entra√Ænement multi-classe"""
        n_samples, n_features = X.shape
        
        # Initialisation
        self.weights = np.random.normal(0, 0.01, (n_features, self.n_classes))
        self.bias = np.zeros(self.n_classes)
        
        # Encodage one-hot
        y_encoded = self.one_hot_encode(y)
        
        print(f"üéØ CLASSIFICATION MULTI-CLASSE")
        print(f"Classes : {self.n_classes}, Features : {n_features}")
        print("-" * 40)
        
        for i in range(self.max_iter):
            # Forward pass
            Z = X.dot(self.weights) + self.bias
            predictions = self.softmax(Z)
            
            # Co√ªt (entropie crois√©e cat√©gorielle)
            cost = -np.mean(np.sum(y_encoded * np.log(predictions + 1e-15), axis=1))
            self.costs.append(cost)
            
            # Gradients
            dW = (1/n_samples) * X.T.dot(predictions - y_encoded)
            db = (1/n_samples) * np.sum(predictions - y_encoded, axis=0)
            
            # Mise √† jour
            self.weights -= self.lr * dW
            self.bias -= self.lr * db
            
            if i % 200 == 0:
                accuracy = np.mean(np.argmax(predictions, axis=1) == y)
                print(f"It√©ration {i:4d}: Co√ªt={cost:.4f}, Pr√©cision={accuracy:.1%}")
        
        print("‚úÖ Entra√Ænement multi-classe termin√© !")
    
    def predict_proba(self, X):
        """Pr√©dictions probabilistes"""
        Z = X.dot(self.weights) + self.bias
        return self.softmax(Z)
    
    def predict(self, X):
        """Pr√©dictions de classe"""
        probas = self.predict_proba(X)
        return np.argmax(probas, axis=1)

# Test avec donn√©es de notes
notes_data = np.array([[8, 5],   # Note, heures ‚Üí √âchec
                      [12, 15],  # ‚Üí Passable  
                      [16, 25],  # ‚Üí Bien
                      [9, 8],    # ‚Üí √âchec
                      [13, 18],  # ‚Üí Passable
                      [17, 30]]) # ‚Üí Bien

# Classes : 0=√âchec, 1=Passable, 2=Bien
classes = np.array([0, 1, 2, 0, 1, 2])

# Entra√Ænement
model_multi = ClassificationMultiClasse(n_classes=3, learning_rate=0.1)
model_multi.fit(notes_data, classes)

# Test sur nouveaux √©tudiants
nouveaux_etudiants = np.array([[11, 12],  # Cas limite
                              [15, 22],  # Bon √©tudiant
                              [7, 6]])   # Faible niveau

probas = model_multi.predict_proba(nouveaux_etudiants)
predictions = model_multi.predict(nouveaux_etudiants)

print(f"\\nüéì PR√âDICTIONS POUR NOUVEAUX √âTUDIANTS")
classes_noms = ['√âchec', 'Passable', 'Bien']
for i, (note, heures) in enumerate(nouveaux_etudiants):
    print(f"\\n√âtudiant {i+1}: Note={note}, Heures={heures}")
    print(f"Pr√©diction: {classes_noms[predictions[i]]}")
    for j, classe in enumerate(classes_noms):
        print(f"  {classe}: {probas[i,j]:.1%}")`,
          },
          {
            type: "concept",
            icon: "üí°",
            title: "M√©triques d'√©valuation : au-del√† de la pr√©cision",
            content: `
                        <p><strong>üéØ La pr√©cision seule peut √™tre trompeuse !</strong></p>
                        
                        <p><strong>üìä Exemple probl√©matique :</strong></p>
                        <p>D√©tection de fraude bancaire : 99% des transactions sont l√©gitimes</p>
                        <ul>
                            <li>ü§ñ <strong>Mod√®le na√Øf</strong> : "Toutes les transactions sont l√©gitimes"</li>
                            <li>üìà <strong>Pr√©cision</strong> : 99% (excellent ?)</li>
                            <li>‚ùå <strong>Probl√®me</strong> : 0% des fraudes d√©tect√©es !</li>
                        </ul>
                        
                        <p><strong>üîç Matrice de confusion (2√ó2) :</strong></p>
                        <table style="margin: 1rem auto; border-collapse: collapse;">
                            <tr>
                                <td style="padding: 0.5rem; border: 1px solid #ddd;"></td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #f8f9fa;" colspan="2"><strong>Pr√©diction</strong></td>
                            </tr>
                            <tr>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #f8f9fa;"><strong>R√©alit√©</strong></td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd;">N√©gatif</td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd;">Positif</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.5rem; border: 1px solid #ddd;">N√©gatif</td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #d4edda;"><strong>VN</strong><br>Vrais N√©gatifs</td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #f8d7da;"><strong>FP</strong><br>Faux Positifs</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.5rem; border: 1px solid #ddd;">Positif</td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #f8d7da;"><strong>FN</strong><br>Faux N√©gatifs</td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #d4edda;"><strong>VP</strong><br>Vrais Positifs</td>
                            </tr>
                        </table>
                        
                        <p><strong>üìê M√©triques d√©riv√©es :</strong></p>
                        <ul style="list-style: none; padding-left: 0">
                            <li><strong>‚Ä¢ Pr√©cision :</strong> $$\\text{Precision} = \\frac{VP}{VP + FP}$$ (parmi les positifs pr√©dits, combien sont vrais ?)</li>
                            <li style="margin-top: 0.5rem"><strong>‚Ä¢ Rappel :</strong> $$\\text{Recall} = \\frac{VP}{VP + FN}$$ (parmi les vrais positifs, combien sont d√©tect√©s ?)</li>
                            <li style="margin-top: 0.5rem"><strong>‚Ä¢ F1-Score :</strong> $$F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$ (moyenne harmonique)</li>
                            <li style="margin-top: 0.5rem"><strong>‚Ä¢ Sp√©cificit√© :</strong> $$\\text{Specificity} = \\frac{VN}{VN + FP}$$ (taux de vrais n√©gatifs)</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Exercice : calcul de m√©triques",
            content: `
                        <p><strong>üéØ Exercice √† r√©soudre :</strong></p>
                        <p>Un mod√®le de d√©tection de spam a test√© 1000 emails :</p>
                        
                        <table style="margin: 1rem auto; border-collapse: collapse;">
                            <tr>
                                <td style="padding: 0.5rem; border: 1px solid #ddd;"></td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #f8f9fa;" colspan="2"><strong>Pr√©diction</strong></td>
                            </tr>
                            <tr>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #f8f9fa;"><strong>R√©alit√©</strong></td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd;">Pas Spam</td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd;">Spam</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.5rem; border: 1px solid #ddd;">Pas Spam</td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #d4edda;"><strong>920</strong></td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #f8d7da;"><strong>30</strong></td>
                            </tr>
                            <tr>
                                <td style="padding: 0.5rem; border: 1px solid #ddd;">Spam</td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #f8d7da;"><strong>10</strong></td>
                                <td style="padding: 0.5rem; border: 1px solid #ddd; background: #d4edda;"><strong>40</strong></td>
                            </tr>
                        </table>
                        
                        <p><strong>üìù Calculez :</strong></p>
                        <ol>
                            <li>Pr√©cision (Precision)</li>
                            <li>Rappel (Recall)</li>
                            <li>F1-Score</li>
                            <li>Sp√©cificit√©</li>
                            <li>Ce mod√®le est-il bon pour d√©tecter les spams ?</li>
                        </ol>
                        
                        <p><strong>‚úÖ Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('metrics-exercise')" style="margin-bottom: 1rem;">
                            üëÅÔ∏è Voir la solution
                        </button>
                        <div id="metrics-exercise" style="display: none;">
                        <ol>
                            <li><strong>Pr√©cision :</strong> VP/(VP+FP) = 40/(40+30) = 40/70 ‚âà <strong>57%</strong>
                                <br>Parmi les emails class√©s spam, 57% le sont vraiment</li>
                            <li><strong>Rappel :</strong> VP/(VP+FN) = 40/(40+10) = 40/50 = <strong>80%</strong>
                                <br>80% des vrais spams sont d√©tect√©s</li>
                            <li><strong>F1-Score :</strong> 2√ó(0.57√ó0.80)/(0.57+0.80) = 2√ó0.456/1.37 ‚âà <strong>67%</strong></li>
                            <li><strong>Sp√©cificit√© :</strong> VN/(VN+FP) = 920/(920+30) = 920/950 ‚âà <strong>97%</strong>
                                <br>97% des vrais non-spams sont correctement identifi√©s</li>
                            <li><strong>√âvaluation :</strong> Mod√®le correct mais perfectible
                                <br>‚Ä¢ Bon rappel (80%) : d√©tecte la plupart des spams
                                <br>‚Ä¢ Pr√©cision moyenne (57%) : trop de faux positifs
                                <br>‚Ä¢ Am√©lioration possible : ajuster le seuil ou plus de donn√©es</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "warning",
            icon: "‚ö†Ô∏è",
            title: "Limites et extensions de la r√©gression logistique",
            content: `
                        <p><strong>‚ö†Ô∏è Hypoth√®ses et limites :</strong></p>
                        
                        <p><strong>üîç Hypoth√®ses du mod√®le :</strong></p>
                        <ul>
                            <li>üìä <strong>Lin√©arit√©</strong> : relation lin√©aire entre features et log-odds</li>
                            <li>üéØ <strong>Ind√©pendance</strong> : observations ind√©pendantes</li>
                            <li>üìà <strong>Pas de multicolin√©arit√©</strong> : features pas trop corr√©l√©es</li>
                            <li>üìã <strong>√âchantillon suffisant</strong> : au moins 10 exemples par feature</li>
                        </ul>
                        
                        <p><strong>‚ùå Quand la r√©gression logistique √©choue :</strong></p>
                        <ul>
                            <li>üåÄ <strong>Relations non-lin√©aires</strong> : patterns complexes (XOR)</li>
                            <li>üîó <strong>Interactions complexes</strong> : features qui se combinent</li>
                            <li>üìä <strong>Classes non s√©parables</strong> : pas de fronti√®re lin√©aire</li>
                            <li>üéØ <strong>Donn√©es d√©s√©quilibr√©es</strong> : 99% classe A, 1% classe B</li>
                        </ul>
                        
                        <p><strong>üöÄ Extensions et alternatives :</strong></p>
                        <ul>
                            <li>üîß <strong>Features polynomiales</strong> : x, x¬≤, x¬≥, xy pour non-lin√©arit√©</li>
                            <li>üéØ <strong>SVM</strong> : marges maximales, kernel trick</li>
                            <li>üå≥ <strong>Arbres de d√©cision</strong> : r√®gles if-then naturelles</li>
                            <li>üß† <strong>R√©seaux de neurones</strong> : compositions de sigmo√Ødes</li>
                            <li>üé≤ <strong>Naive Bayes</strong> : approche probabiliste pure</li>
                        </ul>
                        
                        <p><strong>üí° Point cl√© :</strong> La r√©gression logistique est simple, interpr√©table et efficace. C'est souvent le premier mod√®le √† essayer ! Si elle ne suffit pas, on peut toujours complexifier.</p>
                        
                        <p><strong>üîÆ Prochaine √©tape :</strong> Clustering - d√©couvrir des groupes cach√©s sans √©tiquettes !</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ü§î Pourquoi la r√©gularisation L1 peut-elle mettre des poids √† exactement z√©ro, mais pas L2 ?",
          options: [
            "A) L1 est plus forte que L2",
            "B) La d√©riv√©e de |w| a une discontinuit√© en 0 qui 'pousse' vers z√©ro",
            "C) C'est un bug de l'algorithme",
            "D) L2 met aussi des poids √† z√©ro",
          ],
          correct: 1,
          explanation:
            "La d√©riv√©e de L2 (w¬≤) est 2w, qui tend vers 0 quand w‚Üí0 : le gradient diminue progressivement. La d√©riv√©e de L1 (|w|) est sign(w) = ¬±1, constante jusqu'√† w=0 : le gradient 'pousse' avec force constante vers z√©ro. G√©om√©triquement, L1 cr√©e une contrainte en losange avec des 'coins' sur les axes (w=0), tandis que L2 cr√©e une contrainte circulaire sans points privil√©gi√©s.",
        },
        prevModule: "linear-regression.html",
        nextModule: "clustering.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
