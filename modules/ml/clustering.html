<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Clustering | IA4Ndada</title>

    <!-- MathJax pour les formules mathÃ©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">ğŸ  Accueil</a>
          <span>â€º</span>
          <span>ğŸ¤– Machine Learning</span>
          <span>â€º</span>
          <span>Clustering</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>ğŸ” Clustering : DÃ©couvrir des Groupes CachÃ©s</h1>
      <p class="subtitle">Module 3.4 - Apprentissage Non-SupervisÃ©</p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>ğŸ¯ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajoutÃ©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajoutÃ© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>ğŸ‰ Checkpoint - Clustering</h3>
        <p>
          FÃ©licitations ! Vous maÃ®trisez maintenant l'art de dÃ©couvrir des
          structures cachÃ©es dans les donnÃ©es.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme complÃ©tÃ©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="classification.html" class="nav-link" id="prev-link"
          >â† Module prÃ©cÃ©dent : Classification</a
        >
        <a href="validation.html" class="nav-link" id="next-link"
          >Module suivant : Validation â†’</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module Clustering
      const moduleConfig = {
        id: "ml-clustering",
        title: "Clustering : DÃ©couvrir des Groupes CachÃ©s",
        category: "Machine Learning",
        objectives: [
          "Comprendre le changement de paradigme : pas d'Ã©tiquettes",
          "MaÃ®triser complÃ¨tement l'algorithme K-means",
          "Calculer K-means Ã  la main sur un exemple simple",
          "Comprendre pourquoi il converge mathÃ©matiquement",
          "Identifier ses limites et alternatives",
        ],
        content: [
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Le changement de paradigme : dÃ©couvrir sans guide",
            content: `
                        <p>Le <strong>clustering</strong> marque une rÃ©volution conceptuelle : pour la premiÃ¨re fois, la machine doit dÃ©couvrir des patterns <strong>sans qu'on lui dise quoi chercher</strong>.</p>
                        
                        <p><strong>ğŸ”‘ DiffÃ©rence fondamentale :</strong></p>
                        <ul>
                            <li>ğŸ“š <strong>Apprentissage supervisÃ©</strong> : "Voici des chats et des chiens Ã©tiquetÃ©s. Apprends Ã  les distinguer."</li>
                            <li>ğŸ” <strong>Apprentissage non-supervisÃ©</strong> : "Voici des animaux. Y a-t-il des groupes naturels ?"</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Applications concrÃ¨tes :</strong></p>
                        <ul>
                            <li>ğŸ›’ <strong>Segmentation clients</strong> : dÃ©couvrir des profils d'acheteurs sans les connaÃ®tre Ã  l'avance</li>
                            <li>ğŸ§¬ <strong>Analyse gÃ©nÃ©tique</strong> : identifier des sous-types de maladies</li>
                            <li>ğŸ“± <strong>RÃ©seaux sociaux</strong> : dÃ©tecter des communautÃ©s d'intÃ©rÃªts</li>
                            <li>ğŸµ <strong>Recommandations</strong> : grouper des utilisateurs aux goÃ»ts similaires</li>
                            <li>ğŸ–¼ï¸ <strong>Compression d'images</strong> : regrouper les couleurs similaires</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Philosophie :</strong> Le clustering rÃ©vÃ¨le la <strong>structure cachÃ©e</strong> des donnÃ©es. C'est comme Ãªtre un explorateur qui dÃ©couvre des territoires inconnus et y trouve des patterns naturels.</p>
                    `,
          },
          {
            type: "intuition",
            icon: "ğŸ§ ",
            title: "L'analogie de l'organisation d'une bibliothÃ¨que",
            content: `
                        <p>Imaginez que vous devez <strong>organiser une bibliothÃ¨que</strong> avec 10 000 livres mÃ©langÃ©s, sans systÃ¨me de classification :</p>
                        
                        <p><strong>ğŸ¯ Votre mission :</strong> CrÃ©er des sections logiques pour que les lecteurs trouvent facilement ce qu'ils cherchent.</p>
                        
                        <p><strong>ğŸ” Votre approche naturelle :</strong></p>
                        <ol>
                            <li>ğŸ“– <strong>Observer</strong> : regarder les livres et noter leurs caractÃ©ristiques</li>
                            <li>ğŸ¯ <strong>Grouper</strong> : mettre ensemble les livres qui se ressemblent</li>
                            <li>ğŸ·ï¸ <strong>Ajuster</strong> : dÃ©placer les livres mal classÃ©s</li>
                            <li>ğŸ”„ <strong>RÃ©pÃ©ter</strong> jusqu'Ã  obtenir des sections cohÃ©rentes</li>
                        </ol>
                        
                        <p><strong>ğŸ“š RÃ©sultat final :</strong></p>
                        <ul>
                            <li>ğŸ“– <strong>Section Romans</strong> : livres avec histoires fictives</li>
                            <li>ğŸ”¬ <strong>Section Sciences</strong> : livres techniques et formules</li>
                            <li>ğŸ“š <strong>Section Histoire</strong> : livres sur le passÃ©</li>
                            <li>ğŸ¨ <strong>Section Arts</strong> : livres avec beaucoup d'images</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ C'est exactement ce que fait K-means :</strong></p>
                        <ul>
                            <li>ğŸ” <strong>Observer</strong> : analyser les caractÃ©ristiques des donnÃ©es</li>
                            <li>ğŸ¯ <strong>Grouper</strong> : assigner chaque point au groupe le plus proche</li>
                            <li>ğŸ“ <strong>Recentrer</strong> : dÃ©placer le "centre" de chaque groupe</li>
                            <li>ğŸ”„ <strong>RÃ©pÃ©ter</strong> jusqu'Ã  stabilitÃ©</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Formalisation rigoureuse du problÃ¨me",
            content: `
                        <p><strong>ğŸ“ Posons le problÃ¨me mathÃ©matiquement :</strong></p>
                        
                        <p><strong>DonnÃ©es :</strong> \\(\\mathcal{X} = \\{\\vec{x}_1, \\vec{x}_2, ..., \\vec{x}_n\\}\\) avec \\(\\vec{x}_i \\in \\mathbb{R}^d\\)</p>
                        
                        <p><strong>Objectif :</strong> Partitionner \\(\\mathcal{X}\\) en K clusters \\(C_1, C_2, ..., C_K\\)</p>
                        
                        <p><strong>ğŸ¯ Fonction objectif (inertie intra-cluster) :</strong></p>
                        <p>$$J(\\mathcal{C}, \\mathcal{M}) = \\sum_{k=1}^{K} \\sum_{\\vec{x}_i \\in C_k} ||\\vec{x}_i - \\vec{\\mu}_k||^2$$</p>
                        
                        <p><strong>ğŸ” DÃ©cryptage de la formule :</strong></p>
                        <ul>
                            <li>\\(\\mathcal{C} = \\{C_1, ..., C_K\\}\\) = <strong>partition des donnÃ©es</strong></li>
                            <li>\\(\\mathcal{M} = \\{\\vec{\\mu}_1, ..., \\vec{\\mu}_K\\}\\) = <strong>centres des clusters</strong></li>
                            <li>\\(||\\vec{x}_i - \\vec{\\mu}_k||^2\\) = <strong>distanceÂ² euclidienne</strong> (voir <a href="../math/vectors.html">Module 1.1</a>)</li>
                            <li>\\(J\\) = <strong>somme totale des distancesÂ²</strong> de chaque point Ã  son centre</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ ProblÃ¨me d'optimisation :</strong></p>
                        <p>$$\\min_{\\mathcal{C}, \\mathcal{M}} J(\\mathcal{C}, \\mathcal{M})$$</p>
                        
                        <p><strong>âš ï¸ ComplexitÃ© :</strong> Ce problÃ¨me est <strong>NP-difficile</strong> ! Il y a \\(K^n\\) faÃ§ons d'assigner n points Ã  K clusters.</p>
                        
                        <p><strong>ğŸ’¡ Solution de K-means :</strong> DÃ©composer en deux sous-problÃ¨mes plus simples qui s'alternent.</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Les deux sous-problÃ¨mes de K-means",
            content: `
                        <p><strong>ğŸ§© K-means rÃ©sout le problÃ¨me NP-difficile en l'alternant entre deux sous-problÃ¨mes simples :</strong></p>
                        
                        <p><strong>1ï¸âƒ£ Assignation optimale (centres fixes) :</strong></p>
                        <p>Pour chaque point \\(\\vec{x}_i\\), trouver le cluster qui minimise la distance :</p>
                        <p>$$c^*(\\vec{x}_i) = \\arg\\min_{k \\in \\{1,...,K\\}} ||\\vec{x}_i - \\vec{\\mu}_k||^2$$</p>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>ğŸ’¡ Solution triviale :</strong> Choisir le centre le plus proche !<br>
                            C'est un simple calcul de distances.
                        </div>
                        
                        <p><strong>2ï¸âƒ£ Centres optimaux (assignations fixes) :</strong></p>
                        <p>Pour le cluster k, minimiser :</p>
                        <p>$$\\sum_{\\vec{x}_i \\in C_k} ||\\vec{x}_i - \\vec{\\mu}_k||^2$$</p>
                        
                        <p><strong>ğŸ”§ DÃ©rivation (voir <a href="../math/derivatives.html">Module 1.4</a>) :</strong></p>
                        <p>$$\\frac{\\partial}{\\partial \\vec{\\mu}_k} \\sum_{\\vec{x}_i \\in C_k} ||\\vec{x}_i - \\vec{\\mu}_k||^2 = \\frac{\\partial}{\\partial \\vec{\\mu}_k} \\sum_{\\vec{x}_i \\in C_k} (\\vec{x}_i - \\vec{\\mu}_k)^T(\\vec{x}_i - \\vec{\\mu}_k)$$</p>
                        
                        <p>$$= -2\\sum_{\\vec{x}_i \\in C_k} (\\vec{x}_i - \\vec{\\mu}_k) = 0$$</p>
                        
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>âœ… Solution Ã©lÃ©gante :</strong><br>
                            $$\\vec{\\mu}_k^* = \\frac{1}{|C_k|} \\sum_{\\vec{x}_i \\in C_k} \\vec{x}_i$$<br><br>
                            
                            Le centre optimal est la <strong>moyenne arithmÃ©tique</strong> des points du cluster !<br>
                            C'est pourquoi on l'appelle "K-<strong>means</strong>".
                        </div>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Calcul manuel complet sur 4 points",
            content: `
                        <p><strong>ğŸ“ Exemple concret :</strong> Groupons 4 villes sÃ©nÃ©galaises en 2 clusters selon leurs coordonnÃ©es Ã©conomiques.</p>
                        
                        <p><strong>ğŸ™ï¸ DonnÃ©es :</strong> [PIB/habitant, Taux d'alphabÃ©tisation]</p>
                        <table style="margin: 1rem auto; border-collapse: collapse; text-align: center;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Ville</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">PIB/hab</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">AlphabÃ©tisation</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Point</th>
                            </tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">Dakar</td><td style="padding: 0.5rem; border: 1px solid #ddd;">4</td><td style="padding: 0.5rem; border: 1px solid #ddd;">3</td><td style="padding: 0.5rem; border: 1px solid #ddd;">A(4,3)</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">ThiÃ¨s</td><td style="padding: 0.5rem; border: 1px solid #ddd;">3</td><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">B(3,2)</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">Kaolack</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td><td style="padding: 0.5rem; border: 1px solid #ddd;">C(1,1)</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">Kolda</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">D(1,2)</td></tr>
                        </table>
                        
                        <p><strong>ğŸ¯ Objectif :</strong> K=2 clusters (rÃ©gions dÃ©veloppÃ©es vs en dÃ©veloppement)</p>
                        
                        <p><strong>ğŸš€ Initialisation :</strong> Centres en A(4,3) et C(1,1)</p>
                        
                        <p><strong>ğŸ“Š ItÃ©ration 1 - Assignation :</strong></p>
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>Distances au centre Î¼â‚ = (4,3) :</strong><br>
                            â€¢ d(A, Î¼â‚) = âˆš[(4-4)Â² + (3-3)Â²] = 0<br>
                            â€¢ d(B, Î¼â‚) = âˆš[(3-4)Â² + (2-3)Â²] = âˆš[1+1] = âˆš2 â‰ˆ 1.41<br>
                            â€¢ d(C, Î¼â‚) = âˆš[(1-4)Â² + (1-3)Â²] = âˆš[9+4] = âˆš13 â‰ˆ 3.61<br>
                            â€¢ d(D, Î¼â‚) = âˆš[(1-4)Â² + (2-3)Â²] = âˆš[9+1] = âˆš10 â‰ˆ 3.16<br><br>
                            
                            <strong>Distances au centre Î¼â‚‚ = (1,1) :</strong><br>
                            â€¢ d(A, Î¼â‚‚) = âˆš[(4-1)Â² + (3-1)Â²] = âˆš[9+4] = âˆš13 â‰ˆ 3.61<br>
                            â€¢ d(B, Î¼â‚‚) = âˆš[(3-1)Â² + (2-1)Â²] = âˆš[4+1] = âˆš5 â‰ˆ 2.24<br>
                            â€¢ d(C, Î¼â‚‚) = âˆš[(1-1)Â² + (1-1)Â²] = 0<br>
                            â€¢ d(D, Î¼â‚‚) = âˆš[(1-1)Â² + (2-1)Â²] = 1<br><br>
                            
                            <strong>ğŸ¯ Assignations :</strong> Aâ†’1, Bâ†’1, Câ†’2, Dâ†’2
                        </div>
                        
                        <p><strong>ğŸ“ ItÃ©ration 1 - Recentrage :</strong></p>
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>Nouveau centre Î¼â‚ :</strong><br>
                            Î¼â‚ = moyenne(A, B) = ((4+3)/2, (3+2)/2) = (3.5, 2.5)<br><br>
                            
                            <strong>Nouveau centre Î¼â‚‚ :</strong><br>
                            Î¼â‚‚ = moyenne(C, D) = ((1+1)/2, (1+2)/2) = (1, 1.5)<br><br>
                            
                            <strong>ğŸ“Š Inertie :</strong> J = 0.5 + 0.5 + 0 + 0.25 = 1.25
                        </div>
                        
                        <p><strong>ğŸ”„ ItÃ©ration 2 :</strong> Avec les nouveaux centres, recalculer les distances... Les assignations restent identiques â†’ <strong>Convergence !</strong></p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice pratique : calcul manuel K-means",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Soit 6 points Ã  grouper en 2 clusters :</p>
                        <p><strong>Points :</strong> A(1,1), B(2,1), C(1,2), D(5,5), E(6,5), F(5,6)</p>
                        
                        <p><strong>ğŸ“ Effectuez K-means manuellement :</strong></p>
                        <ol>
                            <li>Initialisez avec Î¼â‚ = A(1,1) et Î¼â‚‚ = D(5,5)</li>
                            <li>Calculez toutes les distances pour l'assignation</li>
                            <li>Assignez chaque point au centre le plus proche</li>
                            <li>Recalculez les centres (moyennes)</li>
                            <li>RÃ©pÃ©tez jusqu'Ã  convergence</li>
                            <li>Calculez l'inertie finale</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('kmeans-manual-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="kmeans-manual-exercise" style="display: none;">
                        <p><strong>ItÃ©ration 1 - Assignation :</strong></p>
                        <ul>
                            <li>A(1,1) : d(A,Î¼â‚)=0, d(A,Î¼â‚‚)=âˆš32â‰ˆ5.66 â†’ Cluster 1</li>
                            <li>B(2,1) : d(B,Î¼â‚)=1, d(B,Î¼â‚‚)=âˆš25=5 â†’ Cluster 1</li>
                            <li>C(1,2) : d(C,Î¼â‚)=1, d(C,Î¼â‚‚)=âˆš25=5 â†’ Cluster 1</li>
                            <li>D(5,5) : d(D,Î¼â‚)=âˆš32â‰ˆ5.66, d(D,Î¼â‚‚)=0 â†’ Cluster 2</li>
                            <li>E(6,5) : d(E,Î¼â‚)=âˆš41â‰ˆ6.4, d(E,Î¼â‚‚)=1 â†’ Cluster 2</li>
                            <li>F(5,6) : d(F,Î¼â‚)=âˆš41â‰ˆ6.4, d(F,Î¼â‚‚)=1 â†’ Cluster 2</li>
                        </ul>
                        
                        <p><strong>Recentrage :</strong></p>
                        <ul>
                            <li>Î¼â‚ = ((1+2+1)/3, (1+1+2)/3) = (4/3, 4/3) â‰ˆ (1.33, 1.33)</li>
                            <li>Î¼â‚‚ = ((5+6+5)/3, (5+5+6)/3) = (16/3, 16/3) â‰ˆ (5.33, 5.33)</li>
                        </ul>
                        
                        <p><strong>ItÃ©ration 2 :</strong> Les assignations restent identiques â†’ Convergence !</p>
                        
                        <p><strong>ğŸ“Š RÃ©sultat final :</strong></p>
                        <ul>
                            <li>Cluster 1 : {A, B, C} centrÃ© en (1.33, 1.33)</li>
                            <li>Cluster 2 : {D, E, F} centrÃ© en (5.33, 5.33)</li>
                            <li>Inertie finale : J â‰ˆ 2.67</li>
                        </ul>
                        </div>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "ThÃ©orÃ¨me de convergence : pourquoi Ã§a marche toujours",
            content: `
                        <p><strong>ğŸ¯ ThÃ©orÃ¨me fondamental :</strong></p>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>K-means converge toujours en un nombre fini d'itÃ©rations.</strong>
                        </div>
                        
                        <p><strong>ğŸ“ Preuve rigoureuse :</strong></p>
                        
                        <p><strong>1ï¸âƒ£ J dÃ©croÃ®t Ã  chaque Ã©tape :</strong></p>
                        <ul>
                            <li><strong>Assignation :</strong> Chaque point choisit le centre minimisant sa distance â†’ J diminue ou reste stable</li>
                            <li><strong>Recentrage :</strong> Chaque centre minimise J pour ses points assignÃ©s â†’ J diminue ou reste stable</li>
                        </ul>
                        
                        <p><strong>2ï¸âƒ£ J est bornÃ© infÃ©rieurement :</strong></p>
                        <p>$$J = \\sum_{k=1}^{K} \\sum_{\\vec{x}_i \\in C_k} ||\\vec{x}_i - \\vec{\\mu}_k||^2 \\geq 0$$</p>
                        <p>Car c'est une somme de carrÃ©s (toujours â‰¥ 0)</p>
                        
                        <p><strong>3ï¸âƒ£ Nombre fini de partitions :</strong></p>
                        <p>Il y a au plus \\(K^n\\) faÃ§ons d'assigner n points Ã  K clusters.</p>
                        
                        <p><strong>4ï¸âƒ£ Conclusion :</strong></p>
                        <ul>
                            <li>J dÃ©croÃ®t et est minorÃ©e â†’ convergence assurÃ©e</li>
                            <li>Nombre fini de partitions â†’ convergence en temps fini</li>
                        </ul>
                        
                        <p><strong>â±ï¸ ComplexitÃ© :</strong> \\(O(nKdI)\\) oÃ¹ :</p>
                        <ul>
                            <li>n = nombre de points</li>
                            <li>K = nombre de clusters</li>
                            <li>d = dimension</li>
                            <li>I = nombre d'itÃ©rations (typiquement < 100)</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Remarquable :</strong> Algorithme efficace pour un problÃ¨me NP-difficile !</p>
                    `,
          },
          {
            type: "code",
            title: "ImplÃ©mentation K-means from scratch",
            description: "ImplÃ©mentons K-means Ã©tape par Ã©tape :",
            code: `import numpy as np
import matplotlib.pyplot as plt

class KMeansFromScratch:
    def __init__(self, k=2, max_iters=100, random_state=42):
        self.k = k
        self.max_iters = max_iters
        self.random_state = random_state
    
    def fit(self, X):
        """EntraÃ®ner K-means sur les donnÃ©es X"""
        np.random.seed(self.random_state)
        n_samples, n_features = X.shape
        
        # Initialisation alÃ©atoire des centres
        self.centroids = X[np.random.choice(n_samples, self.k, replace=False)]
        
        print(f"ğŸ¯ K-means avec K={self.k}")
        print(f"ğŸ“Š DonnÃ©es: {n_samples} points en dimension {n_features}")
        print(f"ğŸš€ Centres initiaux:")
        for i, centroid in enumerate(self.centroids):
            print(f"   Î¼{i+1} = ({centroid[0]:.2f}, {centroid[1]:.2f})")
        
        self.inertias = []
        
        for iteration in range(self.max_iters):
            # Ã‰tape 1: Assignation
            distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
            self.labels = np.argmin(distances, axis=0)
            
            # Ã‰tape 2: Recentrage
            new_centroids = np.array([X[self.labels == k].mean(axis=0) for k in range(self.k)])
            
            # Calcul de l'inertie
            inertia = sum(np.sum((X[self.labels == k] - new_centroids[k])**2) 
                         for k in range(self.k))
            self.inertias.append(inertia)
            
            # VÃ©rification de convergence
            if np.allclose(self.centroids, new_centroids):
                print(f"âœ… Convergence atteinte Ã  l'itÃ©ration {iteration + 1}")
                break
            
            self.centroids = new_centroids
            
            if iteration < 5:  # Afficher les premiÃ¨res itÃ©rations
                print(f"\\nItÃ©ration {iteration + 1}:")
                print(f"   Inertie: {inertia:.2f}")
                for i, centroid in enumerate(self.centroids):
                    print(f"   Î¼{i+1} = ({centroid[0]:.2f}, {centroid[1]:.2f})")
        
        return self
    
    def predict(self, X):
        """PrÃ©dire les clusters pour de nouveaux points"""
        distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))
        return np.argmin(distances, axis=0)

# Test avec donnÃ©es Ã©conomiques sÃ©nÃ©galaises
donnees_villes = np.array([
    [4, 3],  # Dakar
    [3, 2],  # ThiÃ¨s  
    [1, 1],  # Kaolack
    [1, 2],  # Kolda
    [4, 2],  # Saint-Louis
    [2, 1]   # Fatick
])

villes = ['Dakar', 'ThiÃ¨s', 'Kaolack', 'Kolda', 'Saint-Louis', 'Fatick']

# EntraÃ®nement
kmeans = KMeansFromScratch(k=2)
kmeans.fit(donnees_villes)

print(f"\\nğŸ† RÃ‰SULTATS FINAUX:")
for i, ville in enumerate(villes):
    cluster = kmeans.labels[i]
    print(f"{ville}: Cluster {cluster + 1}")

print(f"\\nğŸ“Š Inertie finale: {kmeans.inertias[-1]:.2f}")`,
          },
          {
            type: "code",
            title: "Visualisation du clustering",
            description: "Visualisons le processus K-means :",
            code: `# Visualisation des rÃ©sultats
plt.figure(figsize=(12, 5))

# Graphique 1: DonnÃ©es et clusters finaux
plt.subplot(1, 2, 1)
colors = ['red', 'blue', 'green', 'purple']
for k in range(kmeans.k):
    cluster_points = donnees_villes[kmeans.labels == k]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], 
               c=colors[k], label=f'Cluster {k+1}', s=100, alpha=0.7)

# Centres finaux
plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], 
           c='black', marker='x', s=200, linewidths=3, label='Centres')

# Annotations des villes
for i, ville in enumerate(villes):
    plt.annotate(ville, (donnees_villes[i, 0], donnees_villes[i, 1]),
                xytext=(5, 5), textcoords='offset points', fontsize=9)

plt.xlabel('PIB/habitant (Ã©chelle)')
plt.ylabel('Taux alphabÃ©tisation (Ã©chelle)')
plt.title('Clustering des Villes SÃ©nÃ©galaises')
plt.legend()
plt.grid(True, alpha=0.3)

# Graphique 2: Ã‰volution de l'inertie
plt.subplot(1, 2, 2)
plt.plot(range(1, len(kmeans.inertias) + 1), kmeans.inertias, 'bo-')
plt.xlabel('ItÃ©ration')
plt.ylabel('Inertie')
plt.title('Convergence de K-means')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("ğŸ“ˆ Visualisation terminÃ©e !")`,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : choix du nombre de clusters K",
            content: `
                        <p><strong>ğŸ¤” Comment choisir K ?</strong> Utilisons la <strong>mÃ©thode du coude</strong> :</p>
                        
                        <p><strong>ğŸ¯ Principe :</strong> Tester diffÃ©rentes valeurs de K et choisir celle oÃ¹ l'inertie cesse de diminuer drastiquement.</p>
                        
                        <p><strong>ğŸ“ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Avec les mÃªmes 6 villes, testez K de 1 Ã  5 et trouvez le K optimal.</p>
                        
                        <p><strong>âœ… Solution :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('elbow-method-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="elbow-method-exercise" style="display: none;">
                        <p><strong>Calculs thÃ©oriques :</strong></p>
                        <ul>
                            <li><strong>K=1 :</strong> Tous les points dans un cluster â†’ Inertie Ã©levÃ©e</li>
                            <li><strong>K=2 :</strong> SÃ©paration naturelle â†’ Inertie modÃ©rÃ©e</li>
                            <li><strong>K=3 :</strong> Sur-segmentation â†’ Inertie plus faible</li>
                            <li><strong>K=6 :</strong> Chaque point = 1 cluster â†’ Inertie = 0</li>
                        </ul>
                        <p><strong>ğŸ¯ K optimal :</strong> K=2 car il y a 2 groupes naturels (villes dÃ©veloppÃ©es vs en dÃ©veloppement)</p>
                        </div>
                    `,
          },
          {
            type: "code",
            title: "MÃ©thode du coude en pratique",
            description: "ImplÃ©mentons la mÃ©thode du coude :",
            code: `# MÃ©thode du coude pour choisir K
def methode_coude(X, k_max=6):
    """Teste diffÃ©rentes valeurs de K et calcule l'inertie"""
    inertias = []
    K_values = range(1, k_max + 1)
    
    print("ğŸ“Š MÃ‰THODE DU COUDE")
    print("=" * 30)
    
    for k in K_values:
        if k == 1:
            # K=1: tous les points, centre = moyenne globale
            centroid = X.mean(axis=0)
            inertia = np.sum((X - centroid)**2)
        else:
            kmeans = KMeansFromScratch(k=k, max_iters=50)
            kmeans.fit(X)
            inertia = kmeans.inertias[-1]
        
        inertias.append(inertia)
        print(f"K={k}: Inertie = {inertia:.2f}")
    
    return K_values, inertias

# Test sur nos donnÃ©es
K_values, inertias = methode_coude(donnees_villes)

# Visualisation du coude
plt.figure(figsize=(10, 6))
plt.plot(K_values, inertias, 'bo-', linewidth=2, markersize=8)
plt.xlabel('Nombre de clusters (K)')
plt.ylabel('Inertie')
plt.title('MÃ©thode du Coude - Choix de K')
plt.grid(True, alpha=0.3)

# Marquer le coude optimal (K=2)
plt.annotate('Coude optimal', xy=(2, inertias[1]), xytext=(3, inertias[1] + 2),
            arrowprops=dict(arrowstyle='->', color='red', lw=2),
            fontsize=12, color='red', fontweight='bold')

plt.show()

print(f"\\nğŸ¯ K optimal suggÃ©rÃ©: 2 (coude visible)")`,
          },
          {
            type: "warning",
            icon: "âš ï¸",
            title: "Limites fondamentales de K-means",
            content: `
                        <p><strong>âš ï¸ K-means fait des hypothÃ¨ses fortes qui peuvent Ã©chouer :</strong></p>
                        
                        <p><strong>ğŸ”´ HypothÃ¨se 1 : Clusters sphÃ©riques</strong></p>
                        <ul>
                            <li>âœ… <strong>Fonctionne</strong> : groupes compacts et ronds</li>
                            <li>âŒ <strong>Ã‰choue</strong> : formes allongÃ©es, en croissant, anneaux</li>
                            <li>ğŸ”§ <strong>Cause</strong> : distance euclidienne crÃ©e des "boules"</li>
                        </ul>
                        
                        <p><strong>ğŸ”´ HypothÃ¨se 2 : Clusters de taille similaire</strong></p>
                        <ul>
                            <li>âœ… <strong>Fonctionne</strong> : groupes Ã©quilibrÃ©s</li>
                            <li>âŒ <strong>Ã‰choue</strong> : un gros groupe + plusieurs petits</li>
                            <li>ğŸ”§ <strong>Cause</strong> : minimisation de l'inertie favorise l'Ã©quilibre</li>
                        </ul>
                        
                        <p><strong>ğŸ”´ HypothÃ¨se 3 : Variables de mÃªme Ã©chelle</strong></p>
                        <ul>
                            <li>âœ… <strong>Fonctionne</strong> : Ã¢ge [20-60] et salaire [20k-60k]</li>
                            <li>âŒ <strong>Ã‰choue</strong> : Ã¢ge [20-60] et salaire [20k-2M]</li>
                            <li>ğŸ”§ <strong>Solution</strong> : normalisation obligatoire</li>
                        </ul>
                        
                        <p><strong>ğŸ”´ Autres limitations :</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>K fixÃ© Ã  l'avance</strong> : mÃ©thode du coude pour choisir</li>
                            <li>ğŸ² <strong>Sensible Ã  l'initialisation</strong> : K-means++ amÃ©liore</li>
                            <li>ğŸ“Š <strong>Sensible aux outliers</strong> : la moyenne n'est pas robuste</li>
                            <li>ğŸ“ <strong>FrontiÃ¨res linÃ©aires</strong> : hyperplans de sÃ©paration</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : impact de l'initialisation",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Testez l'impact de diffÃ©rentes initialisations sur le mÃªme dataset.</p>
                        
                        <p><strong>ğŸ“ ScÃ©narios Ã  tester :</strong></p>
                        <ol>
                            <li>Initialisation alÃ©atoire normale</li>
                            <li>Tous les centres au mÃªme point (0,0)</li>
                            <li>Centres trÃ¨s Ã©loignÃ©s des donnÃ©es</li>
                            <li>Un centre au milieu des donnÃ©es</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('initialization-impact-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="initialization-impact-exercise" style="display: none;">
                        <p><strong>RÃ©sultats attendus :</strong></p>
                        <ol>
                            <li><strong>AlÃ©atoire normale :</strong> Convergence vers solution raisonnable</li>
                            <li><strong>MÃªme point :</strong> Tous les points assignÃ©s au mÃªme cluster â†’ Ã©chec total</li>
                            <li><strong>TrÃ¨s Ã©loignÃ©s :</strong> Convergence lente, solution sous-optimale possible</li>
                            <li><strong>Centre au milieu :</strong> Risque de solution dÃ©sÃ©quilibrÃ©e</li>
                        </ol>
                        <p><strong>ğŸ’¡ Conclusion :</strong> L'initialisation est cruciale ! C'est pourquoi K-means++ a Ã©tÃ© dÃ©veloppÃ© pour une initialisation intelligente.</p>
                        </div>
                    `,
          },
          {
            type: "code",
            title: "Comparaison avec d'autres algorithmes",
            description: "Comparons K-means avec d'autres approches :",
            code: `# Simulation de donnÃ©es avec formes diffÃ©rentes
np.random.seed(42)

# Dataset 1: Clusters sphÃ©riques (K-means excellent)
cluster1 = np.random.normal([2, 2], 0.5, (30, 2))
cluster2 = np.random.normal([6, 6], 0.5, (30, 2))
data_spherique = np.vstack([cluster1, cluster2])

# Dataset 2: Clusters allongÃ©s (K-means difficile)
cluster1_long = np.random.normal([2, 2], [0.3, 1.5], (30, 2))
cluster2_long = np.random.normal([6, 6], [1.5, 0.3], (30, 2))
data_allonge = np.vstack([cluster1_long, cluster2_long])

print("ğŸ” COMPARAISON TYPES DE DONNÃ‰ES")
print("=" * 40)

# Test K-means sur donnÃ©es sphÃ©riques
kmeans_sphere = KMeansFromScratch(k=2)
kmeans_sphere.fit(data_spherique)
print(f"\\nâœ… DonnÃ©es sphÃ©riques - Inertie finale: {kmeans_sphere.inertias[-1]:.2f}")

# Test K-means sur donnÃ©es allongÃ©es
kmeans_allonge = KMeansFromScratch(k=2)
kmeans_allonge.fit(data_allonge)
print(f"âš ï¸ DonnÃ©es allongÃ©es - Inertie finale: {kmeans_allonge.inertias[-1]:.2f}")

# Visualisation comparative
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# DonnÃ©es sphÃ©riques
colors = ['red', 'blue']
for k in range(2):
    mask = kmeans_sphere.labels == k
    ax1.scatter(data_spherique[mask, 0], data_spherique[mask, 1], 
               c=colors[k], alpha=0.7, s=50)
ax1.scatter(kmeans_sphere.centroids[:, 0], kmeans_sphere.centroids[:, 1], 
           c='black', marker='x', s=200, linewidths=3)
ax1.set_title('K-means sur Clusters SphÃ©riques\\n(Excellent rÃ©sultat)')
ax1.grid(True, alpha=0.3)

# DonnÃ©es allongÃ©es
for k in range(2):
    mask = kmeans_allonge.labels == k
    ax2.scatter(data_allonge[mask, 0], data_allonge[mask, 1], 
               c=colors[k], alpha=0.7, s=50)
ax2.scatter(kmeans_allonge.centroids[:, 0], kmeans_allonge.centroids[:, 1], 
           c='black', marker='x', s=200, linewidths=3)
ax2.set_title('K-means sur Clusters AllongÃ©s\\n(RÃ©sultat sous-optimal)')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\\nğŸ’¡ Observation: K-means divise artificiellement les clusters allongÃ©s !")`,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Alternatives Ã  K-means : quand l'utiliser ?",
            content: `
                        <p><strong>ğŸ”§ Autres algorithmes de clustering :</strong></p>
                        
                        <p><strong>ğŸ¯ DBSCAN (Density-Based) :</strong></p>
                        <ul>
                            <li>âœ… <strong>Avantages</strong> : formes arbitraires, dÃ©tecte les outliers, K automatique</li>
                            <li>âŒ <strong>InconvÃ©nients</strong> : sensible aux paramÃ¨tres, densitÃ©s variables</li>
                            <li>ğŸ¯ <strong>Usage</strong> : donnÃ©es avec bruit, formes complexes</li>
                        </ul>
                        
                        <p><strong>ğŸŒ³ Clustering HiÃ©rarchique :</strong></p>
                        <ul>
                            <li>âœ… <strong>Avantages</strong> : dendrogramme, pas besoin de fixer K</li>
                            <li>âŒ <strong>InconvÃ©nients</strong> : O(nÂ³), difficile sur gros datasets</li>
                            <li>ğŸ¯ <strong>Usage</strong> : exploration, petits datasets</li>
                        </ul>
                        
                        <p><strong>ğŸ“Š Gaussian Mixture Models (GMM) :</strong></p>
                        <ul>
                            <li>âœ… <strong>Avantages</strong> : clusters elliptiques, assignation probabiliste</li>
                            <li>âŒ <strong>InconvÃ©nients</strong> : plus complexe, suppose des gaussiennes</li>
                            <li>ğŸ¯ <strong>Usage</strong> : donnÃ©es gaussiennes, incertitude d'assignation</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Guide de choix :</strong></p>
                        <ul>
                            <li>ğŸ”µ <strong>Clusters sphÃ©riques, K connu</strong> â†’ K-means</li>
                            <li>ğŸŸ¡ <strong>Formes complexes, outliers</strong> â†’ DBSCAN</li>
                            <li>ğŸŸ¢ <strong>Exploration, hiÃ©rarchie</strong> â†’ Clustering hiÃ©rarchique</li>
                            <li>ğŸŸ£ <strong>Incertitude, ellipses</strong> â†’ GMM</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : clustering de clients e-commerce",
            content: `
                        <p><strong>ğŸ¯ Exercice pratique complet :</strong></p>
                        <p>Segmentez des clients d'un site e-commerce sÃ©nÃ©galais selon leurs habitudes d'achat.</p>
                        
                        <p><strong>ğŸ“Š Variables :</strong></p>
                        <ul>
                            <li>ğŸ’° <strong>Montant moyen par commande</strong> (milliers FCFA)</li>
                            <li>ğŸ“… <strong>FrÃ©quence d'achat</strong> (commandes/mois)</li>
                            <li>â­ <strong>Note de satisfaction</strong> (1-5)</li>
                        </ul>
                        
                        <p><strong>ğŸ“ Ã‰tapes Ã  suivre :</strong></p>
                        <ol>
                            <li>Normaliser les donnÃ©es (Ã©chelles diffÃ©rentes)</li>
                            <li>Appliquer la mÃ©thode du coude</li>
                            <li>Effectuer K-means avec K optimal</li>
                            <li>InterprÃ©ter les segments dÃ©couverts</li>
                            <li>Proposer des stratÃ©gies marketing par segment</li>
                        </ol>
                        
                        <p><strong>âœ… Solution :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('ecommerce-clustering-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="ecommerce-clustering-exercise" style="display: none;">
                        <p><strong>Segments typiques attendus :</strong></p>
                        <ul>
                            <li>ğŸ† <strong>VIP</strong> : montant Ã©levÃ©, frÃ©quence Ã©levÃ©e, satisfaction Ã©levÃ©e</li>
                            <li>ğŸ’° <strong>Gros acheteurs occasionnels</strong> : montant Ã©levÃ©, frÃ©quence faible</li>
                            <li>ğŸ›’ <strong>Acheteurs rÃ©guliers</strong> : montant moyen, frÃ©quence Ã©levÃ©e</li>
                            <li>ğŸ˜ <strong>Clients Ã  risque</strong> : montant faible, satisfaction faible</li>
                        </ul>
                        <p><strong>ğŸ¯ StratÃ©gies :</strong></p>
                        <ul>
                            <li><strong>VIP</strong> â†’ Programme de fidÃ©litÃ© premium</li>
                            <li><strong>Gros acheteurs</strong> â†’ Notifications personnalisÃ©es</li>
                            <li><strong>RÃ©guliers</strong> â†’ Offres de volume</li>
                            <li><strong>Ã€ risque</strong> â†’ AmÃ©lioration service client</li>
                        </ul>
                        </div>
                    `,
          },
          {
            type: "warning",
            icon: "âš ï¸",
            title: "K-means en production : bonnes pratiques",
            content: `
                        <p><strong>ğŸ­ Pour utiliser K-means en production :</strong></p>
                        
                        <p><strong>ğŸ“‹ PrÃ©paration des donnÃ©es :</strong></p>
                        <ul>
                            <li>ğŸ§¹ <strong>Nettoyage</strong> : supprimer les outliers extrÃªmes</li>
                            <li>ğŸ“Š <strong>Normalisation</strong> : StandardScaler ou MinMaxScaler</li>
                            <li>ğŸ” <strong>SÃ©lection de features</strong> : garder les variables pertinentes</li>
                            <li>ğŸ“ˆ <strong>Analyse exploratoire</strong> : comprendre les distributions</li>
                        </ul>
                        
                        <p><strong>âš™ï¸ Optimisation de l'algorithme :</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>K-means++</strong> : initialisation intelligente</li>
                            <li>ğŸ”„ <strong>Plusieurs runs</strong> : prendre le meilleur rÃ©sultat</li>
                            <li>â±ï¸ <strong>CritÃ¨re d'arrÃªt</strong> : tolÃ©rance sur le dÃ©placement des centres</li>
                            <li>ğŸ’¾ <strong>Mini-batch K-means</strong> : pour trÃ¨s gros datasets</li>
                        </ul>
                        
                        <p><strong>ğŸ“Š Validation des rÃ©sultats :</strong></p>
                        <ul>
                            <li>ğŸ“ˆ <strong>Silhouette score</strong> : qualitÃ© des clusters</li>
                            <li>ğŸ¯ <strong>Inertie intra vs inter</strong> : sÃ©paration des groupes</li>
                            <li>ğŸ‘ï¸ <strong>Visualisation</strong> : PCA pour projeter en 2D</li>
                            <li>ğŸ§  <strong>InterprÃ©tation mÃ©tier</strong> : les clusters ont-ils du sens ?</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Point clÃ© :</strong> K-means est simple et efficace, mais il faut comprendre ses limites pour l'utiliser Ã  bon escient. Quand il fonctionne, c'est magique. Quand il Ã©choue, il faut savoir pourquoi et choisir une alternative !</p>
                        
                        <p><strong>ğŸ”® Prochaine Ã©tape :</strong> Validation - comment s'assurer que nos clusters sont vraiment significatifs !</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ğŸ¤” Que se passe-t-il si on initialise tous les centres K-means au mÃªme point (0,0) ?",
          options: [
            "A) L'algorithme converge normalement vers la solution optimale",
            "B) L'algorithme ne peut pas dÃ©marrer car tous les points sont assignÃ©s au mÃªme cluster",
            "C) L'algorithme diverge et ne converge jamais",
            "D) On obtient automatiquement K clusters Ã©quilibrÃ©s",
          ],
          correct: 1,
          explanation:
            "Si tous les centres sont identiques, tous les points sont assignÃ©s au mÃªme cluster initial. Lors du recentrage, tous les centres se dÃ©placent vers la moyenne globale (mÃªme point). L'algorithme reste bloquÃ© avec un seul cluster actif. C'est pourquoi une initialisation intelligente (K-means++) est cruciale.",
        },
        prevModule: "classification.html",
        nextModule: "validation.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
