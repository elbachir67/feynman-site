<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Clustering | IA4Ndada</title>

    <!-- MathJax pour les formules math√©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">üè† Accueil</a>
          <span>‚Ä∫</span>
          <span>ü§ñ Machine Learning</span>
          <span>‚Ä∫</span>
          <span>Clustering</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>üîç Clustering</h1>
      <p class="subtitle">Module 3.4 - Machine Learning</p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>üéØ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajout√©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajout√© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>üéâ Checkpoint - Clustering</h3>
        <p>
          Vous comprenez maintenant comment regrouper automatiquement des
          donn√©es similaires.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme compl√©t√©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="classification.html" class="nav-link" id="prev-link"
          >‚Üê Module pr√©c√©dent : Classification</a
        >
        <a href="validation.html" class="nav-link" id="next-link"
          >Module suivant : Validation ‚Üí</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module Clustering
      const moduleConfig = {
        id: "ml-clustering",
        title: "Clustering",
        category: "Machine Learning",
        objectives: [
          "Comprendre ce qu'est l'apprentissage non-supervis√©",
          "Ma√Ætriser compl√®tement l'algorithme K-means",
          "Comprendre pourquoi il converge math√©matiquement",
          "Savoir identifier ses limites fondamentales",
        ],
        content: [
          {
            type: "concept",
            icon: "üí°",
            title: "Le changement de paradigme : pas d'√©tiquettes",
            content: `
                        <p><strong>Jusqu'ici, nous avions toujours un "professeur" : les √©tiquettes y.</strong></p>
                        
                        <p>En clustering, on n'a que les donn√©es X. La machine doit d√©couvrir seule s'il existe une structure.</p>
                        
                        <p>C'est comme trier des billes sans savoir qu'elles ont des couleurs diff√©rentes. On remarque qu'elles forment naturellement des groupes, et on les s√©pare.</p>
                        
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>La diff√©rence fondamentale :</strong><br><br>
                            ‚Ä¢ <strong>Supervis√© :</strong> "Voici des chats et des chiens √©tiquet√©s. Apprends √† les distinguer."<br>
                            ‚Ä¢ <strong>Non-supervis√© :</strong> "Voici des animaux. Y a-t-il des groupes naturels ?"<br><br>
                            
                            Le non-supervis√© r√©v√®le la structure cach√©e des donn√©es.
                        </div>
                        
                        <p><strong>Applications concr√®tes :</strong> Segmentation clients (sans conna√Ætre les segments √† l'avance), compression d'images (regrouper les couleurs similaires), organisation de documents (th√®mes √©mergents).</p>
                    `,
          },
          {
            type: "intuition",
            icon: "üß†",
            title: "K-means : l'algorithme fondamental",
            content: `
                        <p><strong>L'id√©e est simple et √©l√©gante :</strong></p>
                        
                        <p>Si les donn√©es forment K groupes naturels, chaque groupe doit avoir un "centre de gravit√©". Si on conna√Æt les centres, on peut assigner chaque point au centre le plus proche. Si on conna√Æt les assignations, on peut calculer les centres.</p>
                        
                        <p>C'est un probl√®me de poule et ≈ìuf ! K-means le r√©sout en alternant :</p>
                        
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>Les deux √©tapes qui se renforcent mutuellement :</strong><br><br>
                            1. <strong>Assignation :</strong> chaque point rejoint le centre le plus proche<br>
                            2. <strong>Recentrage :</strong> chaque centre se d√©place au milieu de ses points<br><br>
                            
                            R√©p√©ter jusqu'√† stabilit√©. C'est tout !
                        </div>
                        
                        <p><strong>Pourquoi "K-means" ?</strong> K = nombre de clusters (fix√©), means = moyennes (les centres).</p>
                        
                        <p>L'algorithme trouve une partition qui minimise la dispersion interne des groupes. C'est optimal localement.</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "Formalisation math√©matique rigoureuse",
            content: `
                        <p><strong>Posons le probl√®me proprement :</strong></p>
                        
                        <p>Donn√©es : \\(\\mathcal{X} = \\{\\vec{x}_1, ..., \\vec{x}_n\\}\\) avec \\(\\vec{x}_i \\in \\mathbb{R}^d\\)</p>
                        
                        <p>On cherche K centres \\(\\mathcal{M} = \\{\\vec{\\mu}_1, ..., \\vec{\\mu}_K\\}\\) et une fonction d'assignation \\(c: \\mathcal{X} \\to \\{1, ..., K\\}\\).</p>
                        
                        <p><strong>La fonction objectif (inertie) :</strong></p>
                        
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            $$J(\\mathcal{M}, c) = \\sum_{i=1}^{n} ||\\vec{x}_i - \\vec{\\mu}_{c(i)}||^2$$
                            
                            o√π \\(||\\cdot||\\) est la norme euclidienne (<a href="../math/vectors.html">Module 1.1</a>).<br><br>
                            
                            <strong>Rappel :</strong> \\(||\\vec{u}||^2 = \\vec{u}^T\\vec{u} = \\sum_{j=1}^{d} u_j^2\\)<br><br>
                            
                            J mesure la somme totale des distances¬≤ de chaque point √† son centre assign√©.
                        </div>
                        
                        <p><strong>Le probl√®me d'optimisation :</strong></p>
                        <p>$$\\min_{\\mathcal{M}, c} J(\\mathcal{M}, c)$$</p>
                        
                        <p>C'est NP-difficile ! Mais K-means trouve efficacement un minimum local.</p>
                        
                        <p><strong>Les deux sous-probl√®mes (plus simples) :</strong></p>
                        
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>1. Assignation optimale (centres fixes) :</strong><br>
                            $$c^*(\\vec{x}_i) = \\arg\\min_{k \\in \\{1,...,K\\}} ||\\vec{x}_i - \\vec{\\mu}_k||^2$$
                            
                            Trivial : choisir le centre le plus proche !<br><br>
                            
                            <strong>2. Centres optimaux (assignations fixes) :</strong><br>
                            Pour le cluster k, minimiser : \\(\\sum_{i: c(i)=k} ||\\vec{x}_i - \\vec{\\mu}_k||^2\\)<br><br>
                            
                            D√©rivons par rapport √† \\(\\vec{\\mu}_k\\) (<a href="../math/derivatives.html">Module 1.4</a>) :<br>
                            $$\\frac{\\partial}{\\partial \\vec{\\mu}_k} \\sum_{i \\in C_k} ||\\vec{x}_i - \\vec{\\mu}_k||^2 = -2\\sum_{i \\in C_k} (\\vec{x}_i - \\vec{\\mu}_k) = 0$$
                            
                            D'o√π : $$\\vec{\\mu}_k^* = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\vec{x}_i$$
                            
                            Le centre optimal est la moyenne arithm√©tique !
                        </div>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "Pourquoi K-means converge toujours",
            content: `
                        <p><strong>Th√©or√®me de convergence :</strong></p>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>K-means converge en un nombre fini d'it√©rations.</strong><br><br>
                            
                            <strong>Preuve :</strong><br><br>
                            
                            1. <strong>J diminue √† chaque √©tape :</strong><br>
                            ‚Ä¢ L'assignation choisit le centre minimisant la distance ‚Üí J diminue ou reste stable<br>
                            ‚Ä¢ Le recentrage minimise J pour les assignations fixes ‚Üí J diminue ou reste stable<br><br>
                            
                            2. <strong>J est born√© inf√©rieurement :</strong><br>
                            $$J \\geq 0$$ (somme de carr√©s)<br><br>
                            
                            3. <strong>Nombre fini de partitions :</strong><br>
                            Il y a au plus \\(K^n\\) fa√ßons d'assigner n points √† K clusters.<br><br>
                            
                            4. <strong>Conclusion :</strong><br>
                            J d√©cro√Æt et est minor√©e ‚Üí convergence<br>
                            Nombre fini de partitions ‚Üí convergence en temps fini ‚úì
                        </div>
                        
                        <p><strong>Complexit√© :</strong> \\(O(nKdI)\\) o√π I est le nombre d'it√©rations (typiquement < 100).</p>
                        
                        <p>C'est remarquablement efficace pour un probl√®me NP-difficile !</p>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Calcul manuel sur un exemple simple",
            content: `
                        <p><strong>Groupons 4 points en 2 clusters :</strong></p>
                        
                        <table style="margin: 1rem auto; text-align: center;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.5rem;">Point</th>
                                <th style="padding: 0.5rem;">x‚ÇÅ</th>
                                <th style="padding: 0.5rem;">x‚ÇÇ</th>
                            </tr>
                            <tr><td>A</td><td>1</td><td>1</td></tr>
                            <tr><td>B</td><td>2</td><td>1</td></tr>
                            <tr><td>C</td><td>4</td><td>3</td></tr>
                            <tr><td>D</td><td>5</td><td>4</td></tr>
                        </table>
                        
                        <p><strong>Initialisation :</strong> Centres en A(1,1) et C(4,3)</p>
                        
                        <p><strong>It√©ration 1 - Assignation :</strong></p>
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            Distances au centre Œº‚ÇÅ = (1,1) :<br>
                            ‚Ä¢ d(A, Œº‚ÇÅ) = 0<br>
                            ‚Ä¢ d(B, Œº‚ÇÅ) = \\(\\sqrt{(2-1)^2 + (1-1)^2} = 1\\)<br>
                            ‚Ä¢ d(C, Œº‚ÇÅ) = \\(\\sqrt{(4-1)^2 + (3-1)^2} = \\sqrt{9+4} = \\sqrt{13} \\approx 3.6\\)<br>
                            ‚Ä¢ d(D, Œº‚ÇÅ) = \\(\\sqrt{(5-1)^2 + (4-1)^2} = \\sqrt{16+9} = 5\\)<br><br>
                            
                            Distances au centre Œº‚ÇÇ = (4,3) :<br>
                            ‚Ä¢ d(A, Œº‚ÇÇ) = \\(\\sqrt{13} \\approx 3.6\\)<br>
                            ‚Ä¢ d(B, Œº‚ÇÇ) = \\(\\sqrt{8} \\approx 2.8\\)<br>
                            ‚Ä¢ d(C, Œº‚ÇÇ) = 0<br>
                            ‚Ä¢ d(D, Œº‚ÇÇ) = \\(\\sqrt{2} \\approx 1.4\\)<br><br>
                            
                            <strong>Assignations :</strong> A‚Üí1, B‚Üí1, C‚Üí2, D‚Üí2
                        </div>
                        
                        <p><strong>It√©ration 1 - Recentrage :</strong></p>
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            Œº‚ÇÅ = moyenne(A, B) = ((1+2)/2, (1+1)/2) = (1.5, 1)<br>
                            Œº‚ÇÇ = moyenne(C, D) = ((4+5)/2, (3+4)/2) = (4.5, 3.5)<br><br>
                            
                            <strong>Inertie :</strong> J = 0.25 + 0.25 + 0.5 + 0.5 = 1.5
                        </div>
                        
                        <p><strong>It√©ration 2 :</strong></p>
                        <p>Avec les nouveaux centres, les assignations restent identiques.<br>
                        Les centres ne bougent plus ‚Üí <strong>Convergence !</strong></p>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>R√©sultat final :</strong><br>
                            ‚Ä¢ Cluster 1 : {A, B} centr√© en (1.5, 1)<br>
                            ‚Ä¢ Cluster 2 : {C, D} centr√© en (4.5, 3.5)<br>
                            ‚Ä¢ Inertie finale : J = 1.5
                        </div>
                    `,
          },
          {
            type: "warning",
            icon: "‚ö†Ô∏è",
            title: "La limite fondamentale de K-means",
            content: `
                        <p><strong>K-means fait une hypoth√®se implicite forte :</strong></p>
                        
                        <div style="background: #ffebee; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>Les clusters sont sph√©riques et de taille comparable.</strong><br><br>
                            
                            Pourquoi ? La distance euclidienne cr√©e des "boules" autour des centres.<br>
                            K-means partitionne l'espace par des hyperplans (fronti√®res lin√©aires).<br><br>
                            
                            <strong>Cons√©quences :</strong><br>
                            ‚Ä¢ √âchoue sur des formes en croissant, anneaux, spirales<br>
                            ‚Ä¢ Divise artificiellement les clusters allong√©s<br>
                            ‚Ä¢ Sensible aux √©chelles diff√©rentes (‚Üí normalisation cruciale)
                        </div>
                        
                        <p><strong>Autres limitations :</strong></p>
                        <ul>
                            <li><strong>K fix√© √† l'avance :</strong> M√©thode du coude pour choisir</li>
                            <li><strong>Sensible √† l'initialisation :</strong> K-means++ am√©liore</li>
                            <li><strong>Sensible aux outliers :</strong> La moyenne n'est pas robuste</li>
                        </ul>
                        
                        <p><strong>Pour aller plus loin :</strong> DBSCAN (densit√©), clustering hi√©rarchique (arbre), GMM (probabiliste).</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ü§î Que se passe-t-il si on initialise tous les centres au m√™me point ?",
          options: [
            "A) L'algorithme converge normalement",
            "B) L'algorithme ne peut pas d√©marrer car tous les points sont assign√©s au m√™me cluster",
            "C) L'algorithme diverge",
            "D) On obtient K clusters √©quilibr√©s",
          ],
          correct: 1,
          explanation:
            "Si tous les centres sont identiques, tous les points sont assign√©s au m√™me cluster initial. Lors du recentrage, tous les centres se d√©placent au m√™me point (la moyenne globale). L'algorithme est bloqu√© avec un seul cluster actif. C'est pourquoi l'initialisation est cruciale : il faut des centres distincts pour que K-means puisse s√©parer les donn√©es.",
        },
        prevModule: "classification.html",
        nextModule: "validation.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
