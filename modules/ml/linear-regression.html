<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>R√©gression Lin√©aire | IA4Ndada</title>

    <!-- MathJax pour les formules math√©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">üè† Accueil</a>
          <span>‚Ä∫</span>
          <span>ü§ñ Machine Learning</span>
          <span>‚Ä∫</span>
          <span>R√©gression Lin√©aire</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>üìà R√©gression Lin√©aire</h1>
      <p class="subtitle">
        Module 3.2 - Le premier algorithme d'apprentissage automatique
      </p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>üéØ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajout√©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajout√© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>üéâ Checkpoint - R√©gression Lin√©aire</h3>
        <p>
          F√©licitations ! Vous ma√Ætrisez maintenant le premier et plus important
          algorithme d'apprentissage automatique.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme compl√©t√©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="introduction.html" class="nav-link" id="prev-link"
          >‚Üê Module pr√©c√©dent : Introduction ML</a
        >
        <a href="classification.html" class="nav-link" id="next-link"
          >Module suivant : Classification ‚Üí</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module R√©gression Lin√©aire
      const moduleConfig = {
        id: "ml-linear-regression",
        title: "R√©gression Lin√©aire",
        category: "Machine Learning",
        objectives: [
          "Comprendre le probl√®me que r√©sout la r√©gression",
          "Construire une fonction de co√ªt depuis z√©ro",
          "D√©river math√©matiquement la solution optimale",
          "Comprendre pourquoi cette solution est unique",
          "Calculer manuellement sur un exemple concret",
          "Impl√©menter l'algorithme en Python",
        ],
        content: [
          {
            type: "concept",
            icon: "üí°",
            title: "Le probl√®me fondamental : pr√©dire des valeurs continues",
            content: `
                        <p>La <strong>r√©gression lin√©aire</strong> est le premier algorithme d'apprentissage automatique √† ma√Ætriser. Elle r√©sout un probl√®me simple mais fondamental : <strong>pr√©dire une valeur num√©rique</strong> √† partir d'autres variables.</p>
                        
                        <p><strong>üîë Exemples concrets :</strong></p>
                        <ul>
                            <li>üè† <strong>Immobilier</strong> : pr√©dire le prix d'une maison selon sa surface</li>
                            <li>üìà <strong>Finance</strong> : pr√©dire le cours d'une action selon les indicateurs</li>
                            <li>üåæ <strong>Agriculture</strong> : pr√©dire le rendement selon la pluviom√©trie</li>
                            <li>üéì <strong>√âducation</strong> : pr√©dire la note selon les heures d'√©tude</li>
                        </ul>
                        
                        <p><strong>üéØ L'objectif :</strong></p>
                        <p>Trouver une <strong>relation math√©matique</strong> entre les variables d'entr√©e (features) et la variable de sortie (target).</p>
                        
                        <p><strong>üìê L'hypoth√®se de base :</strong></p>
                        <p>Cette relation est <strong>lin√©aire</strong> : $$y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$</p>
                        
                        <p><strong>ü§ñ Pourquoi commencer par la r√©gression lin√©aire ?</strong></p>
                        <ul>
                            <li>‚úÖ <strong>Simple √† comprendre</strong> : g√©om√©trie intuitive</li>
                            <li>‚úÖ <strong>Solution analytique</strong> : formule exacte</li>
                            <li>‚úÖ <strong>Rapide √† calculer</strong> : pas d'it√©rations</li>
                            <li>‚úÖ <strong>Baseline parfaite</strong> : point de comparaison</li>
                            <li>‚úÖ <strong>Fondation</strong> : base de tous les autres algorithmes</li>
                        </ul>
                    `,
          },
          {
            type: "intuition",
            icon: "üß†",
            title: "L'analogie du tailleur exp√©riment√©",
            content: `
                        <p>Imaginez un <strong>tailleur exp√©riment√©</strong> qui doit estimer le prix d'un boubou :</p>
                        
                        <p><strong>üßµ Ses observations :</strong></p>
                        <ul>
                            <li>Plus le tissu est cher ‚Üí prix plus √©lev√©</li>
                            <li>Plus le travail est complexe ‚Üí prix plus √©lev√©</li>
                            <li>Plus l'exp√©rience du tailleur ‚Üí prix plus √©lev√©</li>
                        </ul>
                        
                        <p><strong>üß† Son raisonnement intuitif :</strong></p>
                        <p><em>"Le prix final, c'est un prix de base, plus un montant qui d√©pend de chaque facteur"</em></p>
                        
                        <p><strong>üìê En formule :</strong></p>
                        <p>$$\\text{Prix} = \\text{Base} + \\text{Coeff}_1 \\times \\text{Qualit√© tissu} + \\text{Coeff}_2 \\times \\text{Complexit√©} + ...$$</p>
                        
                        <p><strong>üí° C'est exactement la r√©gression lin√©aire !</strong></p>
                        <ul>
                            <li>üéØ <strong>Base</strong> = biais (b)</li>
                            <li>üî¢ <strong>Coefficients</strong> = poids (w‚ÇÅ, w‚ÇÇ, ...)</li>
                            <li>üìä <strong>Facteurs</strong> = features (x‚ÇÅ, x‚ÇÇ, ...)</li>
                        </ul>
                        
                        <p><strong>ü§ñ L'IA fait pareil :</strong></p>
                        <p>Elle observe des milliers d'exemples et trouve automatiquement les meilleurs coefficients !</p>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Probl√®me concret : prix des terrains √† Dakar",
            content: `
                        <p><strong>üè† Donn√©es observ√©es :</strong></p>
                        
                        <table style="margin: 1rem auto; text-align: center; border-collapse: collapse;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Surface (m¬≤)</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Distance centre (km)</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Prix (millions FCFA)</th>
                            </tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">200</td><td style="padding: 0.5rem; border: 1px solid #ddd;">5</td><td style="padding: 0.5rem; border: 1px solid #ddd;">15</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">300</td><td style="padding: 0.5rem; border: 1px solid #ddd;">3</td><td style="padding: 0.5rem; border: 1px solid #ddd;">25</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">150</td><td style="padding: 0.5rem; border: 1px solid #ddd;">8</td><td style="padding: 0.5rem; border: 1px solid #ddd;">8</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">400</td><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">35</td></tr>
                        </table>
                        
                        <p><strong>üéØ Question :</strong> Quel prix pour un terrain de 250m¬≤ √† 4km du centre ?</p>
                        
                        <p><strong>üìê Mod√®le lin√©aire :</strong></p>
                        <p>$$\\text{Prix} = w_1 \\times \\text{Surface} + w_2 \\times \\text{Distance} + b$$</p>
                        
                        <p><strong>ü§î Comment trouver w‚ÇÅ, w‚ÇÇ et b ?</strong></p>
                        <p>C'est exactement ce que va r√©soudre la r√©gression lin√©aire automatiquement !</p>
                        
                        <p><strong>üí° Intuition :</strong></p>
                        <ul>
                            <li>w‚ÇÅ > 0 : plus de surface ‚Üí prix plus √©lev√©</li>
                            <li>w‚ÇÇ < 0 : plus loin du centre ‚Üí prix plus bas</li>
                            <li>b : prix de base (terrain th√©orique de 0m¬≤ au centre)</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "Construction de la fonction de co√ªt",
            content: `
                        <p><strong>üéØ Qu'est-ce qu'une "bonne" pr√©diction ?</strong></p>
                        <p>Une pr√©diction est bonne si elle est <strong>proche de la r√©alit√©</strong>. Mais "proche", √ßa veut dire quoi math√©matiquement ?</p>
                        
                        <p><strong>üìä Pour chaque exemple i :</strong></p>
                        <ul>
                            <li><strong>Valeur r√©elle :</strong> \\(y_i\\)</li>
                            <li><strong>Valeur pr√©dite :</strong> \\(\\hat{y}_i = \\vec{w}^T\\vec{x}_i + b\\)</li>
                            <li><strong>Erreur :</strong> \\(e_i = y_i - \\hat{y}_i\\)</li>
                        </ul>
                        
                        <p><strong>ü§î Comment mesurer l'erreur totale ?</strong></p>
                        
                        <div style="background: #ffebee; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                            <strong>‚ùå Option 1 : Somme des erreurs</strong><br>
                            $$\\sum_{i=1}^{n} e_i = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$$
                            
                            <strong>Probl√®me fatal :</strong> Les erreurs + et - s'annulent !<br>
                            Une droite tr√®s mauvaise pourrait avoir une erreur totale de 0.
                        </div>
                        
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                            <strong>üü° Option 2 : Somme des valeurs absolues</strong><br>
                            $$\\sum_{i=1}^{n} |e_i| = \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$$
                            
                            ‚úì Pas d'annulation<br>
                            ‚ùå <strong>Probl√®me :</strong> Pas d√©rivable en 0 (voir <a href="../math/derivatives.html">Module 1.4</a>)
                        </div>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                            <strong>‚úÖ Option 3 : Somme des carr√©s (MSE)</strong><br>
                            $$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$
                            
                            ‚úì Pas d'annulation (carr√© toujours ‚â• 0)<br>
                            ‚úì D√©rivable partout<br>
                            ‚úì P√©nalise plus les grandes erreurs<br>
                            ‚úì Solution math√©matique unique !
                        </div>
                        
                        <p><strong>üîë Fonction de co√ªt finale :</strong></p>
                        <p>$$J(\\vec{w}, b) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - (\\vec{w}^T\\vec{x}_i + b))^2$$</p>
                        <p><em>Le facteur 1/2 simplifie les calculs de d√©riv√©e</em></p>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Exercice : construction manuelle de la fonction de co√ªt",
            content: `
                        <p><strong>üéØ Exercice √† r√©soudre :</strong></p>
                        <p>Avec nos donn√©es de terrains :</p>
                        
                        <table style="margin: 1rem auto; text-align: center;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.5rem;">Surface (x‚ÇÅ)</th>
                                <th style="padding: 0.5rem;">Distance (x‚ÇÇ)</th>
                                <th style="padding: 0.5rem;">Prix r√©el (y)</th>
                            </tr>
                            <tr><td>200</td><td>5</td><td>15</td></tr>
                            <tr><td>300</td><td>3</td><td>25</td></tr>
                        </table>
                        
                        <p><strong>üìù Supposons w‚ÇÅ = 0.08, w‚ÇÇ = -2, b = 5. Calculez :</strong></p>
                        <ol>
                            <li>Les pr√©dictions ≈∑‚ÇÅ et ≈∑‚ÇÇ</li>
                            <li>Les erreurs e‚ÇÅ et e‚ÇÇ</li>
                            <li>La fonction de co√ªt J</li>
                            <li>Ces param√®tres sont-ils bons ?</li>
                        </ol>
                        
                        <p><strong>‚úÖ Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('cost-function-exercise')" style="margin-bottom: 1rem;">
                            üëÅÔ∏è Voir la solution
                        </button>
                        <div id="cost-function-exercise" style="display: none;">
                        <ol>
                            <li><strong>Pr√©dictions :</strong><br>
                                ≈∑‚ÇÅ = 0.08√ó200 + (-2)√ó5 + 5 = 16 - 10 + 5 = 11<br>
                                ≈∑‚ÇÇ = 0.08√ó300 + (-2)√ó3 + 5 = 24 - 6 + 5 = 23</li>
                            <li><strong>Erreurs :</strong><br>
                                e‚ÇÅ = 15 - 11 = 4<br>
                                e‚ÇÇ = 25 - 23 = 2</li>
                            <li><strong>Fonction de co√ªt :</strong><br>
                                J = (1/4) √ó (4¬≤ + 2¬≤) = (1/4) √ó (16 + 4) = 5</li>
                            <li><strong>√âvaluation :</strong><br>
                                Erreur moyenne ‚âà 3 millions FCFA ‚Üí assez bon mais peut √™tre am√©lior√©</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "D√©rivation de la solution optimale",
            content: `
                        <p><strong>üéØ Trouvons les param√®tres qui minimisent J :</strong></p>
                        <p>Au minimum, le gradient s'annule (voir <a href="../math/gradients.html">Module 1.5</a>) :</p>
                        <p>$$\\nabla J = \\vec{0}$$</p>
                        
                        <p><strong>üìê Calcul des d√©riv√©es partielles :</strong></p>
                        
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                            <strong>D√©riv√©e par rapport √† w_j :</strong><br><br>
                            
                            $$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{2n} \\sum_{i=1}^{n} 2(y_i - \\vec{w}^T\\vec{x}_i - b) \\cdot (-x_{ij})$$<br><br>
                            
                            $$= -\\frac{1}{n} \\sum_{i=1}^{n} x_{ij}(y_i - \\vec{w}^T\\vec{x}_i - b)$$<br><br>
                            
                            <strong>D√©riv√©e par rapport √† b :</strong><br><br>
                            
                            $$\\frac{\\partial J}{\\partial b} = -\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\vec{w}^T\\vec{x}_i - b)$$
                        </div>
                        
                        <p><strong>üîß Conditions d'optimalit√© (gradient = 0) :</strong></p>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                            <strong>Syst√®me d'√©quations normales :</strong><br><br>
                            
                            $$\\sum_{i=1}^{n} x_{ij}(y_i - \\vec{w}^T\\vec{x}_i - b) = 0 \\quad \\forall j$$<br><br>
                            
                            $$\\sum_{i=1}^{n} (y_i - \\vec{w}^T\\vec{x}_i - b) = 0$$<br><br>
                            
                            En notation matricielle :<br>
                            $$X^T(\\vec{y} - X\\vec{w} - b\\vec{1}) = \\vec{0}$$<br>
                            $$\\vec{1}^T(\\vec{y} - X\\vec{w} - b\\vec{1}) = 0$$
                        </div>
                        
                        <p><strong>üí° Astuce :</strong> Si on ajoute une colonne de 1 √† X pour le biais, on obtient :</p>
                        <p>$$\\vec{w}^* = (X^TX)^{-1}X^T\\vec{y}$$</p>
                        <p><em>C'est la formule des moindres carr√©s !</em></p>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Exercice : r√©solution manuelle du syst√®me",
            content: `
                        <p><strong>üéØ Exercice √† r√©soudre :</strong></p>
                        <p>R√©solvons manuellement avec nos 4 points de terrains :</p>
                        
                        <p><strong>üìê √âtape 1 : Calcul des moyennes</strong></p>
                        <p>$$\\bar{x}_1 = \\frac{200+300+150+400}{4} = 262.5$$</p>
                        <p>$$\\bar{x}_2 = \\frac{5+3+8+2}{4} = 4.5$$</p>
                        <p>$$\\bar{y} = \\frac{15+25+8+35}{4} = 20.75$$</p>
                        
                        <p><strong>üìù Calculez :</strong></p>
                        <ol>
                            <li>Les √©carts √† la moyenne pour chaque variable</li>
                            <li>Les covariances Cov(x‚ÇÅ,y) et Cov(x‚ÇÇ,y)</li>
                            <li>Les variances Var(x‚ÇÅ) et Var(x‚ÇÇ)</li>
                            <li>La covariance Cov(x‚ÇÅ,x‚ÇÇ)</li>
                            <li>Les coefficients w‚ÇÅ et w‚ÇÇ (formule matricielle)</li>
                        </ol>
                        
                        <p><strong>‚úÖ Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('manual-regression-solution')" style="margin-bottom: 1rem;">
                            üëÅÔ∏è Voir la solution
                        </button>
                        <div id="manual-regression-solution" style="display: none;">
                        <ol>
                            <li><strong>√âcarts √† la moyenne :</strong><br>
                                Surface : [-62.5, 37.5, -112.5, 137.5]<br>
                                Distance : [0.5, -1.5, 3.5, -2.5]<br>
                                Prix : [-5.75, 4.25, -12.75, 14.25]</li>
                            <li><strong>Covariances :</strong><br>
                                Cov(x‚ÇÅ,y) = ((-62.5)√ó(-5.75) + 37.5√ó4.25 + (-112.5)√ó(-12.75) + 137.5√ó14.25)/4<br>
                                = (359.375 + 159.375 + 1434.375 + 1959.375)/4 = 978.125<br><br>
                                Cov(x‚ÇÇ,y) = (0.5√ó(-5.75) + (-1.5)√ó4.25 + 3.5√ó(-12.75) + (-2.5)√ó14.25)/4<br>
                                = (-2.875 - 6.375 - 44.625 - 35.625)/4 = -22.375</li>
                            <li><strong>Variances :</strong><br>
                                Var(x‚ÇÅ) = (62.5¬≤ + 37.5¬≤ + 112.5¬≤ + 137.5¬≤)/4 = 9843.75<br>
                                Var(x‚ÇÇ) = (0.5¬≤ + 1.5¬≤ + 3.5¬≤ + 2.5¬≤)/4 = 4.25</li>
                            <li><strong>Covariance crois√©e :</strong><br>
                                Cov(x‚ÇÅ,x‚ÇÇ) = ((-62.5)√ó0.5 + 37.5√ó(-1.5) + (-112.5)√ó3.5 + 137.5√ó(-2.5))/4<br>
                                = (-31.25 - 56.25 - 393.75 - 343.75)/4 = -206.25</li>
                            <li><strong>Coefficients (formule matricielle) :</strong><br>
                                Matrice de covariance des X :<br>
                                $$C = \\begin{bmatrix} 9843.75 & -206.25 \\\\ -206.25 & 4.25 \\end{bmatrix}$$<br>
                                Vecteur covariances X-y : $$\\vec{c} = \\begin{bmatrix} 978.125 \\\\ -22.375 \\end{bmatrix}$$<br>
                                Solution : $$\\vec{w} = C^{-1}\\vec{c} \\approx \\begin{bmatrix} 0.102 \\\\ -3.85 \\end{bmatrix}$$<br>
                                Biais : $$b = \\bar{y} - \\vec{w}^T\\bar{\\vec{x}} \\approx 20.75 - (0.102√ó262.5 - 3.85√ó4.5) \\approx 4.6$$</li>
                        </ol>
                        <p><strong>üéØ Mod√®le final :</strong> Prix = 0.102√óSurface - 3.85√óDistance + 4.6</p>
                        </div>
                    `,
          },
          {
            type: "mathematique",
            icon: "‚àë",
            title: "Pourquoi cette solution est unique et optimale",
            content: `
                        <p><strong>üîç Unicit√© de la solution :</strong></p>
                        
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                            <strong>Th√©or√®me :</strong> Si la matrice \\(X^TX\\) est inversible, la solution des moindres carr√©s est unique.<br><br>
                            
                            <strong>Preuve g√©om√©trique :</strong><br>
                            La fonction de co√ªt \\(J(\\vec{w}, b)\\) forme un parabolo√Øde (bol) dans l'espace des param√®tres.<br>
                            Un bol n'a qu'UN SEUL point le plus bas !
                        </div>
                        
                        <p><strong>üìê Condition d'inversibilit√© :</strong></p>
                        <p>\\(X^TX\\) est inversible ‚ü∫ les colonnes de X sont lin√©airement ind√©pendantes</p>
                        
                        <p><strong>üîç En pratique, cela signifie :</strong></p>
                        <ul>
                            <li>‚úÖ Pas de features redondantes (ex: x‚ÇÉ = 2√óx‚ÇÅ)</li>
                            <li>‚úÖ Plus d'exemples que de features (n > d)</li>
                            <li>‚úÖ Features avec de la variabilit√© (pas toutes constantes)</li>
                        </ul>
                        
                        <p><strong>‚ö†Ô∏è Cas d√©g√©n√©r√©s :</strong></p>
                        <div style="background: #ffebee; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                            <strong>Si X^TX n'est pas inversible :</strong><br>
                            ‚Ä¢ Infinit√© de solutions optimales<br>
                            ‚Ä¢ Utiliser la pseudo-inverse : \\(\\vec{w}^* = (X^TX)^+X^T\\vec{y}\\)<br>
                            ‚Ä¢ Ou ajouter de la r√©gularisation : \\((X^TX + \\lambda I)^{-1}\\)
                        </div>
                        
                        <p><strong>üéØ Optimalit√© globale :</strong></p>
                        <p>La matrice Hessienne \\(H = X^TX\\) est semi-d√©finie positive ‚Üí minimum global garanti !</p>
                    `,
          },
          {
            type: "code",
            title: "Impl√©mentation from scratch",
            description:
              "Impl√©mentons la r√©gression lin√©aire sans biblioth√®ques :",
            code: `import numpy as np
import matplotlib.pyplot as plt

class RegressionLineaire:
    """R√©gression lin√©aire impl√©ment√©e from scratch"""
    
    def __init__(self):
        self.w = None
        self.b = None
        self.cout_historique = []
    
    def entrainer(self, X, y):
        """Entra√Æner le mod√®le avec la solution analytique"""
        n, d = X.shape
        
        # Ajouter une colonne de 1 pour le biais
        X_avec_biais = np.column_stack([X, np.ones(n)])
        
        # Solution des moindres carr√©s : (X^T X)^(-1) X^T y
        try:
            XTX = X_avec_biais.T @ X_avec_biais
            XTy = X_avec_biais.T @ y
            parametres = np.linalg.solve(XTX, XTy)
            
            self.w = parametres[:-1]  # Tous sauf le dernier
            self.b = parametres[-1]   # Le dernier
            
            print("‚úÖ Entra√Ænement r√©ussi !")
            print(f"Poids : {self.w}")
            print(f"Biais : {self.b:.3f}")
            
        except np.linalg.LinAlgError:
            print("‚ùå Matrice non inversible - donn√©es d√©g√©n√©r√©es")
    
    def predire(self, X):
        """Faire des pr√©dictions"""
        if self.w is None:
            raise ValueError("Mod√®le non entra√Æn√© !")
        return X @ self.w + self.b
    
    def evaluer(self, X, y):
        """√âvaluer les performances"""
        y_pred = self.predire(X)
        
        # M√©triques
        mse = np.mean((y - y_pred) ** 2)
        mae = np.mean(np.abs(y - y_pred))
        
        # R¬≤ (coefficient de d√©termination)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        r2 = 1 - (ss_res / ss_tot)
        
        return {
            'MSE': mse,
            'RMSE': np.sqrt(mse),
            'MAE': mae,
            'R¬≤': r2
        }

# Test avec nos donn√©es de terrains
print("üè† R√âGRESSION LIN√âAIRE : PRIX DES TERRAINS DAKAR")
print("=" * 50)

# Donn√©es
X = np.array([
    [200, 5],   # Surface, Distance
    [300, 3],
    [150, 8],
    [400, 2],
    [250, 6],
    [350, 4]
])

y = np.array([15, 25, 8, 35, 18, 28])  # Prix en millions FCFA

print("üìä Donn√©es d'entra√Ænement :")
print("Surface (m¬≤) | Distance (km) | Prix (M FCFA)")
print("-" * 40)
for i in range(len(X)):
    print(f"{X[i,0]:8.0f} | {X[i,1]:10.0f} | {y[i]:8.0f}")

# Entra√Ænement
modele = RegressionLineaire()
modele.entrainer(X, y)

# √âvaluation
metriques = modele.evaluer(X, y)
print(f"\\nüìä PERFORMANCE :")
print(f"MSE : {metriques['MSE']:.2f}")
print(f"RMSE : {metriques['RMSE']:.2f} millions FCFA")
print(f"R¬≤ : {metriques['R¬≤']:.3f} ({metriques['R¬≤']*100:.1f}% de variance expliqu√©e)")

# Pr√©diction sur nouveau terrain
nouveau_terrain = np.array([[250, 4]])  # 250m¬≤, 4km du centre
prix_predit = modele.predire(nouveau_terrain)
print(f"\\nüéØ PR√âDICTION :")
print(f"Terrain 250m¬≤ √† 4km ‚Üí {prix_predit[0]:.1f} millions FCFA")`,
          },
          {
            type: "code",
            title: "Visualisation des r√©sultats",
            description: "Visualisons notre mod√®le et ses pr√©dictions :",
            code: `# Visualisation 3D des donn√©es et du mod√®le
fig = plt.figure(figsize=(12, 9))
ax = fig.add_subplot(111, projection='3d')

# Points r√©els
ax.scatter(X[:, 0], X[:, 1], y, c='red', s=100, alpha=0.8, label='Donn√©es r√©elles')

# Surface du mod√®le
surface_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 20)
distance_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 20)
Surface_grid, Distance_grid = np.meshgrid(surface_range, distance_range)

# Pr√©dictions sur la grille
X_grid = np.column_stack([Surface_grid.ravel(), Distance_grid.ravel()])
Prix_grid = modele.predire(X_grid).reshape(Surface_grid.shape)

# Surface du mod√®le
ax.plot_surface(Surface_grid, Distance_grid, Prix_grid, alpha=0.3, color='blue')

ax.set_xlabel('Surface (m¬≤)')
ax.set_ylabel('Distance centre (km)')
ax.set_zlabel('Prix (millions FCFA)')
ax.set_title('Mod√®le de R√©gression Lin√©aire - Terrains Dakar')
ax.legend()

plt.show()

# Analyse des r√©sidus
y_pred = modele.predire(X)
residus = y - y_pred

print("\\nüîç ANALYSE DES R√âSIDUS :")
print("Point | R√©el | Pr√©dit | R√©sidu")
print("-" * 35)
for i in range(len(y)):
    print(f"{i+1:3d}   | {y[i]:4.1f} | {y_pred[i]:6.1f} | {residus[i]:6.1f}")

# Graphique des r√©sidus
plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residus, alpha=0.7, s=100)
plt.axhline(y=0, color='red', linestyle='--', alpha=0.8)
plt.xlabel('Valeurs pr√©dites')
plt.ylabel('R√©sidus (r√©el - pr√©dit)')
plt.title('Analyse des R√©sidus')
plt.grid(True, alpha=0.3)
plt.show()

print("\\nüí° Interpr√©tation des r√©sidus :")
if np.std(residus) < 2:
    print("‚úÖ R√©sidus faibles et al√©atoires ‚Üí bon mod√®le")
else:
    print("‚ö†Ô∏è R√©sidus importants ‚Üí mod√®le peut √™tre am√©lior√©")`,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Exercice : r√©gression multiple compl√®te",
            content: `
                        <p><strong>üéØ Exercice √† r√©soudre :</strong></p>
                        <p>Pr√©disons les notes d'√©tudiants avec 3 variables :</p>
                        
                        <table style="margin: 1rem auto; text-align: center; font-size: 0.9rem;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.5rem;">Heures √©tude</th>
                                <th style="padding: 0.5rem;">Heures sommeil</th>
                                <th style="padding: 0.5rem;">Cours assist√©s (%)</th>
                                <th style="padding: 0.5rem;">Note (/20)</th>
                            </tr>
                            <tr><td>10</td><td>8</td><td>90</td><td>16</td></tr>
                            <tr><td>5</td><td>6</td><td>70</td><td>12</td></tr>
                            <tr><td>15</td><td>7</td><td>95</td><td>18</td></tr>
                            <tr><td>8</td><td>9</td><td>80</td><td>14</td></tr>
                            <tr><td>12</td><td>8</td><td>85</td><td>15</td></tr>
                        </table>
                        
                        <p><strong>üìù Questions :</strong></p>
                        <ol>
                            <li>Calculez les coefficients w‚ÇÅ, w‚ÇÇ, w‚ÇÉ et b</li>
                            <li>Interpr√©tez chaque coefficient</li>
                            <li>Pr√©disez la note pour : 9h √©tude, 7h sommeil, 88% pr√©sence</li>
                            <li>Calculez le R¬≤ du mod√®le</li>
                        </ol>
                        
                        <p><strong>‚úÖ Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('multiple-regression-exercise')" style="margin-bottom: 1rem;">
                            üëÅÔ∏è Voir la solution
                        </button>
                        <div id="multiple-regression-exercise" style="display: none;">
                        <p><strong>R√©solution avec NumPy :</strong></p>
                        <pre style="background: #f4f4f4; padding: 1rem; border-radius: 4px; font-size: 0.85rem;">
X = [[10, 8, 90], [5, 6, 70], [15, 7, 95], [8, 9, 80], [12, 8, 85]]
y = [16, 12, 18, 14, 15]
X_avec_biais = [x + [1] for x in X]  # Ajouter colonne de 1

# Solution : w = (X^T X)^(-1) X^T y
# R√©sultat approximatif :
w‚ÇÅ ‚âà 0.65  (heures √©tude)
w‚ÇÇ ‚âà 0.23  (heures sommeil)  
w‚ÇÉ ‚âà 0.08  (pr√©sence)
b ‚âà -2.1   (biais)
</pre>
                        <ol>
                            <li><strong>Coefficients :</strong> w‚ÇÅ=0.65, w‚ÇÇ=0.23, w‚ÇÉ=0.08, b=-2.1</li>
                            <li><strong>Interpr√©tations :</strong><br>
                                ‚Ä¢ +1h √©tude ‚Üí +0.65 point<br>
                                ‚Ä¢ +1h sommeil ‚Üí +0.23 point<br>
                                ‚Ä¢ +1% pr√©sence ‚Üí +0.08 point<br>
                                ‚Ä¢ √âtude > Sommeil > Pr√©sence en importance</li>
                            <li><strong>Pr√©diction :</strong><br>
                                Note = 0.65√ó9 + 0.23√ó7 + 0.08√ó88 - 2.1<br>
                                = 5.85 + 1.61 + 7.04 - 2.1 = 12.4/20</li>
                            <li><strong>R¬≤ :</strong> Environ 0.85 (85% de variance expliqu√©e)</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "concept",
            icon: "üí°",
            title: "R√©gularisation : contr√¥ler la complexit√©",
            content: `
                        <p><strong>ü§î Probl√®me :</strong> Que faire si on a trop de features par rapport aux donn√©es ?</p>
                        <p>Avec 1000 features et 100 exemples, le mod√®le peut "m√©moriser" au lieu d'apprendre !</p>
                        
                        <p><strong>üí° Solution :</strong> P√©naliser les poids trop grands</p>
                        
                        <p><strong>üéØ R√©gularisation Ridge (L2) :</strong></p>
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                            $$J_{Ridge}(\\vec{w}, b) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\vec{w}^T\\vec{x}_i - b)^2 + \\lambda ||\\vec{w}||_2^2$$<br><br>
                            
                            <strong>Solution modifi√©e :</strong><br>
                            $$\\vec{w}^* = (X^TX + \\lambda I)^{-1}X^T\\vec{y}$$<br><br>
                            
                            <strong>Effet :</strong> Œª > 0 garantit l'inversibilit√© et r√©duit les poids
                        </div>
                        
                        <p><strong>üéØ R√©gularisation Lasso (L1) :</strong></p>
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                            $$J_{Lasso}(\\vec{w}, b) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\vec{w}^T\\vec{x}_i - b)^2 + \\lambda ||\\vec{w}||_1$$<br><br>
                            
                            <strong>Effet sp√©cial :</strong> Met certains poids √† exactement 0<br>
                            ‚Üí S√©lection automatique de features !<br><br>
                            
                            <strong>Pas de solution analytique</strong> ‚Üí optimisation it√©rative
                        </div>
                        
                        <p><strong>‚öñÔ∏è Choix de Œª :</strong></p>
                        <ul>
                            <li>Œª = 0 : r√©gression classique</li>
                            <li>Œª petit : peu de r√©gularisation</li>
                            <li>Œª grand : forte r√©gularisation (peut cr√©er du biais)</li>
                            <li>üéØ Optimal : validation crois√©e pour choisir Œª</li>
                        </ul>
                    `,
          },
          {
            type: "code",
            title: "Comparaison avec/sans r√©gularisation",
            description: "Comparons les effets de la r√©gularisation :",
            code: `# G√©n√©ration de donn√©es avec bruit
np.random.seed(42)
n_samples = 50
n_features = 8

# Features corr√©l√©es (probl√®me r√©aliste)
X_base = np.random.randn(n_samples, 3)
X_correlated = np.column_stack([
    X_base,
    X_base[:, 0] + 0.1 * np.random.randn(n_samples),  # Corr√©l√©e √† x1
    X_base[:, 1] + 0.1 * np.random.randn(n_samples),  # Corr√©l√©e √† x2
    X_base[:, 2] + 0.1 * np.random.randn(n_samples),  # Corr√©l√©e √† x3
    np.random.randn(n_samples),  # Ind√©pendante
    np.random.randn(n_samples)   # Ind√©pendante
])

# Vraie relation (seulement 3 features importantes)
y_true = 2*X_base[:, 0] + 1.5*X_base[:, 1] - 1*X_base[:, 2] + 5
y_noisy = y_true + 0.5 * np.random.randn(n_samples)

print("üß™ COMPARAISON R√âGULARISATION")
print("=" * 40)
print(f"Donn√©es : {n_samples} exemples, {n_features} features")
print(f"Vraies features importantes : 3/{n_features}")

# Mod√®le sans r√©gularisation
modele_normal = RegressionLineaire()
modele_normal.entrainer(X_correlated, y_noisy)

print(f"\\nüìä SANS R√âGULARISATION :")
print(f"Poids : {modele_normal.w.round(3)}")
print(f"Norme des poids : {np.linalg.norm(modele_normal.w):.3f}")

# Simulation Ridge (ajout manuel de r√©gularisation)
def ridge_regression(X, y, lambda_reg):
    """R√©gression Ridge manuelle"""
    n, d = X.shape
    X_avec_biais = np.column_stack([X, np.ones(n)])
    
    # Matrice de r√©gularisation (ne pas r√©gulariser le biais)
    reg_matrix = lambda_reg * np.eye(d + 1)
    reg_matrix[-1, -1] = 0  # Pas de r√©gularisation sur le biais
    
    XTX_reg = X_avec_biais.T @ X_avec_biais + reg_matrix
    XTy = X_avec_biais.T @ y
    parametres = np.linalg.solve(XTX_reg, XTy)
    
    return parametres[:-1], parametres[-1]

# Test avec diff√©rents Œª
lambdas = [0.1, 1.0, 10.0]
print(f"\\nüìä AVEC R√âGULARISATION RIDGE :")

for lam in lambdas:
    w_ridge, b_ridge = ridge_regression(X_correlated, y_noisy, lam)
    norme = np.linalg.norm(w_ridge)
    print(f"Œª={lam:4.1f} : Norme poids = {norme:.3f}")
    print(f"         Poids = {w_ridge.round(3)}")

print(f"\\nüí° Observation :")
print(f"Plus Œª augmente, plus les poids diminuent (r√©gularisation)")`,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Exercice : interpr√©tation des coefficients",
            content: `
                        <p><strong>üéØ Exercice √† r√©soudre :</strong></p>
                        <p>Un mod√®le pr√©dit les ventes d'un magasin avec :</p>
                        <p><strong>Mod√®le :</strong> Ventes = 2.5√óPublicit√© + 1.8√óTemp√©rature - 0.3√óPluie + 1000</p>
                        
                        <p><strong>üìù Questions :</strong></p>
                        <ol>
                            <li>Que signifie chaque coefficient ?</li>
                            <li>Quelle variable a le plus d'impact ?</li>
                            <li>Pr√©disez les ventes avec : 50‚Ç¨ pub, 25¬∞C, 2mm pluie</li>
                            <li>De combien augmentent les ventes si on double la publicit√© ?</li>
                            <li>Ce mod√®le est-il coh√©rent avec la r√©alit√© ?</li>
                        </ol>
                        
                        <p><strong>‚úÖ Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('coefficient-interpretation-exercise')" style="margin-bottom: 1rem;">
                            üëÅÔ∏è Voir la solution
                        </button>
                        <div id="coefficient-interpretation-exercise" style="display: none;">
                        <ol>
                            <li><strong>Signification des coefficients :</strong><br>
                                ‚Ä¢ 2.5 : +1‚Ç¨ de pub ‚Üí +2.5 ventes<br>
                                ‚Ä¢ 1.8 : +1¬∞C ‚Üí +1.8 ventes<br>
                                ‚Ä¢ -0.3 : +1mm pluie ‚Üí -0.3 ventes<br>
                                ‚Ä¢ 1000 : ventes de base (sans pub, 0¬∞C, pas de pluie)</li>
                            <li><strong>Impact :</strong> Publicit√© (2.5) > Temp√©rature (1.8) > Pluie (0.3)</li>
                            <li><strong>Pr√©diction :</strong><br>
                                Ventes = 2.5√ó50 + 1.8√ó25 - 0.3√ó2 + 1000<br>
                                = 125 + 45 - 0.6 + 1000 = 1169.4 ventes</li>
                            <li><strong>Doubler la pub :</strong><br>
                                Œî Ventes = 2.5 √ó (100-50) = +125 ventes</li>
                            <li><strong>Coh√©rence :</strong><br>
                                ‚úÖ Plus de pub ‚Üí plus de ventes (logique)<br>
                                ‚úÖ Plus chaud ‚Üí plus de ventes (boissons, glaces)<br>
                                ‚úÖ Plus de pluie ‚Üí moins de ventes (gens restent chez eux)<br>
                                ‚ö†Ô∏è Biais de 1000 semble √©lev√© (√† v√©rifier)</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "concept",
            icon: "üí°",
            title: "Hypoth√®ses et limites de la r√©gression lin√©aire",
            content: `
                        <p><strong>‚ö†Ô∏è La r√©gression lin√©aire fait des hypoth√®ses importantes :</strong></p>
                        
                        <p><strong>üìê 1. Lin√©arit√© :</strong></p>
                        <p>La relation entre X et y est lin√©aire</p>
                        <ul>
                            <li>‚úÖ <strong>Bon :</strong> prix = surface √ó prix_m¬≤ + constante</li>
                            <li>‚ùå <strong>Mauvais :</strong> croissance exponentielle, relations en U</li>
                        </ul>
                        
                        <p><strong>üìä 2. Ind√©pendance des erreurs :</strong></p>
                        <p>Les r√©sidus sont ind√©pendants entre eux</p>
                        <ul>
                            <li>‚úÖ <strong>Bon :</strong> erreurs al√©atoires</li>
                            <li>‚ùå <strong>Mauvais :</strong> donn√©es temporelles avec tendance</li>
                        </ul>
                        
                        <p><strong>üìà 3. Homosc√©dasticit√© :</strong></p>
                        <p>La variance des erreurs est constante</p>
                        <ul>
                            <li>‚úÖ <strong>Bon :</strong> r√©sidus dispers√©s uniform√©ment</li>
                            <li>‚ùå <strong>Mauvais :</strong> r√©sidus en forme de c√¥ne</li>
                        </ul>
                        
                        <p><strong>üîî 4. Normalit√© des erreurs :</strong></p>
                        <p>Les r√©sidus suivent une loi normale</p>
                        <ul>
                            <li>‚úÖ <strong>Bon :</strong> histogramme des r√©sidus en cloche</li>
                            <li>‚ùå <strong>Mauvais :</strong> r√©sidus asym√©triques ou multimodaux</li>
                        </ul>
                        
                        <p><strong>üîç Comment v√©rifier ces hypoth√®ses ?</strong></p>
                        <ul>
                            <li>üìä <strong>Graphiques de r√©sidus</strong> : patterns r√©v√©lateurs</li>
                            <li>üìà <strong>Tests statistiques</strong> : Shapiro-Wilk, Breusch-Pagan</li>
                            <li>üéØ <strong>Validation crois√©e</strong> : performance stable ?</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "üíª",
            title: "Exercice : diagnostic d'un mod√®le",
            content: `
                        <p><strong>üéØ Exercice √† r√©soudre :</strong></p>
                        <p>Analysez ces 3 mod√®les de r√©gression :</p>
                        
                        <p><strong>üìä Mod√®le A :</strong></p>
                        <ul>
                            <li>R¬≤ train = 0.95, R¬≤ test = 0.93</li>
                            <li>R√©sidus : distribution normale, variance constante</li>
                            <li>Coefficients : [2.1, -0.8, 1.5]</li>
                        </ul>
                        
                        <p><strong>üìä Mod√®le B :</strong></p>
                        <ul>
                            <li>R¬≤ train = 0.99, R¬≤ test = 0.65</li>
                            <li>R√©sidus : patterns en forme de U</li>
                            <li>Coefficients : [15.2, -8.7, 22.1, -11.3, 9.8]</li>
                        </ul>
                        
                        <p><strong>üìä Mod√®le C :</strong></p>
                        <ul>
                            <li>R¬≤ train = 0.45, R¬≤ test = 0.43</li>
                            <li>R√©sidus : al√©atoires mais variance √©lev√©e</li>
                            <li>Coefficients : [0.2, 0.1]</li>
                        </ul>
                        
                        <p><strong>üìù Pour chaque mod√®le, diagnostiquez :</strong></p>
                        <ol>
                            <li>Y a-t-il overfitting ou underfitting ?</li>
                            <li>Les hypoth√®ses sont-elles respect√©es ?</li>
                            <li>Quelles am√©liorations proposer ?</li>
                        </ol>
                        
                        <p><strong>‚úÖ Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('model-diagnosis-exercise')" style="margin-bottom: 1rem;">
                            üëÅÔ∏è Voir la solution
                        </button>
                        <div id="model-diagnosis-exercise" style="display: none;">
                        <ol>
                            <li><strong>Diagnostics :</strong><br>
                                <strong>Mod√®le A :</strong> ‚úÖ Excellent ! Bon √©quilibre, hypoth√®ses respect√©es<br>
                                <strong>Mod√®le B :</strong> ‚ùå Overfitting s√©v√®re (0.99 vs 0.65), non-lin√©arit√© (r√©sidus en U)<br>
                                <strong>Mod√®le C :</strong> ‚ùå Underfitting (performances faibles partout)</li>
                            <li><strong>Hypoth√®ses :</strong><br>
                                <strong>A :</strong> Toutes respect√©es<br>
                                <strong>B :</strong> Lin√©arit√© viol√©e (patterns dans r√©sidus)<br>
                                <strong>C :</strong> Probablement respect√©es mais mod√®le trop simple</li>
                            <li><strong>Am√©liorations :</strong><br>
                                <strong>A :</strong> Rien √† changer, mod√®le optimal<br>
                                <strong>B :</strong> R√©gularisation forte, features polynomiales, ou mod√®le non-lin√©aire<br>
                                <strong>C :</strong> Plus de features, interactions, mod√®le plus complexe</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "code",
            title: "R√©gression polynomiale : capturer la non-lin√©arit√©",
            description:
              "√âtendons la r√©gression lin√©aire aux relations courbes :",
            code: `# Donn√©es avec relation non-lin√©aire
np.random.seed(42)
x_nonlin = np.linspace(0, 4, 20)
y_nonlin = 2*x_nonlin**2 - 3*x_nonlin + 1 + 0.5*np.random.randn(20)

print("üìà R√âGRESSION POLYNOMIALE")
print("=" * 40)

def creer_features_polynomiales(x, degre):
    """Cr√©er features polynomiales jusqu'au degr√© donn√©"""
    X_poly = np.column_stack([x**i for i in range(1, degre+1)])
    return X_poly

def regression_polynomiale(x, y, degre):
    """R√©gression polynomiale de degr√© donn√©"""
    X_poly = creer_features_polynomiales(x, degre)
    
    # Ajouter biais
    X_avec_biais = np.column_stack([X_poly, np.ones(len(x))])
    
    # Solution des moindres carr√©s
    parametres = np.linalg.solve(X_avec_biais.T @ X_avec_biais, 
                                X_avec_biais.T @ y)
    
    return parametres[:-1], parametres[-1]  # poids, biais

# Test diff√©rents degr√©s
degres = [1, 2, 3, 5]
for degre in degres:
    w_poly, b_poly = regression_polynomiale(x_nonlin, y_nonlin, degre)
    
    # Pr√©dictions
    X_poly = creer_features_polynomiales(x_nonlin, degre)
    y_pred = X_poly @ w_poly + b_poly
    
    # M√©triques
    mse = np.mean((y_nonlin - y_pred)**2)
    r2 = 1 - np.sum((y_nonlin - y_pred)**2) / np.sum((y_nonlin - np.mean(y_nonlin))**2)
    
    print(f"\\nDegr√© {degre} :")
    print(f"  Coefficients : {w_poly.round(3)}")
    print(f"  MSE : {mse:.3f}")
    print(f"  R¬≤ : {r2:.3f}")

# Visualisation
plt.figure(figsize=(12, 8))

# Points originaux
plt.scatter(x_nonlin, y_nonlin, color='red', s=50, alpha=0.7, label='Donn√©es')

# Courbes pour chaque degr√©
x_smooth = np.linspace(0, 4, 100)
couleurs = ['blue', 'green', 'orange', 'purple']

for i, degre in enumerate(degres):
    w_poly, b_poly = regression_polynomiale(x_nonlin, y_nonlin, degre)
    X_smooth = creer_features_polynomiales(x_smooth, degre)
    y_smooth = X_smooth @ w_poly + b_poly
    
    plt.plot(x_smooth, y_smooth, color=couleurs[i], linewidth=2, 
             label=f'Degr√© {degre}')

plt.xlabel('x')
plt.ylabel('y')
plt.title('R√©gression Polynomiale - Diff√©rents Degr√©s')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print("\\nüí° Observation :")
print("Degr√© 2 capture bien la vraie relation (parabole)")
print("Degr√©s √©lev√©s risquent l'overfitting")`,
          },
          {
            type: "warning",
            icon: "‚ö†Ô∏è",
            title: "R√©gression lin√©aire : fondation de l'IA moderne",
            content: `
                        <p><strong>üß† Pourquoi la r√©gression lin√©aire est-elle si importante ?</strong></p>
                        
                        <p><strong>üèóÔ∏è Fondation th√©orique :</strong></p>
                        <ul>
                            <li>üéØ <strong>Optimisation convexe</strong> : un seul minimum global</li>
                            <li>üìä <strong>Solution analytique</strong> : formule exacte sans it√©rations</li>
                            <li>üìà <strong>Interpr√©tabilit√©</strong> : coefficients ont un sens clair</li>
                            <li>‚ö° <strong>Rapidit√©</strong> : calcul instantan√© m√™me sur gros datasets</li>
                        </ul>
                        
                        <p><strong>ü§ñ Connexions avec l'IA avanc√©e :</strong></p>
                        <ul>
                            <li>üß† <strong>R√©seaux de neurones</strong> : chaque neurone fait une r√©gression</li>
                            <li>üéØ <strong>SVM</strong> : r√©gression dans l'espace des features</li>
                            <li>üå≥ <strong>Random Forest</strong> : moyenne de r√©gressions sur arbres</li>
                            <li>üìä <strong>Deep Learning</strong> : composition de r√©gressions lin√©aires + non-lin√©arit√©s</li>
                        </ul>
                        
                        <p><strong>üîß Outils pratiques :</strong></p>
                        <ul>
                            <li>üìã <strong>Baseline</strong> : toujours commencer par une r√©gression simple</li>
                            <li>üîç <strong>Feature importance</strong> : coefficients r√©v√®lent l'importance</li>
                            <li>üìà <strong>Debugging</strong> : comprendre pourquoi un mod√®le complexe √©choue</li>
                            <li>‚öñÔ∏è <strong>R√©gularisation</strong> : contr√¥ler la complexit√©</li>
                        </ul>
                        
                        <p><strong>üí° Points cl√©s √† retenir :</strong></p>
                        <div style="background: #d1ecf1; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                            1. <strong>Simplicit√© ‚â† Faiblesse</strong> : souvent la meilleure solution<br>
                            2. <strong>Interpr√©tabilit√©</strong> : comprendre > performance pure<br>
                            3. <strong>Fondation solide</strong> : ma√Ætriser avant d'aller plus loin<br>
                            4. <strong>Diagnostic essentiel</strong> : v√©rifier les hypoth√®ses<br>
                            5. <strong>R√©gularisation</strong> : contr√¥ler overfitting
                        </div>
                        
                        <p><strong>üîÆ Prochaine √©tape :</strong></p>
                        <p>Maintenant que vous ma√Ætrisez la r√©gression, nous allons voir la <strong>classification</strong> - pr√©dire des cat√©gories au lieu de valeurs continues !</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ü§î Que se passe-t-il si tous les points sont parfaitement align√©s sur une droite ?",
          options: [
            "A) La r√©gression √©choue",
            "B) Il y a une infinit√© de solutions",
            "C) L'erreur J = 0 et la solution est exacte",
            "D) Les coefficients deviennent infinis",
          ],
          correct: 2,
          explanation:
            "Si les points sont parfaitement align√©s, la r√©gression trouve LA droite qui passe exactement par tous les points. L'erreur J = 0 (aucune erreur !), et les formules donnent les param√®tres exacts de cette droite. C'est le cas id√©al o√π le mod√®le lin√©aire correspond parfaitement √† la r√©alit√©.",
        },
        prevModule: "introduction.html",
        nextModule: "classification.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
