<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>R√©gression Lin√©aire | IA4Ndada</title>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">üè† Accueil</a>
          <span>‚Ä∫</span>
          <span>ü§ñ Machine Learning</span>
          <span>‚Ä∫</span>
          <span>R√©gression Lin√©aire</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div class="progress-fill" id="progress-fill" style="width: 0%"></div>
          </div>
        </div>
      </div>
    </nav>

    <div class="container">
      <h1>üìà R√©gression Lin√©aire</h1>
      <p class="subtitle">Module 3.2 - Pr√©dire des valeurs continues</p>

      <div id="pyodide-status" class="pyodide-status">
        <span class="status-loading">‚è≥ Chargement de Python...</span>
      </div>

      <div class="objectives">
        <h2>üéØ Objectifs d'apprentissage</h2>
        <ul id="objectives-list"></ul>
      </div>

      <div id="module-content"></div>

      <div class="checkpoint">
        <h3>üéâ Checkpoint - R√©gression Lin√©aire</h3>
        <p>Vous ma√Ætrisez maintenant le premier algorithme d'apprentissage automatique !</p>
        <button class="checkpoint-btn" id="checkpoint-btn" onclick="completeCheckpoint()" disabled>
          Valider le module
        </button>
      </div>

      <div class="module-nav">
        <a href="introduction.html" class="nav-link">‚Üê Introduction ML</a>
        <a href="classification.html" class="nav-link">Classification ‚Üí</a>
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      const moduleConfig = {
        id: "ml-linear-regression",
        title: "R√©gression Lin√©aire",
        objectives: [
          "Comprendre le probl√®me de r√©gression",
          "Construire la fonction de co√ªt MSE",
          "Calculer la solution analytique",
          "Impl√©menter et interpr√©ter le mod√®le",
        ],
        content: [
          // Section 1: Le probl√®me
          {
            type: "concept",
            icon: "üí°",
            title: "Le probl√®me : pr√©dire des valeurs continues",
            content: `
              <p>La <strong>r√©gression lin√©aire</strong> pr√©dit une valeur num√©rique √† partir de variables d'entr√©e.</p>

              <p><strong>Exemples concrets :</strong></p>
              <ul>
                <li>üè† Prix d'une maison selon sa surface</li>
                <li>üìà Cours d'une action selon des indicateurs</li>
                <li>üåæ Rendement agricole selon la pluviom√©trie</li>
              </ul>

              <p><strong>L'hypoth√®se de base :</strong></p>
              <p>La relation est <strong>lin√©aire</strong> :</p>
              <p style="text-align: center; font-size: 1.2em;">$$y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$</p>

              <p>o√π :</p>
              <ul>
                <li><strong>w‚ÇÅ, w‚ÇÇ, ...</strong> : poids (importance de chaque variable)</li>
                <li><strong>b</strong> : biais (valeur de base)</li>
              </ul>
            `,
          },
          {
            type: "quiz",
            title: "V√©rification - R√©gression vs Classification",
            question: "Lequel de ces probl√®mes est un probl√®me de R√âGRESSION ?",
            options: [
              "Pr√©dire si un email est spam ou non",
              "Pr√©dire le prix d'une voiture d'occasion",
              "Classifier une image comme chat ou chien",
              "D√©tecter si une transaction est frauduleuse",
            ],
            correctAnswer: 1,
            explanation: "Le prix est une valeur continue (150 000 FCFA, 2 500 000 FCFA...) ‚Üí r√©gression. Les autres sont des classifications binaires (spam/pas spam, chat/chien, fraude/l√©gitime).",
          },
          // Section 2: Fonction de co√ªt
          {
            type: "mathematique",
            icon: "‚àë",
            title: "La fonction de co√ªt MSE",
            content: `
              <p><strong>Comment mesurer la qualit√© d'un mod√®le ?</strong></p>

              <p>Pour chaque pr√©diction, on calcule l'erreur :</p>
              <p style="text-align: center;">$$e_i = y_i - \\hat{y}_i = y_i - (\\vec{w}^T\\vec{x}_i + b)$$</p>

              <p><strong>Pourquoi utiliser le carr√© des erreurs ?</strong></p>
              <ul>
                <li>‚ùå Somme simple : les erreurs +/- s'annulent</li>
                <li>‚ö†Ô∏è Valeur absolue : non d√©rivable en 0</li>
                <li>‚úÖ Carr√© : toujours positif ET d√©rivable</li>
              </ul>

              <p><strong>MSE (Mean Squared Error) :</strong></p>
              <p style="text-align: center; font-size: 1.1em; background: #e8f5e9; padding: 1rem; border-radius: 8px;">
                $$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$
              </p>

              <p><strong>Objectif :</strong> Trouver w et b qui minimisent le MSE.</p>
            `,
          },
          {
            type: "exercise",
            title: "Calcul manuel du MSE",
            exerciseType: "numeric",
            content: `
              <p>Un mod√®le pr√©dit :</p>
              <table style="border-collapse: collapse; margin: 1rem 0;">
                <tr style="background: #f0f0f0;">
                  <th style="padding: 0.5rem; border: 1px solid #ddd;">R√©el (y)</th>
                  <th style="padding: 0.5rem; border: 1px solid #ddd;">Pr√©dit (≈∑)</th>
                </tr>
                <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">100</td><td style="padding: 0.5rem; border: 1px solid #ddd;">90</td></tr>
                <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">150</td><td style="padding: 0.5rem; border: 1px solid #ddd;">160</td></tr>
                <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">200</td><td style="padding: 0.5rem; border: 1px solid #ddd;">195</td></tr>
              </table>
              <p><strong>Calculez le MSE</strong></p>
            `,
            correctAnswer: 75,
            tolerance: 0.1,
            hint: "Erreurs : (100-90)¬≤=100, (150-160)¬≤=100, (200-195)¬≤=25. MSE = moyenne.",
            explanation: "MSE = (100 + 100 + 25) / 3 = 225 / 3 = 75",
          },
          // Section 3: Solution analytique
          {
            type: "mathematique",
            icon: "üìê",
            title: "La solution analytique",
            content: `
              <p>Pour minimiser le MSE, on cherche o√π le gradient s'annule :</p>
              <p style="text-align: center;">$$\\nabla_{w,b} MSE = 0$$</p>

              <p><strong>En notation matricielle :</strong></p>
              <p>Si on ajoute une colonne de 1 √† X pour le biais :</p>

              <p style="text-align: center; font-size: 1.1em; background: #e3f2fd; padding: 1rem; border-radius: 8px;">
                $$\\vec{w}^* = (X^TX)^{-1}X^T\\vec{y}$$
              </p>

              <p><strong>Conditions :</strong></p>
              <ul>
                <li>X<sup>T</sup>X doit √™tre inversible</li>
                <li>Plus d'exemples que de features (n > d)</li>
                <li>Pas de features redondantes</li>
              </ul>

              <p><strong>Complexit√© :</strong> O(n√ód¬≤ + d¬≥) ‚Äî rapide !</p>
            `,
          },
          {
            type: "quiz",
            title: "Quiz - Solution analytique",
            question: "Pourquoi utilise-t-on (X^T X)^{-1} X^T y plut√¥t que X^{-1} y directement ?",
            options: [
              "C'est plus rapide √† calculer",
              "X n'est g√©n√©ralement pas carr√©e, donc pas inversible",
              "X^{-1} donnerait le m√™me r√©sultat",
              "C'est une convention arbitraire",
            ],
            correctAnswer: 1,
            explanation: "X a n lignes (exemples) et d colonnes (features). Comme n ‚â† d g√©n√©ralement, X n'est pas carr√©e et donc pas inversible. X^T X est carr√©e (d√ód) et peut √™tre invers√©e.",
          },
          // Section 4: Impl√©mentation
          {
            type: "code",
            title: "Impl√©mentation from scratch",
            description: "Impl√©mentons la r√©gression lin√©aire en Python :",
            code: `import numpy as np
import matplotlib.pyplot as plt

# Donn√©es : Prix de terrains (surface m¬≤, distance km, prix M FCFA)
X = np.array([
    [200, 5],
    [300, 3],
    [150, 8],
    [400, 2],
    [250, 6],
    [350, 4]
])
y = np.array([15, 25, 8, 35, 18, 28])

print("üìä DONN√âES : TERRAINS √Ä DAKAR")
print("Surface (m¬≤) | Distance (km) | Prix (M FCFA)")
print("-" * 45)
for i in range(len(X)):
    print(f"    {X[i,0]:4d}     |      {X[i,1]:d}        |     {y[i]:2d}")

# R√©gression lin√©aire analytique
def regression_lineaire(X, y):
    """Solution analytique : w = (X^T X)^(-1) X^T y"""
    # Ajouter colonne de 1 pour le biais
    n = len(X)
    X_b = np.column_stack([X, np.ones(n)])

    # Formule des moindres carr√©s
    params = np.linalg.solve(X_b.T @ X_b, X_b.T @ y)

    return params[:-1], params[-1]  # poids, biais

# Entra√Æner
w, b = regression_lineaire(X, y)
print(f"\\nü§ñ MOD√àLE ENTRA√éN√â")
print(f"Prix = {w[0]:.4f} √ó Surface + {w[1]:.2f} √ó Distance + {b:.2f}")

# Interpr√©tation
print(f"\\nüìä INTERPR√âTATION :")
print(f"‚Ä¢ +1 m¬≤ ‚Üí +{w[0]*1000:.0f} FCFA")
print(f"‚Ä¢ +1 km du centre ‚Üí {w[1]:.2f} M FCFA (diminution)")

# Pr√©diction
nouveau = np.array([[280, 5]])
prix_predit = nouveau @ w + b
print(f"\\nüéØ PR√âDICTION : Terrain 280m¬≤ √† 5km ‚Üí {prix_predit[0]:.1f} M FCFA")

# √âvaluation
y_pred = X @ w + b
mse = np.mean((y - y_pred)**2)
r2 = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)
print(f"\\nüìà PERFORMANCE : MSE={mse:.2f}, R¬≤={r2:.3f}")`,
          },
          {
            type: "exercise",
            title: "Interpr√©tation des coefficients",
            exerciseType: "mcq",
            content: `
              <p>Un mod√®le pr√©dit les ventes d'un magasin :</p>
              <p><code>Ventes = 3.2 √ó Temp√©rature + 1.5 √ó Publicit√© - 0.8 √ó Pluie + 500</code></p>
              <p><strong>Que signifie le coefficient -0.8 pour la pluie ?</strong></p>
            `,
            options: [
              "La pluie n'a pas d'effet sur les ventes",
              "Chaque mm de pluie suppl√©mentaire diminue les ventes de 0.8",
              "Il pleut en moyenne 0.8 fois par jour",
              "Les ventes augmentent de 0.8 quand il pleut",
            ],
            correctAnswer: 1,
            explanation: "Un coefficient n√©gatif signifie une relation inverse. Chaque unit√© suppl√©mentaire de pluie diminue les ventes de 0.8 unit√©s. C'est logique : les gens sortent moins quand il pleut.",
          },
          // Section 5: Exercice code
          {
            type: "exercise-code",
            title: "Calculez le R¬≤ d'un mod√®le",
            content: `
              <p>Le coefficient R¬≤ mesure la proportion de variance expliqu√©e :</p>
              <p style="text-align: center;">$$R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$</p>
              <p>Compl√©tez le code pour calculer R¬≤ :</p>
            `,
            starterCode: `import numpy as np

# Donn√©es
y_reel = np.array([10, 15, 20, 25, 30])
y_predit = np.array([12, 14, 19, 26, 29])

# Calculer R¬≤
y_mean = np.mean(y_reel)

# TODO: Calculer SS_res (somme des carr√©s des r√©sidus)
ss_res = np.sum((y_reel - y_predit) ** 2)

# TODO: Calculer SS_tot (somme des carr√©s totaux)
ss_tot = np.sum((y_reel - y_mean) ** 2)

# TODO: Calculer R¬≤
r2 = 1 - (ss_res / ss_tot)

print(f"SS_res = {ss_res}")
print(f"SS_tot = {ss_tot}")
print(f"R¬≤ = {r2:.4f}")
print(f"\\nLe mod√®le explique {r2*100:.1f}% de la variance")`,
            solution: `import numpy as np

y_reel = np.array([10, 15, 20, 25, 30])
y_predit = np.array([12, 14, 19, 26, 29])

y_mean = np.mean(y_reel)
ss_res = np.sum((y_reel - y_predit) ** 2)
ss_tot = np.sum((y_reel - y_mean) ** 2)
r2 = 1 - (ss_res / ss_tot)

print(f"SS_res = {ss_res}")
print(f"SS_tot = {ss_tot}")
print(f"R¬≤ = {r2:.4f}")`,
            expectedOutput: "R¬≤",
          },
          // Section 6: R√©gularisation
          {
            type: "concept",
            icon: "‚öñÔ∏è",
            title: "R√©gularisation : √©viter l'overfitting",
            content: `
              <p><strong>Probl√®me :</strong> Avec beaucoup de features, le mod√®le peut m√©moriser au lieu d'apprendre.</p>

              <p><strong>Solution :</strong> P√©naliser les poids trop grands.</p>

              <p><strong>Ridge (L2) :</strong></p>
              <p style="text-align: center;">$$J_{Ridge} = MSE + \\lambda \\sum w_i^2$$</p>
              <p>R√©duit tous les poids proportionnellement.</p>

              <p><strong>Lasso (L1) :</strong></p>
              <p style="text-align: center;">$$J_{Lasso} = MSE + \\lambda \\sum |w_i|$$</p>
              <p>Met certains poids √† exactement 0 ‚Üí s√©lection de features.</p>

              <p><strong>Choix de Œª :</strong></p>
              <ul>
                <li>Œª = 0 : pas de r√©gularisation</li>
                <li>Œª petit : l√©g√®re r√©gularisation</li>
                <li>Œª grand : forte r√©gularisation (peut sous-apprendre)</li>
              </ul>
            `,
          },
          {
            type: "quiz",
            title: "Quiz - R√©gularisation",
            question: "Vous avez un mod√®le avec 100 features mais seulement 50 exemples. Que risque-t-il de se passer ?",
            options: [
              "Le mod√®le sera trop simple (underfitting)",
              "Le mod√®le m√©morisera les donn√©es (overfitting)",
              "Le mod√®le fonctionnera parfaitement",
              "Impossible de calculer une solution",
            ],
            correctAnswer: 1,
            explanation: "Plus de features que d'exemples = risque majeur d'overfitting. Le mod√®le peut trouver des combinaisons de features qui passent par tous les points. Solution : r√©gularisation forte (Ridge/Lasso) ou r√©duction de features.",
          },
          // Section 7: Visualisation
          {
            type: "code",
            title: "Visualisation : underfitting vs overfitting",
            description: "Comparons diff√©rents niveaux de complexit√© :",
            code: `import numpy as np
import matplotlib.pyplot as plt

# Donn√©es : relation parabolique avec bruit
np.random.seed(42)
X = np.linspace(0, 10, 15)
y_true = 0.5 * X**2 - 2*X + 3
y = y_true + np.random.randn(15) * 2

# R√©gression polynomiale de diff√©rents degr√©s
def fit_polynomial(X, y, degree):
    coeffs = np.polyfit(X, y, degree)
    return coeffs

# Visualisation
fig, axes = plt.subplots(1, 3, figsize=(14, 4))
X_plot = np.linspace(0, 10, 100)
degrees = [1, 2, 10]
titles = ['UNDERFITTING\\n(trop simple)', 'BON √âQUILIBRE', 'OVERFITTING\\n(trop complexe)']
colors = ['red', 'green', 'purple']

for ax, deg, title, color in zip(axes, degrees, titles, colors):
    # Donn√©es
    ax.scatter(X, y, c='blue', s=50, label='Donn√©es', zorder=5)

    # Mod√®le
    coeffs = fit_polynomial(X, y, deg)
    y_pred = np.polyval(coeffs, X_plot)
    ax.plot(X_plot, y_pred, c=color, linewidth=2, label=f'Degr√© {deg}')

    # Vraie fonction
    ax.plot(X_plot, 0.5*X_plot**2 - 2*X_plot + 3, 'g--', alpha=0.5, label='Vraie relation')

    # MSE
    y_model = np.polyval(coeffs, X)
    mse = np.mean((y - y_model)**2)
    ax.set_title(f'{title}\\nMSE train = {mse:.1f}', fontsize=11)
    ax.legend(fontsize=8)
    ax.set_ylim(-5, 30)

plt.tight_layout()
plt.show()

print("üìä ANALYSE :")
print("‚Ä¢ Degr√© 1 (droite) : ne capture pas la courbure")
print("‚Ä¢ Degr√© 2 : capture bien la vraie relation parabolique")
print("‚Ä¢ Degr√© 10 : passe par tous les points mais sera mauvais sur nouvelles donn√©es")`,
          },
          {
            type: "exercise",
            title: "Diagnostic de mod√®le",
            exerciseType: "mcq",
            content: `
              <p>Un mod√®le de r√©gression donne :</p>
              <ul>
                <li>MSE sur train : <strong>5</strong></li>
                <li>MSE sur test : <strong>45</strong></li>
              </ul>
              <p><strong>Quel est le probl√®me et que faire ?</strong></p>
            `,
            options: [
              "Underfitting ‚Üí augmenter la complexit√© du mod√®le",
              "Overfitting ‚Üí r√©gulariser ou simplifier le mod√®le",
              "Le mod√®le est bon, l'√©cart est normal",
              "Probl√®me de donn√©es ‚Üí collecter plus de donn√©es de test",
            ],
            correctAnswer: 1,
            explanation: "MSE train (5) tr√®s inf√©rieur au MSE test (45) = overfitting classique. Le mod√®le a m√©moris√© l'entra√Ænement. Solutions : r√©gularisation L2/L1, r√©duire le nombre de features, ajouter plus de donn√©es d'entra√Ænement.",
          },
          // Section 8: R√©capitulatif
          {
            type: "warning",
            icon: "üéØ",
            title: "R√©capitulatif : la r√©gression lin√©aire en 5 points",
            content: `
              <p><strong>1Ô∏è‚É£ Objectif</strong></p>
              <p>Pr√©dire une valeur continue y √† partir de features X avec y = Xw + b</p>

              <p><strong>2Ô∏è‚É£ Fonction de co√ªt</strong></p>
              <p>MSE = moyenne des erreurs au carr√©. On minimise pour trouver w et b optimaux.</p>

              <p><strong>3Ô∏è‚É£ Solution analytique</strong></p>
              <p>w* = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y ‚Äî formule exacte, calcul direct.</p>

              <p><strong>4Ô∏è‚É£ Interpr√©tation</strong></p>
              <p>Chaque coefficient w<sub>i</sub> indique l'impact de la feature i sur la pr√©diction.</p>

              <p><strong>5Ô∏è‚É£ R√©gularisation</strong></p>
              <p>Ridge (L2) et Lasso (L1) pour contr√¥ler la complexit√© et √©viter l'overfitting.</p>

              <p style="background: #e3f2fd; padding: 1rem; border-radius: 8px; margin-top: 1rem;">
                <strong>üí° √Ä retenir :</strong> La r√©gression lin√©aire est la base de tout le ML. Ma√Ætrisez-la avant d'aller plus loin !
              </p>
            `,
          },
          {
            type: "quiz",
            title: "Quiz final - R√©gression Lin√©aire",
            question: "Vous entra√Ænez une r√©gression lin√©aire et obtenez R¬≤ = 0.95 sur le train et R¬≤ = 0.92 sur le test. Que concluez-vous ?",
            options: [
              "Le mod√®le est en overfitting s√©v√®re",
              "Le mod√®le est excellent et g√©n√©ralise bien",
              "Le mod√®le est en underfitting",
              "Il faut ajouter plus de features",
            ],
            correctAnswer: 1,
            explanation: "R¬≤ = 0.95 (train) et 0.92 (test) sont tr√®s proches et √©lev√©s. Le mod√®le explique ~93% de la variance et g√©n√©ralise bien (faible √©cart train/test). C'est un excellent mod√®le !",
          },
        ],
        prevModule: "introduction.html",
        nextModule: "classification.html",
      };

      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
