<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Validation en Machine Learning | IA4Ndada</title>

    <!-- MathJax pour les formules mathÃ©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">ğŸ  Accueil</a>
          <span>â€º</span>
          <span>ğŸ¤– Machine Learning</span>
          <span>â€º</span>
          <span>Validation</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>âœ… Validation en Machine Learning</h1>
      <p class="subtitle">Module 3.5 - L'art d'Ã©valuer un modÃ¨le</p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>ğŸ¯ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajoutÃ©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajoutÃ© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>ğŸ‰ Checkpoint - Validation</h3>
        <p>
          FÃ©licitations ! Vous savez maintenant Ã©valuer rigoureusement vos
          modÃ¨les ML.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme complÃ©tÃ©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="clustering.html" class="nav-link" id="prev-link"
          >â† Module prÃ©cÃ©dent : Clustering</a
        >
        <a href="../dl/perceptron.html" class="nav-link" id="next-link"
          >Module suivant : Perceptron â†’</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module Validation
      const moduleConfig = {
        id: "ml-validation",
        title: "Validation en Machine Learning",
        category: "Machine Learning",
        objectives: [
          "Comprendre pourquoi la validation est critique",
          "MaÃ®triser le split train/validation/test",
          "DÃ©couvrir la validation croisÃ©e et ses variantes",
          "Apprendre Ã  dÃ©tecter overfitting et underfitting",
        ],
        content: [
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Le piÃ¨ge fondamental du Machine Learning",
            content: `
                        <p>Le Machine Learning cache un <strong>piÃ¨ge dangereux</strong> : un modÃ¨le peut avoir <strong>100% de prÃ©cision</strong> sur vos donnÃ©es... et Ãªtre <strong>complÃ¨tement inutile</strong> en production !</p>
                        
                        <p><strong>ğŸ”‘ Le paradoxe :</strong></p>
                        <p>Plus un modÃ¨le est complexe, plus il peut <strong>mÃ©moriser</strong> les donnÃ©es au lieu de <strong>comprendre</strong> les patterns.</p>
                        
                        <p><strong>ğŸ¯ L'objectif ultime :</strong></p>
                        <p>CrÃ©er un modÃ¨le qui <strong>gÃ©nÃ©ralise</strong> bien sur des donnÃ©es qu'il n'a <strong>jamais vues</strong>.</p>
                        
                        <p><strong>ğŸ’¡ La validation est l'outil qui nous permet de :</strong></p>
                        <ul>
                            <li>âœ… Mesurer la vraie performance</li>
                            <li>ğŸ” DÃ©tecter les problÃ¨mes cachÃ©s</li>
                            <li>âš–ï¸ Ã‰quilibrer complexitÃ© et gÃ©nÃ©ralisation</li>
                            <li>ğŸ¯ Choisir le meilleur modÃ¨le</li>
                        </ul>
                    `,
          },
          {
            type: "intuition",
            icon: "ğŸ§ ",
            title: "L'analogie de l'examen scolaire",
            content: `
                        <p>Imaginez trois types d'Ã©tudiants face Ã  un examen de mathÃ©matiques :</p>
                        
                        <p><strong>ğŸ“š L'Ã©tudiant "Overfitting" (surapprentissage) :</strong></p>
                        <ul>
                            <li>ğŸ”– MÃ©morise toutes les solutions des exercices du livre</li>
                            <li>ğŸ’¯ RÃ©sout parfaitement les exercices qu'il a dÃ©jÃ  vus</li>
                            <li>âŒ Ã‰choue face Ã  un nouvel exercice lÃ©gÃ¨rement diffÃ©rent</li>
                        </ul>
                        
                        <p><strong>ğŸ˜´ L'Ã©tudiant "Underfitting" (sous-apprentissage) :</strong></p>
                        <ul>
                            <li>ğŸ“– N'a pas assez Ã©tudiÃ© ou compris les concepts</li>
                            <li>âŒ Ã‰choue mÃªme sur les exercices simples</li>
                            <li>ğŸ¤· ModÃ¨le trop simple pour le problÃ¨me</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ L'Ã©tudiant "Optimal" :</strong></p>
                        <ul>
                            <li>ğŸ§  Comprend les concepts fondamentaux</li>
                            <li>âœ… RÃ©sout les exercices connus ET inconnus</li>
                            <li>ğŸ”„ Sait adapter ses connaissances Ã  de nouvelles situations</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ C'est exactement ce qu'on cherche en ML : un modÃ¨le qui comprend, pas qui mÃ©morise !</strong></p>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Train/Val/Test : la trinitÃ© sacrÃ©e",
            content: `
                        <p>La division des donnÃ©es en <strong>trois ensembles distincts</strong> est fondamentale :</p>
                        
                        <p><strong>ğŸ“Š La rÃ¨gle d'or :</strong></p>
                        <p>$$\\text{DonnÃ©es} = \\text{Train} \\cup \\text{Validation} \\cup \\text{Test}$$</p>
                        <p>avec $$\\text{Train} \\cap \\text{Validation} \\cap \\text{Test} = \\emptyset$$</p>
                        
                        <p><strong>ğŸ¯ Proportions classiques :</strong></p>
                        <ul>
                            <li>ğŸ“š <strong>Train (60-70%)</strong> : apprentissage des paramÃ¨tres \\(\\theta\\)</li>
                            <li>ğŸ” <strong>Validation (15-20%)</strong> : choix des hyperparamÃ¨tres</li>
                            <li>ğŸ¯ <strong>Test (15-20%)</strong> : Ã©valuation finale non biaisÃ©e</li>
                        </ul>
                        
                        <p><strong>âš ï¸ Le principe fondamental :</strong></p>
                        <p>Le test set est <strong>sacrÃ©</strong> - on ne le touche qu'<strong>UNE SEULE FOIS</strong> Ã  la toute fin !</p>
                        
                        <p><strong>ğŸ“ MathÃ©matiquement :</strong></p>
                        <p>Si \\(\\mathcal{L}_{train}\\) est l'erreur sur train et \\(\\mathcal{L}_{test}\\) sur test :</p>
                        <ul>
                            <li>\\(\\mathcal{L}_{train} \\ll \\mathcal{L}_{test}\\) â†’ <strong>Overfitting</strong></li>
                            <li>\\(\\mathcal{L}_{train} \\approx \\mathcal{L}_{test}\\) (Ã©levÃ©es) â†’ <strong>Underfitting</strong></li>
                            <li>\\(\\mathcal{L}_{train} \\approx \\mathcal{L}_{test}\\) (faibles) â†’ <strong>Optimal !</strong></li>
                        </ul>
                    `,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Validation croisÃ©e : utiliser 100% des donnÃ©es",
            content: `
                        <p>La <strong>validation croisÃ©e k-fold</strong> est une technique Ã©lÃ©gante qui utilise <strong>toutes les donnÃ©es</strong> pour l'entraÃ®nement ET la validation !</p>
                        
                        <p><strong>ğŸ”„ Le principe :</strong></p>
                        <p>Diviser les donnÃ©es en k parties Ã©gales, puis faire k expÃ©riences oÃ¹ chaque partie sert une fois de validation.</p>
                        
                        <p><strong>ğŸ¯ Algorithme Ã©tape par Ã©tape :</strong></p>
                        <ol>
                            <li>Diviser les donnÃ©es en k "folds" (plis)</li>
                            <li>Pour chaque fold i :
                                <ul>
                                    <li>Fold i â†’ validation</li>
                                    <li>Autres folds â†’ entraÃ®nement</li>
                                    <li>EntraÃ®ner et calculer l'erreur \\(e_i\\)</li>
                                </ul>
                            </li>
                            <li>Score final = moyenne des k erreurs</li>
                        </ol>
                        
                        <p><strong>ğŸ’¡ Avantages clÃ©s :</strong></p>
                        <ul>
                            <li>âœ… Utilise 100% des donnÃ©es</li>
                            <li>ğŸ“Š Donne une estimation robuste</li>
                            <li>ğŸ“ˆ Mesure la variance du modÃ¨le</li>
                        </ul>
                        
                        <p><strong>ğŸ¤” Choix de k :</strong></p>
                        <ul>
                            <li><strong>k=5 ou 10</strong> : bon compromis temps/variance</li>
                            <li><strong>k=n (LOOCV)</strong> : prÃ©cis mais coÃ»teux</li>
                            <li><strong>k=2</strong> : rapide mais variance Ã©levÃ©e</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Le dilemme biais-variance",
            content: `
                        <p>L'erreur totale d'un modÃ¨le se <strong>dÃ©compose mathÃ©matiquement</strong> en trois parties :</p>
                        
                        <p><strong>ğŸ“ La dÃ©composition fondamentale :</strong></p>
                        <p>$$\\text{Erreur Totale} = \\text{Biais}^2 + \\text{Variance} + \\text{Bruit}$$</p>
                        
                        <p><strong>ğŸ¯ Biais (Bias) :</strong></p>
                        <p>Erreur due aux <strong>hypothÃ¨ses simplificatrices</strong> du modÃ¨le</p>
                        <ul>
                            <li>ModÃ¨le trop simple pour capturer la complexitÃ©</li>
                            <li>Exemple : utiliser une droite pour des donnÃ©es en forme de parabole</li>
                            <li>Cause l'<strong>underfitting</strong></li>
                        </ul>
                        
                        <p><strong>ğŸ“Š Variance :</strong></p>
                        <p>SensibilitÃ© du modÃ¨le aux <strong>fluctuations</strong> dans les donnÃ©es d'entraÃ®nement</p>
                        <ul>
                            <li>ModÃ¨le trop complexe qui "colle" trop aux donnÃ©es</li>
                            <li>Change beaucoup avec diffÃ©rents Ã©chantillons</li>
                            <li>Cause l'<strong>overfitting</strong></li>
                        </ul>
                        
                        <p><strong>ğŸ² Bruit irrÃ©ductible :</strong></p>
                        <p>Erreur intrinsÃ¨que aux donnÃ©es (mesures imprÃ©cises, alÃ©atoire)</p>
                        
                        <p><strong>âš–ï¸ Le trade-off :</strong></p>
                        <p>Diminuer le biais augmente souvent la variance (et vice-versa). L'art est de trouver l'Ã©quilibre optimal !</p>
                    `,
          },
          {
            type: "intuition",
            icon: "ğŸ§ ",
            title: "DÃ©tecter overfitting et underfitting",
            content: `
                        <p><strong>ğŸ” Comment reconnaÃ®tre chaque problÃ¨me ?</strong></p>
                        
                        <p><strong>ğŸ”´ Signes d'OVERFITTING :</strong></p>
                        <ul>
                            <li>ğŸ“ˆ Erreur train trÃ¨s faible (< 5%)</li>
                            <li>ğŸ“‰ Erreur validation beaucoup plus Ã©levÃ©e (> 20%)</li>
                            <li>ğŸ“Š Grand Ã©cart train/validation qui augmente</li>
                            <li>ğŸ¢ Courbe d'apprentissage qui diverge</li>
                        </ul>
                        
                        <p><strong>ğŸŸ¡ Signes d'UNDERFITTING :</strong></p>
                        <ul>
                            <li>ğŸ“Š Erreurs train ET validation Ã©levÃ©es</li>
                            <li>ğŸ“ˆ Peu de diffÃ©rence train/validation</li>
                            <li>ğŸ“‰ Performance mÃ©diocre mÃªme sur train</li>
                            <li>ğŸ”„ Courbes qui stagnent rapidement</li>
                        </ul>
                        
                        <p><strong>ğŸŸ¢ Signes d'un modÃ¨le OPTIMAL :</strong></p>
                        <ul>
                            <li>âœ… Erreurs train et validation faibles</li>
                            <li>ğŸ“Š Petit Ã©cart train/validation (< 5%)</li>
                            <li>ğŸ“ˆ Courbes qui convergent</li>
                            <li>ğŸ¯ Performance stable sur nouveaux donnÃ©es</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ RÃ¨gle pratique :</strong></p>
                        <p>Si erreur_validation / erreur_train > 1.5 â†’ probable overfitting !</p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Validation pratique : prÃ©diction de revenus",
            description:
              "DÃ©tectons l'overfitting avec une validation rigoureuse :",
            code: `import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# DonnÃ©es simulÃ©es : expÃ©rience vs salaire
np.random.seed(42)
n_samples = 100

# GÃ©nÃ©ration rÃ©aliste
experience = np.random.uniform(0, 20, n_samples)  # AnnÃ©es d'expÃ©rience
salaire_base = 25000  # Salaire de base (FCFA/mois Ã— 1000)
salaire = salaire_base + 3000 * experience + 150 * experience**2 - 5 * experience**3
salaire += np.random.normal(0, 5000, n_samples)  # Bruit

X = experience.reshape(-1, 1)
y = salaire

print("=" * 60)
print("ğŸ’¼ VALIDATION : PRÃ‰DICTION DE SALAIRES")
print("=" * 60)

# 1. SPLIT DONNÃ‰ES
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.4, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

print(f"\\nğŸ“Š Division des donnÃ©es :")
print(f"   Train : {len(X_train)} Ã©chantillons")
print(f"   Validation : {len(X_val)} Ã©chantillons")
print(f"   Test : {len(X_test)} Ã©chantillons")

# 2. TEST DIFFÃ‰RENTS DEGRÃ‰S (complexitÃ© du modÃ¨le)
degrees = range(1, 10)
train_scores = []
val_scores = []

print(f"\\nğŸ” Test de complexitÃ© (degrÃ© polynomial) :")
print("-" * 40)

for degree in degrees:
    # Transformation polynomiale
    poly = PolynomialFeatures(degree)
    X_train_poly = poly.fit_transform(X_train)
    X_val_poly = poly.transform(X_val)
    
    # ModÃ¨le
    model = Ridge(alpha=0.01)
    model.fit(X_train_poly, y_train)
    
    # Scores RÂ²
    train_score = model.score(X_train_poly, y_train)
    val_score = model.score(X_val_poly, y_val)
    
    train_scores.append(train_score)
    val_scores.append(val_score)
    
    # DÃ©tection overfitting
    ecart = train_score - val_score
    status = "âš ï¸ OVERFITTING" if ecart > 0.15 else "âœ… OK"
    
    print(f"DegrÃ© {degree}: Train={train_score:.3f}, Val={val_score:.3f} {status}")

# Meilleur degrÃ©
best_degree = list(degrees)[np.argmax(val_scores)]
print(f"\\nğŸ† Meilleur degrÃ© : {best_degree}")

# 3. VALIDATION CROISÃ‰E avec le meilleur degrÃ©
print(f"\\nğŸ”„ Validation croisÃ©e 5-fold (degrÃ© {best_degree}) :")
print("-" * 40)

poly_best = PolynomialFeatures(best_degree)
X_all_poly = poly_best.fit_transform(np.vstack([X_train, X_val]))
y_all = np.concatenate([y_train, y_val])

model_cv = Ridge(alpha=0.01)
cv_scores = cross_val_score(model_cv, X_all_poly, y_all, cv=5, 
                            scoring='r2')

for i, score in enumerate(cv_scores, 1):
    print(f"Fold {i}: RÂ² = {score:.3f}")

print(f"\\nğŸ“Š Score moyen : {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")

# 4. Ã‰VALUATION FINALE
print(f"\\nğŸ¯ Ã‰VALUATION FINALE sur TEST SET :")
print("=" * 60)

X_test_poly = poly_best.transform(X_test)
model_final = Ridge(alpha=0.01)
model_final.fit(X_all_poly, y_all)
y_pred = model_final.predict(X_test_poly)

test_r2 = r2_score(y_test, y_pred)
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"RÂ² Test : {test_r2:.3f}")
print(f"RMSE Test : {test_rmse:,.0f} FCFA")

# Visualisation
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Courbes de validation
ax1.plot(degrees, train_scores, 'b-o', label='Train', linewidth=2)
ax1.plot(degrees, val_scores, 'r-s', label='Validation', linewidth=2)
ax1.axvline(best_degree, color='g', linestyle='--', alpha=0.5)
ax1.set_xlabel('DegrÃ© polynomial')
ax1.set_ylabel('Score RÂ²')
ax1.set_title('DÃ©tection Overfitting/Underfitting')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Zone d'overfitting
overfitting_zone = [i for i, (t, v) in enumerate(zip(train_scores, val_scores)) 
                    if t - v > 0.15]
if overfitting_zone:
    ax1.axvspan(min(overfitting_zone) + 0.5, max(overfitting_zone) + 1.5, 
                alpha=0.2, color='red', label='Zone Overfitting')

# PrÃ©dictions vs RÃ©el
ax2.scatter(y_test, y_pred, alpha=0.6, s=50)
ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
         'r--', linewidth=2)
ax2.set_xlabel('Salaire rÃ©el (FCFA)')
ax2.set_ylabel('Salaire prÃ©dit (FCFA)')
ax2.set_title(f'PrÃ©dictions Test (RÂ²={test_r2:.3f})')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\\nğŸ’¡ Conclusion :")
if test_r2 > 0.85:
    print("   âœ… ModÃ¨le performant et bien gÃ©nÃ©ralisÃ© !")
elif test_r2 > 0.70:
    print("   ğŸŸ¡ ModÃ¨le correct mais peut Ãªtre amÃ©liorÃ©")
else:
    print("   ğŸ”´ ModÃ¨le insuffisant, revoir l'approche")`,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "MÃ©triques d'Ã©valuation : choisir les bonnes",
            content: `
                        <p><strong>ğŸ“Š Pour la RÃ‰GRESSION :</strong></p>
                        
                        <p><strong>MSE et RMSE :</strong></p>
                        <ul>
                            <li>MSE = moyenne des erreurs au carrÃ©</li>
                            <li>RMSE = \\(\\sqrt{MSE}\\) (mÃªme unitÃ© que y)</li>
                            <li>PÃ©nalise fortement les grandes erreurs</li>
                        </ul>
                        
                        <p><strong>MAE (Mean Absolute Error) :</strong></p>
                        <ul>
                            <li>Moyenne des erreurs absolues</li>
                            <li>Robuste aux outliers</li>
                            <li>Plus interprÃ©table</li>
                        </ul>
                        
                        <p><strong>RÂ² (coefficient de dÃ©termination) :</strong></p>
                        <ul>
                            <li>Proportion de variance expliquÃ©e</li>
                            <li>RÂ²=1 : parfait, RÂ²=0 : nul</li>
                            <li>Peut Ãªtre nÃ©gatif si trÃ¨s mauvais !</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Pour la CLASSIFICATION :</strong></p>
                        
                        <p><strong>Accuracy :</strong></p>
                        <ul>
                            <li>% de prÃ©dictions correctes</li>
                            <li>âš ï¸ Trompeur si classes dÃ©sÃ©quilibrÃ©es</li>
                        </ul>
                        
                        <p><strong>Precision & Recall :</strong></p>
                        <ul>
                            <li>Precision : parmi les positifs prÃ©dits, combien sont vrais ?</li>
                            <li>Recall : parmi les vrais positifs, combien sont dÃ©tectÃ©s ?</li>
                        </ul>
                        
                        <p><strong>F1-Score :</strong></p>
                        <ul>
                            <li>Moyenne harmonique precision/recall</li>
                            <li>Ã‰quilibre les deux mÃ©triques</li>
                        </ul>
                    `,
          },
          {
            type: "intuition",
            icon: "ğŸ§ ",
            title: "Validation temporelle : le cas particulier",
            content: `
                        <p><strong>â° Les donnÃ©es temporelles ont leurs propres rÃ¨gles !</strong></p>
                        
                        <p><strong>âŒ L'erreur fatale :</strong></p>
                        <p>Utiliser des donnÃ©es du <strong>futur</strong> pour prÃ©dire le <strong>passÃ©</strong> = triche involontaire !</p>
                        
                        <p><strong>âœ… La bonne approche :</strong></p>
                        <ul>
                            <li>ğŸ“… <strong>Train</strong> : donnÃ©es historiques (ex: jan-juin)</li>
                            <li>ğŸ” <strong>Validation</strong> : pÃ©riode suivante (juillet-aoÃ»t)</li>
                            <li>ğŸ¯ <strong>Test</strong> : futur proche (septembre-octobre)</li>
                        </ul>
                        
                        <p><strong>ğŸ”„ Techniques spÃ©ciales :</strong></p>
                        <ul>
                            <li><strong>Walk-forward</strong> : fenÃªtre glissante dans le temps</li>
                            <li><strong>Expanding window</strong> : train grandit progressivement</li>
                            <li><strong>Time series split</strong> : k-fold adaptÃ© au temporel</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ RÃ¨gle d'or :</strong></p>
                        <p>Toujours respecter l'ordre chronologique : passÃ© â†’ prÃ©sent â†’ futur !</p>
                    `,
          },
          {
            type: "warning",
            icon: "âš ï¸",
            title: "PiÃ¨ges et bonnes pratiques",
            content: `
                        <p><strong>âŒ PIÃˆGES COURANTS :</strong></p>
                        
                        <p><strong>1. Data Leakage (fuite de donnÃ©es) :</strong></p>
                        <ul>
                            <li>Normaliser sur tout le dataset</li>
                            <li>SÃ©lection de features sur test set</li>
                            <li>Information du futur dans le passÃ©</li>
                        </ul>
                        
                        <p><strong>2. Mauvaise validation :</strong></p>
                        <ul>
                            <li>Test set trop petit (< 15%)</li>
                            <li>Pas de stratification sur classes rares</li>
                            <li>Optimiser sur le test set</li>
                        </ul>
                        
                        <p><strong>3. MÃ©triques inappropriÃ©es :</strong></p>
                        <ul>
                            <li>Accuracy seule sur donnÃ©es dÃ©sÃ©quilibrÃ©es</li>
                            <li>Ignorer le contexte mÃ©tier</li>
                            <li>Se fier Ã  une seule mÃ©trique</li>
                        </ul>
                        
                        <p><strong>âœ… BONNES PRATIQUES :</strong></p>
                        
                        <p><strong>1. Pipeline rigoureux :</strong></p>
                        <ol>
                            <li>SÃ©parer test set dÃ¨s le dÃ©but</li>
                            <li>Preprocessing dans le pipeline</li>
                            <li>Cross-validation pour hyperparamÃ¨tres</li>
                            <li>Test final une seule fois</li>
                        </ol>
                        
                        <p><strong>2. Validation adaptÃ©e :</strong></p>
                        <ul>
                            <li>K-fold avec k=5 ou 10</li>
                            <li>Stratification si nÃ©cessaire</li>
                            <li>Temporal split pour sÃ©ries</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Conseil d'or :</strong></p>
                        <p>Un modÃ¨le simple bien validÃ© > modÃ¨le complexe mal Ã©valuÃ© !</p>
                        
                        <p><strong>ğŸ”® Prochaine Ã©tape :</strong></p>
                        <p>Maintenant que vous savez valider, passons au Deep Learning avec le <strong>Perceptron</strong> !</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ğŸ¤” Votre modÃ¨le a 2% d'erreur sur train et 25% sur validation. Diagnostic ?",
          options: [
            "A) Underfitting - modÃ¨le trop simple",
            "B) Overfitting - modÃ¨le mÃ©morise les donnÃ©es",
            "C) DonnÃ©es insuffisantes",
            "D) ModÃ¨le optimal",
          ],
          correct: 1,
          explanation:
            "Grand Ã©cart train (2%) vs validation (25%) = overfitting classique ! Le modÃ¨le a mÃ©morisÃ© les donnÃ©es d'entraÃ®nement au lieu d'apprendre les patterns gÃ©nÃ©ralisables. Solutions : rÃ©gularisation, dropout, ou modÃ¨le plus simple.",
        },
        prevModule: "clustering.html",
        nextModule: "../dl/perceptron.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
