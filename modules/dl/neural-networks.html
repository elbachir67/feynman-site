<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>RÃ©seaux de Neurones | IA4Ndada</title>

    <!-- MathJax pour les formules mathÃ©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">ğŸ  Accueil</a>
          <span>â€º</span>
          <span>ğŸ§  Deep Learning</span>
          <span>â€º</span>
          <span>RÃ©seaux de Neurones</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>ğŸ§  RÃ©seaux de Neurones : L'Architecture de l'Intelligence</h1>
      <p class="subtitle">Module 4.2 - Deep Learning Fondamental</p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>ğŸ¯ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajoutÃ©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajoutÃ© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>ğŸ‰ Checkpoint - RÃ©seaux de Neurones</h3>
        <p>
          FÃ©licitations ! Vous comprenez maintenant l'architecture qui a
          rÃ©volutionnÃ© l'IA moderne.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme complÃ©tÃ©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="perceptron.html" class="nav-link" id="prev-link"
          >â† Module prÃ©cÃ©dent : Perceptron</a
        >
        <a href="backpropagation.html" class="nav-link" id="next-link"
          >Module suivant : Backpropagation â†’</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module RÃ©seaux de Neurones
      const moduleConfig = {
        id: "dl-neural-networks",
        title: "RÃ©seaux de Neurones : L'Architecture de l'Intelligence",
        category: "Deep Learning",
        objectives: [
          "Comprendre l'Ã©volution du perceptron vers les rÃ©seaux multicouches",
          "MaÃ®triser l'architecture et la propagation avant",
          "Calculer manuellement les sorties d'un rÃ©seau simple",
          "Comprendre les fonctions d'activation et leur rÃ´le crucial",
          "ImplÃ©menter un rÃ©seau de neurones from scratch",
        ],
        content: [
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "L'Ã©volution rÃ©volutionnaire : du perceptron aux rÃ©seaux",
            content: `
                        <p>Les <strong>rÃ©seaux de neurones multicouches</strong> reprÃ©sentent le saut quantique qui a permis Ã  l'IA de rÃ©soudre des problÃ¨mes impossibles pour un simple perceptron.</p>
                        
                        <p><strong>ğŸ”‘ RÃ©volution conceptuelle :</strong></p>
                        <ul>
                            <li>ğŸ§  <strong>Perceptron</strong> : 1 neurone = 1 dÃ©cision simple (ET, OU)</li>
                            <li>ğŸŒ <strong>RÃ©seau multicouche</strong> : millions de neurones = intelligence complexe</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ ProblÃ¨mes rÃ©solus :</strong></p>
                        <ul>
                            <li>âŒ <strong>XOR impossible</strong> â†’ âœ… <strong>Toute fonction boolÃ©enne</strong></li>
                            <li>âŒ <strong>SÃ©paration linÃ©aire</strong> â†’ âœ… <strong>FrontiÃ¨res complexes</strong></li>
                            <li>âŒ <strong>ProblÃ¨mes simples</strong> â†’ âœ… <strong>Reconnaissance d'images, langage, jeux</strong></li>
                        </ul>
                        
                        <p><strong>ğŸš€ Applications rÃ©volutionnaires :</strong></p>
                        <ul>
                            <li>ğŸ–¼ï¸ <strong>Vision</strong> : reconnaissance faciale, diagnostic mÃ©dical</li>
                            <li>ğŸ’¬ <strong>Langage</strong> : traduction, ChatGPT, assistants vocaux</li>
                            <li>ğŸ® <strong>Jeux</strong> : AlphaGo, Ã©checs, poker</li>
                            <li>ğŸš— <strong>Autonomie</strong> : voitures, drones, robots</li>
                            <li>ğŸ¨ <strong>CrÃ©ativitÃ©</strong> : art, musique, Ã©criture</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Point clÃ© :</strong> Les rÃ©seaux de neurones ne sont pas juste "plusieurs perceptrons" - ils crÃ©ent une <strong>hiÃ©rarchie de concepts</strong> qui permet l'apprentissage de reprÃ©sentations complexes.</p>
                    `,
          },
          {
            type: "intuition",
            icon: "ğŸ§ ",
            title: "L'analogie de l'Ã©quipe mÃ©dicale sÃ©nÃ©galaise",
            content: `
                        <p>Imaginez le <strong>diagnostic mÃ©dical Ã  l'hÃ´pital Aristide Le Dantec</strong> Ã  Dakar :</p>
                        
                        <p><strong>ğŸ¥ Ã‰quipe mÃ©dicale hiÃ©rarchisÃ©e :</strong></p>
                        <ul>
                            <li>ğŸ‘©â€âš•ï¸ <strong>InfirmiÃ¨res</strong> : observent les symptÃ´mes de base (fiÃ¨vre, tension)</li>
                            <li>ğŸ‘¨â€âš•ï¸ <strong>MÃ©decins gÃ©nÃ©ralistes</strong> : combinent les observations (syndrome grippal ?)</li>
                            <li>ğŸ”¬ <strong>SpÃ©cialistes</strong> : analyses complexes (cardiologue, neurologue)</li>
                            <li>ğŸ‘¨â€âš•ï¸ <strong>Chef de service</strong> : diagnostic final et traitement</li>
                        </ul>
                        
                        <p><strong>ğŸ§  ParallÃ¨le avec rÃ©seau de neurones :</strong></p>
                        <ul>
                            <li>ğŸ“Š <strong>Couche d'entrÃ©e</strong> : infirmiÃ¨res (donnÃ©es brutes)</li>
                            <li>ğŸ” <strong>Couches cachÃ©es</strong> : mÃ©decins (extraction de caractÃ©ristiques)</li>
                            <li>ğŸ¯ <strong>Couche de sortie</strong> : chef de service (dÃ©cision finale)</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Processus hiÃ©rarchique :</strong></p>
                        <ol>
                            <li>ğŸ“‹ <strong>Niveau 1</strong> : "TempÃ©rature 39Â°C, pouls rapide"</li>
                            <li>ğŸ” <strong>Niveau 2</strong> : "Syndrome infectieux probable"</li>
                            <li>ğŸ¯ <strong>Niveau 3</strong> : "Paludisme : 85% de probabilitÃ©"</li>
                        </ol>
                        
                        <p><strong>ğŸŒŸ Chaque niveau ajoute de l'intelligence :</strong></p>
                        <ul>
                            <li>ğŸ”¢ <strong>DonnÃ©es â†’ SymptÃ´mes â†’ Syndromes â†’ Diagnostic</strong></li>
                            <li>ğŸ§  <strong>Simple â†’ Complexe â†’ Abstrait â†’ DÃ©cision</strong></li>
                        </ul>
                        
                        <p><strong>ğŸ¯ C'est exactement ce que fait un rÃ©seau de neurones :</strong> il construit une hiÃ©rarchie de concepts de plus en plus abstraits pour prendre des dÃ©cisions intelligentes !</p>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Architecture formelle : couches et connexions",
            content: `
                        <p><strong>ğŸ“ Formalisation mathÃ©matique d'un rÃ©seau multicouche :</strong></p>
                        
                        <p><strong>ğŸ—ï¸ Structure gÃ©nÃ©rale :</strong></p>
                        <ul style="list-style: none; padding-left: 0">
                            <li><strong>â€¢ Couche d'entrÃ©e :</strong> \\(\\vec{x} \\in \\mathbb{R}^{n_0}\\) (nâ‚€ features)</li>
                            <li style="margin-top: 0.5rem"><strong>â€¢ Couche cachÃ©e 1 :</strong> \\(\\vec{h}^{(1)} \\in \\mathbb{R}^{n_1}\\) (nâ‚ neurones)</li>
                            <li style="margin-top: 0.5rem"><strong>â€¢ Couche cachÃ©e 2 :</strong> \\(\\vec{h}^{(2)} \\in \\mathbb{R}^{n_2}\\) (nâ‚‚ neurones)</li>
                            <li style="margin-top: 0.5rem"><strong>â€¢ ... :</strong> autant de couches que nÃ©cessaire</li>
                            <li style="margin-top: 0.5rem"><strong>â€¢ Couche de sortie :</strong> \\(\\vec{y} \\in \\mathbb{R}^{n_L}\\) (nâ‚— sorties)</li>
                        </ul>
                        
                        <p><strong>ğŸ”— Propagation avant (Forward Pass) :</strong></p>
                        <p>Pour chaque couche \\(l\\) :</p>
                        <p>$$\\vec{z}^{(l)} = W^{(l)} \\vec{h}^{(l-1)} + \\vec{b}^{(l)}$$</p>
                        <p>$$\\vec{h}^{(l)} = \\sigma(\\vec{z}^{(l)})$$</p>
                        
                        <p><strong>ğŸ” DÃ©cryptage des formules :</strong></p>
                        <ul>
                            <li>\\(W^{(l)}\\) = <strong>matrice des poids</strong> (taille n_l Ã— n_{l-1})</li>
                            <li>\\(\\vec{b}^{(l)}\\) = <strong>vecteur des biais</strong> (taille n_l)</li>
                            <li>\\(\\vec{z}^{(l)}\\) = <strong>combinaison linÃ©aire</strong> (avant activation)</li>
                            <li>\\(\\sigma\\) = <strong>fonction d'activation</strong> (non-linÃ©aritÃ©)</li>
                            <li>\\(\\vec{h}^{(l)}\\) = <strong>sortie activÃ©e</strong> (aprÃ¨s non-linÃ©aritÃ©)</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Notation compacte :</strong></p>
                        <p>$$\\vec{y} = f_{\\theta}(\\vec{x}) = \\sigma_L(W^{(L)} \\sigma_{L-1}(W^{(L-1)} \\cdots \\sigma_1(W^{(1)} \\vec{x} + \\vec{b}^{(1)}) \\cdots + \\vec{b}^{(L-1)}) + \\vec{b}^{(L)})$$</p>
                        
                        <p><strong>ğŸ’¡ OÃ¹ \\(\\theta = \\{W^{(1)}, \\vec{b}^{(1)}, W^{(2)}, \\vec{b}^{(2)}, ..., W^{(L)}, \\vec{b}^{(L)}\\}\\) reprÃ©sente tous les paramÃ¨tres du rÃ©seau.</strong></p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Calcul manuel : rÃ©seau 2-3-1",
            content: `
                        <p><strong>ğŸ“ Exemple concret :</strong> RÃ©seau pour prÃ©dire le prix d'un terrain Ã  Dakar</p>
                        
                        <p><strong>ğŸ—ï¸ Architecture :</strong></p>
                        <ul>
                            <li>ğŸ“Š <strong>EntrÃ©e</strong> : [surface_mÂ², distance_centre] â†’ 2 neurones</li>
                            <li>ğŸ§  <strong>Couche cachÃ©e</strong> : 3 neurones (dÃ©tecteurs de patterns)</li>
                            <li>ğŸ¯ <strong>Sortie</strong> : prix_millions_fcfa â†’ 1 neurone</li>
                        </ul>
                        
                        <p><strong>âš™ï¸ ParamÃ¨tres du rÃ©seau :</strong></p>
                        <p>$$W^{(1)} = \\begin{bmatrix} 0.5 & 0.3 \\\\ -0.2 & 0.8 \\\\ 0.7 & -0.4 \\end{bmatrix}, \\quad \\vec{b}^{(1)} = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$$</p>
                        
                        <p>$$W^{(2)} = \\begin{bmatrix} 0.6 & -0.3 & 0.9 \\end{bmatrix}, \\quad b^{(2)} = 0.2$$</p>
                        
                        <p><strong>ğŸ“ Exemple :</strong> Terrain de 500mÂ² Ã  5km du centre</p>
                        <p>$$\\vec{x} = \\begin{bmatrix} 500 \\\\ 5 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ”¢ Calcul couche cachÃ©e :</strong></p>
                        <p>$$\\vec{z}^{(1)} = W^{(1)} \\vec{x} + \\vec{b}^{(1)} = \\begin{bmatrix} 0.5 \\times 500 + 0.3 \\times 5 + 0.1 \\\\ -0.2 \\times 500 + 0.8 \\times 5 - 0.2 \\\\ 0.7 \\times 500 - 0.4 \\times 5 + 0.3 \\end{bmatrix} = \\begin{bmatrix} 251.6 \\\\ -96.2 \\\\ 348.3 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ”„ Activation (sigmoid) :</strong></p>
                        <p>$$\\vec{h}^{(1)} = \\sigma(\\vec{z}^{(1)}) = \\begin{bmatrix} \\sigma(251.6) \\\\ \\sigma(-96.2) \\\\ \\sigma(348.3) \\end{bmatrix} â‰ˆ \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 1.0 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ¯ Sortie finale :</strong></p>
                        <p>$$z^{(2)} = W^{(2)} \\vec{h}^{(1)} + b^{(2)} = 0.6 \\times 1.0 - 0.3 \\times 0.0 + 0.9 \\times 1.0 + 0.2 = 1.7$$</p>
                        <p>$$y = \\sigma(1.7) â‰ˆ 0.85$$</p>
                        
                        <p><strong>ğŸ’° InterprÃ©tation :</strong> Le rÃ©seau prÃ©dit un prix de 0.85 Ã— Ã©chelle = ~85 millions FCFA pour ce terrain.</p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice pratique : calcul manuel complet",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>RÃ©seau 2-2-1 pour classifier des emails (spam/pas spam) :</p>
                        
                        <p><strong>âš™ï¸ ParamÃ¨tres :</strong></p>
                        <p>$$W^{(1)} = \\begin{bmatrix} 0.8 & -0.5 \\\\ -0.3 & 0.7 \\end{bmatrix}, \\quad \\vec{b}^{(1)} = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$$</p>
                        <p>$$W^{(2)} = \\begin{bmatrix} 1.2 & -0.8 \\end{bmatrix}, \\quad b^{(2)} = 0.3$$</p>
                        
                        <p><strong>ğŸ“§ Email Ã  analyser :</strong> [nb_mots_suspects=3, nb_liens=1]</p>
                        <p>$$\\vec{x} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ“ Calculez :</strong></p>
                        <ol>
                            <li>\\(\\vec{z}^{(1)}\\) (combinaisons linÃ©aires couche cachÃ©e)</li>
                            <li>\\(\\vec{h}^{(1)}\\) (activations couche cachÃ©e avec sigmoid)</li>
                            <li>\\(z^{(2)}\\) (combinaison linÃ©aire sortie)</li>
                            <li>\\(y\\) (probabilitÃ© finale que ce soit un spam)</li>
                            <li>Classification finale (spam si y > 0.5)</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('neural-network-manual-calculation')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="neural-network-manual-calculation" style="display: none;">
                        <ol>
                            <li><strong>Combinaisons linÃ©aires :</strong><br>
                                \\(z_1^{(1)} = 0.8 \\times 3 + (-0.5) \\times 1 + 0.2 = 2.4 - 0.5 + 0.2 = 2.1\\)<br>
                                \\(z_2^{(1)} = (-0.3) \\times 3 + 0.7 \\times 1 + (-0.1) = -0.9 + 0.7 - 0.1 = -0.3\\)<br>
                                Donc \\(\\vec{z}^{(1)} = \\begin{bmatrix} 2.1 \\\\ -0.3 \\end{bmatrix}\\)</li>
                            <li><strong>Activations sigmoid :</strong><br>
                                \\(h_1^{(1)} = \\sigma(2.1) = \\frac{1}{1 + e^{-2.1}} â‰ˆ 0.891\\)<br>
                                \\(h_2^{(1)} = \\sigma(-0.3) = \\frac{1}{1 + e^{0.3}} â‰ˆ 0.426\\)<br>
                                Donc \\(\\vec{h}^{(1)} = \\begin{bmatrix} 0.891 \\\\ 0.426 \\end{bmatrix}\\)</li>
                            <li><strong>Sortie linÃ©aire :</strong><br>
                                \\(z^{(2)} = 1.2 \\times 0.891 + (-0.8) \\times 0.426 + 0.3 = 1.069 - 0.341 + 0.3 = 1.028\\)</li>
                            <li><strong>ProbabilitÃ© finale :</strong><br>
                                \\(y = \\sigma(1.028) = \\frac{1}{1 + e^{-1.028}} â‰ˆ 0.736\\)</li>
                            <li><strong>Classification :</strong><br>
                                0.736 > 0.5 â†’ <strong>SPAM</strong> (73.6% de confiance)</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Fonctions d'activation : le secret de la non-linÃ©aritÃ©",
            content: `
                        <p><strong>ğŸ¤” Pourquoi les fonctions d'activation sont-elles cruciales ?</strong></p>
                        <p>Sans fonctions d'activation, un rÃ©seau de neurones multicouche serait Ã©quivalent Ã ... un simple perceptron ! Les activations introduisent la <strong>non-linÃ©aritÃ©</strong> qui permet l'apprentissage de patterns complexes.</p>
                        
                        <p><strong>âš ï¸ DÃ©monstration du problÃ¨me :</strong></p>
                        <p>Sans activation : \\(\\vec{y} = W^{(2)}(W^{(1)} \\vec{x} + \\vec{b}^{(1)}) + \\vec{b}^{(2)} = \\underbrace{W^{(2)}W^{(1)}}_{W_{Ã©quivalent}} \\vec{x} + \\underbrace{W^{(2)}\\vec{b}^{(1)} + \\vec{b}^{(2)}}_{\\vec{b}_{Ã©quivalent}}\\)</p>
                        <p>â†’ C'est juste une transformation linÃ©aire ! Aucun gain par rapport au perceptron.</p>
                        
                        <p><strong>ğŸ¯ Fonctions d'activation principales :</strong></p>
                        
                        <p><strong>1ï¸âƒ£ Sigmoid :</strong> \\(\\sigma(x) = \\frac{1}{1 + e^{-x}}\\)</p>
                        <ul>
                            <li>âœ… <strong>Avantages</strong> : sortie [0,1], interprÃ©table comme probabilitÃ©</li>
                            <li>âŒ <strong>ProblÃ¨mes</strong> : gradient qui disparaÃ®t, saturation</li>
                            <li>ğŸ¯ <strong>Usage</strong> : couche de sortie pour classification binaire</li>
                        </ul>
                        
                        <p><strong>2ï¸âƒ£ Tanh :</strong> \\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)</p>
                        <ul>
                            <li>âœ… <strong>Avantages</strong> : sortie [-1,1], centrÃ© sur 0</li>
                            <li>âŒ <strong>ProblÃ¨mes</strong> : gradient qui disparaÃ®t aussi</li>
                            <li>ğŸ¯ <strong>Usage</strong> : couches cachÃ©es (mieux que sigmoid)</li>
                        </ul>
                        
                        <p><strong>3ï¸âƒ£ ReLU :</strong> \\(\\text{ReLU}(x) = \\max(0, x)\\)</p>
                        <ul>
                            <li>âœ… <strong>Avantages</strong> : simple, pas de saturation, calcul rapide</li>
                            <li>âŒ <strong>ProblÃ¨mes</strong> : "neurones morts" (sortie toujours 0)</li>
                            <li>ğŸ¯ <strong>Usage</strong> : standard moderne pour couches cachÃ©es</li>
                        </ul>
                        
                        <p><strong>4ï¸âƒ£ Softmax :</strong> \\(\\text{softmax}(\\vec{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\\)</p>
                        <ul>
                            <li>âœ… <strong>Avantages</strong> : distribution de probabilitÃ© (somme = 1)</li>
                            <li>ğŸ¯ <strong>Usage</strong> : classification multi-classe</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "DÃ©rivÃ©es des fonctions d'activation",
            content: `
                        <p><strong>ğŸ”§ Pour l'entraÃ®nement, nous devons calculer les dÃ©rivÃ©es :</strong></p>
                        
                        <p><strong>ğŸ“ DÃ©rivÃ©e de Sigmoid :</strong></p>
                        <p>$$\\frac{d}{dx}\\sigma(x) = \\frac{d}{dx}\\left(\\frac{1}{1 + e^{-x}}\\right)$$</p>
                        
                        <p><strong>ğŸ” Calcul dÃ©taillÃ© :</strong></p>
                        <p>Posons \\(u = 1 + e^{-x}\\), alors \\(\\sigma(x) = u^{-1}\\)</p>
                        <p>$$\\frac{d\\sigma}{dx} = \\frac{d}{du}(u^{-1}) \\cdot \\frac{du}{dx} = -u^{-2} \\cdot (-e^{-x}) = \\frac{e^{-x}}{(1 + e^{-x})^2}$$</p>
                        
                        <p><strong>âœ¨ PropriÃ©tÃ© remarquable :</strong></p>
                        <p>$$\\sigma'(x) = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} = \\sigma(x)(1 - \\sigma(x))$$</p>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>ğŸ’¡ Formule magique :</strong> \\(\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\\)<br>
                            La dÃ©rivÃ©e s'exprime simplement en fonction de la valeur !
                        </div>
                        
                        <p><strong>ğŸ“ Autres dÃ©rivÃ©es importantes :</strong></p>
                        <ul style="list-style: none; padding-left: 0">
                            <li><strong>â€¢ Tanh :</strong> \\(\\tanh'(x) = 1 - \\tanh^2(x)\\)</li>
                            <li style="margin-top: 0.5rem"><strong>â€¢ ReLU :</strong> \\(\\text{ReLU}'(x) = \\begin{cases} 1 & \\text{si } x > 0 \\\\ 0 & \\text{si } x \\leq 0 \\end{cases}\\)</li>
                            <li style="margin-top: 0.5rem"><strong>â€¢ Leaky ReLU :</strong> \\(\\text{LeakyReLU}'(x) = \\begin{cases} 1 & \\text{si } x > 0 \\\\ 0.01 & \\text{si } x \\leq 0 \\end{cases}\\)</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Pourquoi importantes ?</strong></p>
                        <p>Ces dÃ©rivÃ©es sont utilisÃ©es dans l'algorithme de <strong>rÃ©tropropagation</strong> pour calculer comment ajuster les poids du rÃ©seau.</p>
                    `,
          },
          {
            type: "code",
            title: "ImplÃ©mentation rÃ©seau de neurones from scratch",
            description: "CrÃ©ons un rÃ©seau de neurones complet :",
            code: `import numpy as np
import matplotlib.pyplot as plt

class ReseauNeurones:
    def __init__(self, architecture):
        """
        Architecture: liste des tailles de couches
        Ex: [2, 3, 1] = 2 entrÃ©es, 3 neurones cachÃ©s, 1 sortie
        """
        self.architecture = architecture
        self.nb_couches = len(architecture)
        self.poids = {}
        self.biais = {}
        
        # Initialisation des poids (Xavier/Glorot)
        for i in range(1, self.nb_couches):
            taille_entree = architecture[i-1]
            taille_sortie = architecture[i]
            
            # Initialisation Xavier pour Ã©viter l'explosion/disparition des gradients
            limite = np.sqrt(6 / (taille_entree + taille_sortie))
            self.poids[i] = np.random.uniform(-limite, limite, (taille_sortie, taille_entree))
            self.biais[i] = np.zeros((taille_sortie, 1))
        
        print(f"ğŸ§  RÃ©seau crÃ©Ã©: {' â†’ '.join(map(str, architecture))}")
        print(f"ğŸ“Š ParamÃ¨tres totaux: {self.compter_parametres()}")
    
    def compter_parametres(self):
        """Compter le nombre total de paramÃ¨tres"""
        total = 0
        for i in range(1, self.nb_couches):
            total += self.poids[i].size + self.biais[i].size
        return total
    
    def sigmoid(self, x):
        """Fonction sigmoid avec protection contre overflow"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def sigmoid_derivee(self, x):
        """DÃ©rivÃ©e de sigmoid"""
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def propagation_avant(self, X):
        """Propagation avant Ã  travers le rÃ©seau"""
        self.activations = {0: X}  # Stocker toutes les activations
        self.z_values = {}  # Stocker les valeurs avant activation
        
        for i in range(1, self.nb_couches):
            # Combinaison linÃ©aire
            self.z_values[i] = self.poids[i] @ self.activations[i-1] + self.biais[i]
            
            # Activation
            if i == self.nb_couches - 1:  # DerniÃ¨re couche
                self.activations[i] = self.sigmoid(self.z_values[i])
            else:  # Couches cachÃ©es
                self.activations[i] = self.sigmoid(self.z_values[i])
        
        return self.activations[self.nb_couches - 1]
    
    def predire(self, X):
        """Faire des prÃ©dictions"""
        if X.ndim == 1:
            X = X.reshape(-1, 1)
        return self.propagation_avant(X)

# Test du rÃ©seau
print("ğŸ§  CRÃ‰ATION D'UN RÃ‰SEAU DE NEURONES")
print("=" * 40)

# Architecture pour classification de terrains
reseau = ReseauNeurones([2, 4, 3, 1])  # 2 entrÃ©es, 2 couches cachÃ©es, 1 sortie

# Test sur donnÃ©es de terrains dakarois
terrains_test = np.array([
    [500, 5],   # 500mÂ², 5km du centre
    [200, 2],   # 200mÂ², 2km du centre  
    [800, 8],   # 800mÂ², 8km du centre
    [300, 3]    # 300mÂ², 3km du centre
]).T

print("ğŸ  Test sur 4 terrains:")
for i, terrain in enumerate(terrains_test.T):
    prediction = reseau.predire(terrain)
    print(f"Terrain {i+1} ({terrain[0]}mÂ², {terrain[1]}km): {prediction[0,0]:.3f}")`,
          },
          {
            type: "code",
            title: "Visualisation de l'architecture",
            description: "Visualisons la structure de notre rÃ©seau :",
            code: `def visualiser_architecture(architecture):
    """Visualise l'architecture du rÃ©seau de neurones"""
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # ParamÃ¨tres de visualisation
    nb_couches = len(architecture)
    max_neurones = max(architecture)
    
    # Positions des couches
    x_positions = np.linspace(0, 10, nb_couches)
    
    # Couleurs par type de couche
    couleurs = ['lightblue', 'lightgreen', 'lightgreen', 'lightcoral']
    noms_couches = ['EntrÃ©e', 'CachÃ©e 1', 'CachÃ©e 2', 'Sortie']
    
    # Dessiner les neurones
    for i, (x_pos, nb_neurones) in enumerate(zip(x_positions, architecture)):
        # Positions verticales centrÃ©es
        if nb_neurones == 1:
            y_positions = [max_neurones / 2]
        else:
            y_positions = np.linspace(0, max_neurones, nb_neurones)
        
        # Dessiner les neurones
        for y_pos in y_positions:
            circle = plt.Circle((x_pos, y_pos), 0.3, 
                              color=couleurs[min(i, 3)], 
                              ec='black', linewidth=2)
            ax.add_patch(circle)
        
        # Labels des couches
        ax.text(x_pos, -1, f'{noms_couches[min(i, 3)]}\\n({nb_neurones})', 
               ha='center', va='top', fontweight='bold')
    
    # Dessiner les connexions (Ã©chantillon)
    for i in range(nb_couches - 1):
        x1, x2 = x_positions[i], x_positions[i+1]
        
        # Positions des neurones
        if architecture[i] == 1:
            y1_positions = [max_neurones / 2]
        else:
            y1_positions = np.linspace(0, max_neurones, architecture[i])
            
        if architecture[i+1] == 1:
            y2_positions = [max_neurones / 2]
        else:
            y2_positions = np.linspace(0, max_neurones, architecture[i+1])
        
        # Dessiner quelques connexions reprÃ©sentatives
        for y1 in y1_positions[::max(1, len(y1_positions)//3)]:
            for y2 in y2_positions[::max(1, len(y2_positions)//3)]:
                ax.plot([x1+0.3, x2-0.3], [y1, y2], 'gray', alpha=0.3, linewidth=1)
    
    ax.set_xlim(-1, 11)
    ax.set_ylim(-2, max_neurones + 1)
    ax.set_aspect('equal')
    ax.axis('off')
    ax.set_title('Architecture du RÃ©seau de Neurones [2-4-3-1]', fontsize=16, pad=20)
    
    plt.tight_layout()
    plt.show()

# Visualisation de notre rÃ©seau
visualiser_architecture([2, 4, 3, 1])
print("ğŸ¨ Architecture visualisÃ©e !")`,
          },
          {
            type: "code",
            title: "Comparaison des fonctions d'activation",
            description: "Comparons visuellement les diffÃ©rentes activations :",
            code: `# Comparaison des fonctions d'activation
x = np.linspace(-5, 5, 100)

# Calcul des fonctions
sigmoid_vals = 1 / (1 + np.exp(-x))
tanh_vals = np.tanh(x)
relu_vals = np.maximum(0, x)
leaky_relu_vals = np.where(x > 0, x, 0.01 * x)

# Calcul des dÃ©rivÃ©es
sigmoid_deriv = sigmoid_vals * (1 - sigmoid_vals)
tanh_deriv = 1 - tanh_vals**2
relu_deriv = np.where(x > 0, 1, 0)
leaky_relu_deriv = np.where(x > 0, 1, 0.01)

# Visualisation
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

# Fonctions d'activation
ax1.plot(x, sigmoid_vals, 'b-', label='Sigmoid', linewidth=2)
ax1.plot(x, tanh_vals, 'r-', label='Tanh', linewidth=2)
ax1.plot(x, relu_vals, 'g-', label='ReLU', linewidth=2)
ax1.plot(x, leaky_relu_vals, 'm-', label='Leaky ReLU', linewidth=2)
ax1.set_title('Fonctions d\'Activation')
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.set_xlabel('x')
ax1.set_ylabel('f(x)')

# DÃ©rivÃ©es
ax2.plot(x, sigmoid_deriv, 'b-', label="Sigmoid'", linewidth=2)
ax2.plot(x, tanh_deriv, 'r-', label="Tanh'", linewidth=2)
ax2.plot(x, relu_deriv, 'g-', label="ReLU'", linewidth=2)
ax2.plot(x, leaky_relu_deriv, 'm-', label="Leaky ReLU'", linewidth=2)
ax2.set_title('DÃ©rivÃ©es des Activations')
ax2.legend()
ax2.grid(True, alpha=0.3)
ax2.set_xlabel('x')
ax2.set_ylabel("f'(x)")

print("ğŸ“Š Comparaison des activations terminÃ©e !")`,
          },
          {
            type: "code",
            title: "ProblÃ¨me du gradient qui disparaÃ®t",
            description:
              "DÃ©monstrons le problÃ¨me des gradients qui disparaissent :",
            code: `# Simulation du problÃ¨me du gradient qui disparaÃ®t
def simuler_gradient_disparition():
    """Simule la propagation du gradient dans un rÃ©seau profond"""
    
    # RÃ©seau trÃ¨s profond avec sigmoid
    nb_couches = 10
    gradient_initial = 1.0
    gradients = [gradient_initial]
    
    print("âš ï¸ SIMULATION: GRADIENT QUI DISPARAÃT")
    print("=" * 45)
    print("RÃ©seau profond avec 10 couches sigmoid")
    print()
    
    # Simulation de la rÃ©tropropagation
    gradient_actuel = gradient_initial
    
    for couche in range(nb_couches, 0, -1):
        # DÃ©rivÃ©e sigmoid typique au centre (x=0) : Ïƒ'(0) = 0.25
        derivee_activation = 0.25
        
        # Poids typique (initialisÃ© autour de 0.5)
        poids_moyen = 0.5
        
        # Le gradient se multiplie par ces facteurs
        gradient_actuel *= derivee_activation * poids_moyen
        gradients.append(gradient_actuel)
        
        print(f"Couche {couche}: gradient = {gradient_actuel:.6f}")
        
        if gradient_actuel < 1e-10:
            print(f"ğŸ’€ Gradient mort Ã  la couche {couche} !")
            break
    
    return gradients

# Comparaison Sigmoid vs ReLU
def comparer_activations_gradient():
    """Compare l'impact des activations sur les gradients"""
    couches = list(range(1, 11))
    
    # Sigmoid: gradient Ã— 0.25 Ã— 0.5 = Ã— 0.125 par couche
    gradients_sigmoid = [1.0 * (0.125 ** i) for i in range(10)]
    
    # ReLU: gradient Ã— 1.0 Ã— 0.5 = Ã— 0.5 par couche  
    gradients_relu = [1.0 * (0.5 ** i) for i in range(10)]
    
    plt.figure(figsize=(12, 6))
    plt.semilogy(couches, gradients_sigmoid[1:], 'r-o', label='Sigmoid', linewidth=2)
    plt.semilogy(couches, gradients_relu[1:], 'g-o', label='ReLU', linewidth=2)
    plt.axhline(y=1e-6, color='black', linestyle='--', alpha=0.5, label='Seuil critique')
    
    plt.title('Disparition des Gradients selon l\'Activation')
    plt.xlabel('NumÃ©ro de couche (depuis la sortie)')
    plt.ylabel('Magnitude du gradient (Ã©chelle log)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    print("ğŸ“‰ ReLU prÃ©serve mieux les gradients que Sigmoid !")

# ExÃ©cution des simulations
gradients = simuler_gradient_disparition()
print()
comparer_activations_gradient()`,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : choix d'activation selon le problÃ¨me",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Pour chaque problÃ¨me, choisissez la meilleure fonction d'activation et justifiez :</p>
                        
                        <ol>
                            <li><strong>Classification binaire</strong> : email spam/pas spam</li>
                            <li><strong>RÃ©gression</strong> : prÃ©diction prix immobilier (toujours positif)</li>
                            <li><strong>Classification 5 classes</strong> : reconnaissance de chiffres 0-4</li>
                            <li><strong>Couches cachÃ©es</strong> : rÃ©seau profond (20 couches)</li>
                            <li><strong>PrÃ©diction tempÃ©rature</strong> : peut Ãªtre nÃ©gative ou positive</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('activation-choice-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="activation-choice-exercise" style="display: none;">
                        <ol>
                            <li><strong>Classification binaire :</strong> <strong>Sigmoid</strong> en sortie<br>
                                <em>Justification :</em> Sortie [0,1] interprÃ©table comme probabilitÃ©</li>
                            <li><strong>RÃ©gression positive :</strong> <strong>ReLU</strong> en sortie<br>
                                <em>Justification :</em> Garantit des valeurs â‰¥ 0, pas de saturation</li>
                            <li><strong>Classification 5 classes :</strong> <strong>Softmax</strong> en sortie<br>
                                <em>Justification :</em> Distribution de probabilitÃ© (somme = 1)</li>
                            <li><strong>Couches cachÃ©es profondes :</strong> <strong>ReLU</strong> ou <strong>Leaky ReLU</strong><br>
                                <em>Justification :</em> Ã‰vite la disparition des gradients</li>
                            <li><strong>TempÃ©rature (+ ou -) :</strong> <strong>LinÃ©aire</strong> (pas d'activation) en sortie<br>
                                <em>Justification :</em> Permet toute valeur rÃ©elle</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "code",
            title: "EntraÃ®nement sur donnÃ©es rÃ©elles",
            description: "EntraÃ®nons notre rÃ©seau sur un problÃ¨me concret :",
            code: `# GÃ©nÃ©ration de donnÃ©es synthÃ©tiques rÃ©alistes
np.random.seed(42)

def generer_donnees_immobilier(n_samples=200):
    """GÃ©nÃ¨re des donnÃ©es immobiliÃ¨res rÃ©alistes pour Dakar"""
    
    # Variables d'entrÃ©e
    surface = np.random.uniform(100, 1000, n_samples)  # mÂ²
    distance_centre = np.random.uniform(1, 15, n_samples)  # km
    
    # Fonction complexe non-linÃ©aire pour le prix
    # Prix dÃ©pend de surfaceÂ² et 1/distance (plus proche = plus cher)
    prix_base = (surface * 0.8 + 1000 / (distance_centre + 0.5)) / 1000
    
    # Ajout de non-linÃ©aritÃ© (quartiers huppÃ©s)
    bonus_quartier = np.where((surface > 500) & (distance_centre < 5), 0.5, 0)
    prix_normalise = (prix_base + bonus_quartier) / 2  # Normalisation [0,1]
    
    # Ajout de bruit rÃ©aliste
    bruit = np.random.normal(0, 0.05, n_samples)
    prix_final = np.clip(prix_normalise + bruit, 0, 1)
    
    return np.column_stack([surface, distance_centre]), prix_final

# GÃ©nÃ©ration des donnÃ©es
X, y = generer_donnees_immobilier(150)
print(f"ğŸ“Š Dataset gÃ©nÃ©rÃ©: {X.shape[0]} terrains")
print(f"Surface moyenne: {X[:, 0].mean():.0f}mÂ²")
print(f"Distance moyenne: {X[:, 1].mean():.1f}km")
print(f"Prix moyen: {y.mean():.2f} (normalisÃ©)")

# Normalisation des entrÃ©es
X_norm = (X - X.mean(axis=0)) / X.std(axis=0)

# Visualisation des donnÃ©es
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Scatter plot 3D projetÃ©
scatter = ax1.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)
ax1.set_xlabel('Surface (mÂ²)')
ax1.set_ylabel('Distance centre (km)')
ax1.set_title('DonnÃ©es ImmobiliÃ¨res Dakar')
plt.colorbar(scatter, ax=ax1, label='Prix (normalisÃ©)')

# Distribution des prix
ax2.hist(y, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
ax2.set_xlabel('Prix (normalisÃ©)')
ax2.set_ylabel('Nombre de terrains')
ax2.set_title('Distribution des Prix')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("ğŸ“ˆ DonnÃ©es visualisÃ©es - prÃªtes pour l'entraÃ®nement !")`,
          },
          {
            type: "code",
            title: "EntraÃ®nement simple (sans backprop)",
            description:
              "Testons notre rÃ©seau avec des poids fixes optimisÃ©s :",
            code: `# Test avec poids optimisÃ©s manuellement
reseau_immobilier = ReseauNeurones([2, 6, 4, 1])

# Optimisation manuelle des poids (simulation d'entraÃ®nement)
# En rÃ©alitÃ©, ceci serait fait par backpropagation
reseau_immobilier.poids[1] = np.array([
    [ 0.8, -0.6],  # Neurone 1: surface+, distance-
    [-0.3,  0.9],  # Neurone 2: surface-, distance+
    [ 0.7,  0.2],  # Neurone 3: surface+, distance+
    [-0.1, -0.8],  # Neurone 4: surface-, distance-
    [ 0.9, -0.3],  # Neurone 5: surface+, distance-
    [ 0.4,  0.7]   # Neurone 6: surface+, distance+
])

reseau_immobilier.poids[2] = np.array([
    [ 0.8, -0.2,  0.6, -0.3],  # Combinaison complexe 1
    [-0.4,  0.9, -0.1,  0.7],  # Combinaison complexe 2
    [ 0.6,  0.3,  0.8, -0.5],  # Combinaison complexe 3
    [ 0.2, -0.6,  0.4,  0.9]   # Combinaison complexe 4
])

reseau_immobilier.poids[3] = np.array([[0.9, -0.3, 0.8, 0.6]])

# Test sur Ã©chantillon
echantillon_indices = np.random.choice(len(X), 10, replace=False)
X_test = X_norm[echantillon_indices]
y_test = y[echantillon_indices]

print("ğŸ  TEST SUR Ã‰CHANTILLON DE TERRAINS")
print("=" * 50)
print(f"{'Surface':>8} {'Distance':>9} {'Prix rÃ©el':>10} {'PrÃ©diction':>11} {'Erreur':>8}")
print("-" * 50)

erreurs = []
for i in range(len(X_test)):
    prediction = reseau_immobilier.predire(X_test[i].reshape(-1, 1))[0, 0]
    erreur = abs(prediction - y_test[i])
    erreurs.append(erreur)
    
    # DonnÃ©es originales pour affichage
    surface_orig = X[echantillon_indices[i], 0]
    distance_orig = X[echantillon_indices[i], 1]
    
    print(f"{surface_orig:8.0f} {distance_orig:9.1f} {y_test[i]:10.3f} {prediction:11.3f} {erreur:8.3f}")

print("-" * 50)
print(f"ğŸ“Š Erreur moyenne: {np.mean(erreurs):.3f}")
print(f"ğŸ“ˆ Erreur max: {np.max(erreurs):.3f}")
print(f"ğŸ“‰ Erreur min: {np.min(erreurs):.3f}")`,
          },
          {
            type: "warning",
            icon: "âš ï¸",
            title: "DÃ©fis et solutions des rÃ©seaux profonds",
            content: `
                        <p><strong>âš ï¸ Les rÃ©seaux de neurones profonds posent des dÃ©fis uniques :</strong></p>
                        
                        <p><strong>ğŸ”´ ProblÃ¨me 1 : Gradient qui disparaÃ®t</strong></p>
                        <ul>
                            <li>ğŸ“‰ <strong>SymptÃ´me</strong> : couches profondes n'apprennent plus</li>
                            <li>ğŸ”§ <strong>Solutions</strong> : ReLU, normalisation, connexions rÃ©siduelles</li>
                        </ul>
                        
                        <p><strong>ğŸ”´ ProblÃ¨me 2 : Surapprentissage</strong></p>
                        <ul>
                            <li>ğŸ“Š <strong>SymptÃ´me</strong> : performance parfaite sur train, mauvaise sur test</li>
                            <li>ğŸ”§ <strong>Solutions</strong> : dropout, rÃ©gularisation, early stopping</li>
                        </ul>
                        
                        <p><strong>ğŸ”´ ProblÃ¨me 3 : Initialisation</strong></p>
                        <ul>
                            <li>âš¡ <strong>SymptÃ´me</strong> : explosion ou disparition dÃ¨s le dÃ©but</li>
                            <li>ğŸ”§ <strong>Solutions</strong> : Xavier, He, normalisation des poids</li>
                        </ul>
                        
                        <p><strong>ğŸ”´ ProblÃ¨me 4 : Optimisation</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>SymptÃ´me</strong> : convergence lente ou vers minima locaux</li>
                            <li>ğŸ”§ <strong>Solutions</strong> : Adam, momentum, learning rate scheduling</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Solutions modernes :</strong></p>
                        <ul>
                            <li>ğŸ—ï¸ <strong>Architectures</strong> : ResNet, DenseNet, Transformer</li>
                            <li>ğŸ”§ <strong>Techniques</strong> : Batch Normalization, Dropout, Skip Connections</li>
                            <li>âš™ï¸ <strong>Optimiseurs</strong> : Adam, AdamW, RMSprop</li>
                            <li>ğŸ“Š <strong>RÃ©gularisation</strong> : L1/L2, Early Stopping, Data Augmentation</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : diagnostic de rÃ©seau",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Analysez ces 4 scÃ©narios d'entraÃ®nement et diagnostiquez les problÃ¨mes :</p>
                        
                        <p><strong>ğŸ“Š ScÃ©nario A :</strong></p>
                        <ul>
                            <li>Loss train : 0.8 â†’ 0.1 â†’ 0.05 â†’ 0.02</li>
                            <li>Loss validation : 0.8 â†’ 0.3 â†’ 0.4 â†’ 0.6</li>
                        </ul>
                        
                        <p><strong>ğŸ“Š ScÃ©nario B :</strong></p>
                        <ul>
                            <li>Loss train : 0.8 â†’ 0.7 â†’ 0.65 â†’ 0.6</li>
                            <li>Loss validation : 0.8 â†’ 0.7 â†’ 0.65 â†’ 0.6</li>
                        </ul>
                        
                        <p><strong>ğŸ“Š ScÃ©nario C :</strong></p>
                        <ul>
                            <li>Loss train : 0.8 â†’ 0.8 â†’ 0.8 â†’ 0.8</li>
                            <li>Gradients : 1.0 â†’ 0.1 â†’ 0.01 â†’ 0.001</li>
                        </ul>
                        
                        <p><strong>ğŸ“Š ScÃ©nario D :</strong></p>
                        <ul>
                            <li>Loss train : 0.8 â†’ 0.2 â†’ 0.05 â†’ 0.01</li>
                            <li>Loss validation : 0.8 â†’ 0.25 â†’ 0.08 â†’ 0.02</li>
                        </ul>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('network-diagnosis-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="network-diagnosis-exercise" style="display: none;">
                        <ol>
                            <li><strong>ScÃ©nario A :</strong> <strong>Surapprentissage sÃ©vÃ¨re</strong><br>
                                <em>SymptÃ´mes :</em> Train continue de baisser, validation remonte<br>
                                <em>Solutions :</em> Dropout, rÃ©gularisation L2, early stopping</li>
                            <li><strong>ScÃ©nario B :</strong> <strong>Sous-apprentissage</strong><br>
                                <em>SymptÃ´mes :</em> Les deux loss stagnent Ã  un niveau Ã©levÃ©<br>
                                <em>Solutions :</em> RÃ©seau plus complexe, learning rate plus Ã©levÃ©</li>
                            <li><strong>ScÃ©nario C :</strong> <strong>Gradient qui disparaÃ®t</strong><br>
                                <em>SymptÃ´mes :</em> Loss ne bouge pas, gradients â†’ 0<br>
                                <em>Solutions :</em> ReLU, normalisation, rÃ©seau moins profond</li>
                            <li><strong>ScÃ©nario D :</strong> <strong>EntraÃ®nement optimal</strong><br>
                                <em>SymptÃ´mes :</em> Les deux loss baissent ensemble<br>
                                <em>Action :</em> Continuer l'entraÃ®nement !</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "warning",
            icon: "âš ï¸",
            title: "RÃ©seaux de neurones : rÃ©volution de l'IA moderne",
            content: `
                        <p><strong>ğŸš€ Les rÃ©seaux de neurones ont rÃ©volutionnÃ© notre monde :</strong></p>
                        
                        <p><strong>ğŸ“± Applications quotidiennes :</strong></p>
                        <ul>
                            <li>ğŸ“¸ <strong>Photos</strong> : reconnaissance automatique de visages</li>
                            <li>ğŸ—£ï¸ <strong>Voix</strong> : Siri, Alexa, Google Assistant</li>
                            <li>ğŸŒ <strong>Traduction</strong> : Google Translate en temps rÃ©el</li>
                            <li>ğŸ›’ <strong>Recommandations</strong> : Netflix, Spotify, Amazon</li>
                            <li>ğŸš— <strong>Navigation</strong> : optimisation de trajets en temps rÃ©el</li>
                        </ul>
                        
                        <p><strong>ğŸ¥ Impact sociÃ©tal au SÃ©nÃ©gal :</strong></p>
                        <ul>
                            <li>ğŸ¥ <strong>SantÃ©</strong> : diagnostic automatique de maladies tropicales</li>
                            <li>ğŸŒ¾ <strong>Agriculture</strong> : optimisation des cultures selon le climat</li>
                            <li>ğŸ“š <strong>Ã‰ducation</strong> : personnalisation de l'apprentissage</li>
                            <li>ğŸ’° <strong>Finance</strong> : dÃ©tection de fraudes, crÃ©dit scoring</li>
                            <li>ğŸŒ <strong>Environnement</strong> : prÃ©diction climatique, gestion des ressources</li>
                        </ul>
                        
                        <p><strong>ğŸ”® Ã‰volution vers le Deep Learning :</strong></p>
                        <ul>
                            <li>ğŸ§  <strong>AnnÃ©es 1980</strong> : rÃ©seaux simples (quelques couches)</li>
                            <li>ğŸš€ <strong>AnnÃ©es 2010</strong> : deep learning (dizaines de couches)</li>
                            <li>ğŸŒŸ <strong>Aujourd'hui</strong> : transformers (milliards de paramÃ¨tres)</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Point clÃ© :</strong> Les rÃ©seaux de neurones ne sont pas juste un outil technique - ils reprÃ©sentent une nouvelle faÃ§on de <strong>programmer l'intelligence</strong>. Au lieu d'Ã©crire des rÃ¨gles explicites, nous laissons le rÃ©seau dÃ©couvrir les patterns dans les donnÃ©es !</p>
                        
                        <p><strong>ğŸ”® Prochaine Ã©tape :</strong> Backpropagation - l'algorithme magique qui permet aux rÃ©seaux d'apprendre automatiquement !</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ğŸ¤” Pourquoi un rÃ©seau de neurones sans fonctions d'activation ne peut-il pas rÃ©soudre des problÃ¨mes non-linÃ©aires ?",
          options: [
            "A) Il manque de neurones",
            "B) Il devient Ã©quivalent Ã  une transformation linÃ©aire",
            "C) Les poids sont mal initialisÃ©s",
            "D) Il faut plus de donnÃ©es d'entraÃ®nement",
          ],
          correct: 1,
          explanation:
            "Sans fonctions d'activation, toutes les couches se combinent en une seule transformation linÃ©aire. Peu importe le nombre de couches, le rÃ©seau ne peut crÃ©er que des frontiÃ¨res de dÃ©cision linÃ©aires, comme un simple perceptron.",
        },
        prevModule: "perceptron.html",
        nextModule: "backpropagation.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
