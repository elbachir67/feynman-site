<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>RNN/LSTM | IA4Ndada</title>

    <!-- MathJax pour les formules mathÃ©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">ğŸ  Accueil</a>
          <span>â€º</span>
          <span>ğŸ§  Deep Learning</span>
          <span>â€º</span>
          <span>RNN/LSTM</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>ğŸ”— RNN/LSTM : MÃ©moire et SÃ©quences</h1>
      <p class="subtitle">Module 4.5 - Deep Learning SÃ©quentiel</p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>ğŸ¯ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajoutÃ©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajoutÃ© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>ğŸ‰ Checkpoint - RNN/LSTM</h3>
        <p>
          FÃ©licitations ! Vous comprenez maintenant comment l'IA dÃ©veloppe une
          mÃ©moire et traite les sÃ©quences.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme complÃ©tÃ©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="cnn.html" class="nav-link" id="prev-link"
          >â† Module prÃ©cÃ©dent : CNN</a
        >
        <a href="../llm/attention.html" class="nav-link" id="next-link"
          >Module suivant : Attention â†’</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module RNN/LSTM
      const moduleConfig = {
        id: "dl-rnn-lstm",
        title: "RNN/LSTM : MÃ©moire et SÃ©quences",
        category: "Deep Learning",
        objectives: [
          "Comprendre pourquoi les rÃ©seaux classiques Ã©chouent sur les sÃ©quences",
          "MaÃ®triser l'architecture RNN et ses calculs",
          "DÃ©couvrir le problÃ¨me du gradient qui disparaÃ®t dans le temps",
          "Comprendre l'innovation LSTM et ses portes",
          "ImplÃ©menter RNN et LSTM from scratch",
        ],
        content: [
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Le dÃ©fi impossible : donner une mÃ©moire Ã  l'IA",
            content: `
                        <p>Les <strong>rÃ©seaux de neurones classiques</strong> ont une limitation fondamentale : ils n'ont <strong>aucune mÃ©moire</strong>. Chaque prÃ©diction est indÃ©pendante, comme si l'IA souffrait d'amnÃ©sie totale.</p>
                        
                        <p><strong>ğŸ”‘ ProblÃ¨me concret :</strong></p>
                        <ul>
                            <li>ğŸ’¬ <strong>Phrase</strong> : "Le prÃ©sident du SÃ©nÃ©gal habite Ã ..."</li>
                            <li>ğŸ¤– <strong>IA classique</strong> : voit seulement "Ã " â†’ impossible de prÃ©dire "Dakar"</li>
                            <li>ğŸ§  <strong>IA avec mÃ©moire</strong> : se souvient de "prÃ©sident du SÃ©nÃ©gal" â†’ prÃ©dit "Dakar"</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Types de sÃ©quences cruciales :</strong></p>
                        <ul>
                            <li>ğŸ’¬ <strong>Langage</strong> : mots dans une phrase, phrases dans un texte</li>
                            <li>ğŸ“ˆ <strong>SÃ©ries temporelles</strong> : cours de bourse, mÃ©tÃ©o, ventes</li>
                            <li>ğŸµ <strong>Audio</strong> : notes de musique, reconnaissance vocale</li>
                            <li>ğŸ§¬ <strong>ADN</strong> : sÃ©quences gÃ©nÃ©tiques, protÃ©ines</li>
                            <li>ğŸ¬ <strong>VidÃ©o</strong> : sÃ©quence d'images dans le temps</li>
                        </ul>
                        
                        <p><strong>âš ï¸ Ã‰chec des rÃ©seaux classiques :</strong></p>
                        <ul>
                            <li>âŒ <strong>Pas de contexte</strong> : chaque mot traitÃ© isolÃ©ment</li>
                            <li>âŒ <strong>Taille fixe</strong> : impossible de traiter des sÃ©quences de longueur variable</li>
                            <li>âŒ <strong>Pas d'ordre</strong> : "chat mange souris" = "souris mange chat"</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Solution rÃ©volutionnaire :</strong> Les <strong>RÃ©seaux de Neurones RÃ©currents</strong> (RNN) introduisent une boucle qui permet de "se souvenir" des entrÃ©es prÃ©cÃ©dentes !</p>
                    `,
          },
          {
            type: "intuition",
            icon: "ğŸ§ ",
            title: "L'analogie du griot sÃ©nÃ©galais",
            content: `
                        <p>Imaginez un <strong>griot traditionnel</strong> qui raconte l'histoire du SÃ©nÃ©gal devant un public Ã  GorÃ©e :</p>
                        
                        <p><strong>ğŸ­ Le griot intelligent (RNN) :</strong></p>
                        <ul>
                            <li>ğŸ“š <strong>Se souvient</strong> de ce qu'il vient de dire</li>
                            <li>ğŸ§  <strong>Adapte</strong> la suite selon le contexte</li>
                            <li>ğŸ¯ <strong>Maintient la cohÃ©rence</strong> narrative</li>
                            <li>ğŸ’« <strong>CrÃ©e des liens</strong> entre les Ã©vÃ©nements distants</li>
                        </ul>
                        
                        <p><strong>ğŸ¤– Le conteur amnÃ©sique (rÃ©seau classique) :</strong></p>
                        <ul>
                            <li>ğŸ˜µ <strong>Oublie</strong> chaque phrase aprÃ¨s l'avoir dite</li>
                            <li>ğŸ”€ <strong>Raconte</strong> des histoires incohÃ©rentes</li>
                            <li>âŒ <strong>RÃ©pÃ¨te</strong> les mÃªmes informations</li>
                            <li>ğŸ¤¯ <strong>Perd</strong> le fil de l'histoire</li>
                        </ul>
                        
                        <p><strong>ğŸ“– Exemple concret :</strong></p>
                        <p><strong>DÃ©but :</strong> "LÃ©opold SÃ©dar Senghor Ã©tait un grand poÃ¨te..."</p>
                        <p><strong>Suite intelligente :</strong> "...qui devint le premier prÃ©sident du SÃ©nÃ©gal indÃ©pendant"</p>
                        <p><strong>Suite amnÃ©sique :</strong> "...qui aimait les bananes" (aucun lien !)</p>
                        
                        <p><strong>ğŸ’¡ C'est exactement ce que fait un RNN :</strong></p>
                        <ul>
                            <li>ğŸ§  <strong>Ã‰tat cachÃ©</strong> = mÃ©moire du griot</li>
                            <li>ğŸ“ <strong>Mot actuel</strong> = ce qu'il dit maintenant</li>
                            <li>ğŸ”„ <strong>RÃ©currence</strong> = influence du passÃ© sur le prÃ©sent</li>
                            <li>ğŸ¯ <strong>PrÃ©diction</strong> = mot suivant cohÃ©rent</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Architecture RNN : formalisation mathÃ©matique",
            content: `
                        <p><strong>ğŸ“ Formalisation rigoureuse d'un RNN :</strong></p>
                        
                        <p><strong>ğŸ” Variables du systÃ¨me :</strong></p>
                        <ul style="list-style: none; padding-left: 0">
                            <li><strong>â€¢ \\(\\vec{x}_t\\)</strong> = entrÃ©e au temps t (ex: mot encodÃ©)</li>
                            <li style="margin-top: 0.5rem"><strong>â€¢ \\(\\vec{h}_t\\)</strong> = Ã©tat cachÃ© au temps t (mÃ©moire)</li>
                            <li style="margin-top: 0.5rem"><strong>â€¢ \\(\\vec{y}_t\\)</strong> = sortie au temps t (prÃ©diction)</li>
                            <li style="margin-top: 0.5rem"><strong>â€¢ \\(W_{xh}, W_{hh}, W_{hy}\\)</strong> = matrices de poids</li>
                            <li style="margin-top: 0.5rem"><strong>â€¢ \\(\\vec{b}_h, \\vec{b}_y\\)</strong> = vecteurs de biais</li>
                        </ul>
                        
                        <p><strong>ğŸ”„ Ã‰quations de rÃ©currence :</strong></p>
                        <p>$$\\vec{h}_t = \\tanh(W_{xh} \\vec{x}_t + W_{hh} \\vec{h}_{t-1} + \\vec{b}_h)$$</p>
                        <p>$$\\vec{y}_t = W_{hy} \\vec{h}_t + \\vec{b}_y$$</p>
                        
                        <p><strong>ğŸ” DÃ©cryptage des formules :</strong></p>
                        <ul>
                            <li>\\(W_{xh} \\vec{x}_t\\) = <strong>traitement de l'entrÃ©e actuelle</strong></li>
                            <li>\\(W_{hh} \\vec{h}_{t-1}\\) = <strong>influence de la mÃ©moire prÃ©cÃ©dente</strong></li>
                            <li>\\(\\tanh\\) = <strong>fonction d'activation</strong> (garde les valeurs dans [-1,1])</li>
                            <li>\\(\\vec{h}_t\\) = <strong>nouvelle mÃ©moire</strong> (combinaison prÃ©sent + passÃ©)</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ PropriÃ©tÃ©s remarquables :</strong></p>
                        <ul>
                            <li>ğŸ”„ <strong>RÃ©currence</strong> : \\(\\vec{h}_t\\) dÃ©pend de \\(\\vec{h}_{t-1}\\)</li>
                            <li>ğŸ“ <strong>Longueur variable</strong> : peut traiter des sÃ©quences de toute taille</li>
                            <li>âš–ï¸ <strong>Partage de poids</strong> : mÃªmes W pour tous les pas de temps</li>
                            <li>ğŸ§  <strong>MÃ©moire compressÃ©e</strong> : tout le passÃ© dans \\(\\vec{h}_t\\)</li>
                        </ul>
                        
                        <p><strong>ğŸ”— DÃ©roulement temporel :</strong></p>
                        <p>$$\\vec{h}_1 = \\tanh(W_{xh} \\vec{x}_1 + W_{hh} \\vec{h}_0 + \\vec{b}_h)$$</p>
                        <p>$$\\vec{h}_2 = \\tanh(W_{xh} \\vec{x}_2 + W_{hh} \\vec{h}_1 + \\vec{b}_h)$$</p>
                        <p>$$\\vec{h}_3 = \\tanh(W_{xh} \\vec{x}_3 + W_{hh} \\vec{h}_2 + \\vec{b}_h)$$</p>
                        <p>$$\\vdots$$</p>
                        
                        <p><strong>ğŸ’¡ Observation cruciale :</strong> \\(\\vec{h}_t\\) contient implicitement l'information de TOUTE la sÃ©quence \\(\\vec{x}_1, \\vec{x}_2, ..., \\vec{x}_t\\) !</p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Calcul manuel RNN : prÃ©diction de mots",
            content: `
                        <p><strong>ğŸ“ Exemple concret :</strong> RNN pour complÃ©ter "Le SÃ©nÃ©gal est..."</p>
                        
                        <p><strong>ğŸ”¤ Vocabulaire simplifiÃ© :</strong></p>
                        <ul>
                            <li>Mots : ["Le", "SÃ©nÃ©gal", "est", "beau", "grand"]</li>
                            <li>Encodage : Le=1, SÃ©nÃ©gal=2, est=3, beau=4, grand=5</li>
                        </ul>
                        
                        <p><strong>âš™ï¸ ParamÃ¨tres du RNN :</strong></p>
                        <p>Dimension cachÃ©e = 2, vocabulaire = 5</p>
                        <p>$$W_{xh} = \\begin{bmatrix} 0.5 & 0.3 \\\\ -0.2 & 0.8 \\end{bmatrix}, \\quad W_{hh} = \\begin{bmatrix} 0.4 & -0.3 \\\\ 0.6 & 0.2 \\end{bmatrix}$$</p>
                        <p>$$\\vec{b}_h = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}, \\quad \\vec{h}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ”¢ Calcul Ã©tape par Ã©tape :</strong></p>
                        
                        <p><strong>t=1 : "Le" (xâ‚ = [1, 0, 0, 0, 0]) :</strong></p>
                        <p>$$\\vec{z}_1 = W_{xh} \\vec{x}_1 + W_{hh} \\vec{h}_0 + \\vec{b}_h = \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix} = \\begin{bmatrix} 0.6 \\\\ -0.3 \\end{bmatrix}$$</p>
                        <p>$$\\vec{h}_1 = \\tanh(\\vec{z}_1) = \\begin{bmatrix} \\tanh(0.6) \\\\ \\tanh(-0.3) \\end{bmatrix} â‰ˆ \\begin{bmatrix} 0.537 \\\\ -0.291 \\end{bmatrix}$$</p>
                        
                        <p><strong>t=2 : "SÃ©nÃ©gal" (xâ‚‚ = [0, 1, 0, 0, 0]) :</strong></p>
                        <p>$$\\vec{z}_2 = \\begin{bmatrix} 0.3 \\\\ 0.8 \\end{bmatrix} + \\begin{bmatrix} 0.4 & -0.3 \\\\ 0.6 & 0.2 \\end{bmatrix} \\begin{bmatrix} 0.537 \\\\ -0.291 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$$</p>
                        <p>$$= \\begin{bmatrix} 0.3 \\\\ 0.8 \\end{bmatrix} + \\begin{bmatrix} 0.302 \\\\ 0.264 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix} = \\begin{bmatrix} 0.702 \\\\ 0.964 \\end{bmatrix}$$</p>
                        <p>$$\\vec{h}_2 = \\tanh(\\vec{z}_2) â‰ˆ \\begin{bmatrix} 0.604 \\\\ 0.747 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ’¡ Observation cruciale :</strong> \\(\\vec{h}_2\\) contient maintenant l'information de "Le SÃ©nÃ©gal" ! La mÃ©moire s'enrichit Ã  chaque Ã©tape.</p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice pratique : calcul manuel RNN",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>RNN simple pour analyser le sentiment de "Dakar est belle" :</p>
                        
                        <p><strong>âš™ï¸ ParamÃ¨tres :</strong></p>
                        <p>$$W_{xh} = \\begin{bmatrix} 0.6 \\\\ -0.4 \\end{bmatrix}, \\quad W_{hh} = \\begin{bmatrix} 0.8 & 0.2 \\end{bmatrix}, \\quad \\vec{b}_h = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$</p>
                        
                        <p><strong>ğŸ“ SÃ©quence :</strong> ["Dakar"=1, "est"=2, "belle"=3]</p>
                        <p><strong>Ã‰tat initial :</strong> \\(h_0 = 0\\)</p>
                        
                        <p><strong>ğŸ“ Calculez :</strong></p>
                        <ol>
                            <li>\\(h_1\\) aprÃ¨s "Dakar"</li>
                            <li>\\(h_2\\) aprÃ¨s "Dakar est"</li>
                            <li>\\(h_3\\) aprÃ¨s "Dakar est belle"</li>
                            <li>Comment \\(h_3\\) encode-t-il toute la phrase ?</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('rnn-manual-calculation')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="rnn-manual-calculation" style="display: none;">
                        <ol>
                            <li><strong>AprÃ¨s "Dakar" (xâ‚=1) :</strong><br>
                                \\(z_1 = 0.6 \\times 1 + 0.8 \\times 0 + 0 = 0.6\\)<br>
                                \\(h_1 = \\tanh(0.6) â‰ˆ 0.537\\)</li>
                            <li><strong>AprÃ¨s "est" (xâ‚‚=2) :</strong><br>
                                \\(z_2 = 0.6 \\times 2 + 0.8 \\times 0.537 = 1.2 + 0.430 = 1.630\\)<br>
                                \\(h_2 = \\tanh(1.630) â‰ˆ 0.928\\)</li>
                            <li><strong>AprÃ¨s "belle" (xâ‚ƒ=3) :</strong><br>
                                \\(z_3 = 0.6 \\times 3 + 0.8 \\times 0.928 = 1.8 + 0.742 = 2.542\\)<br>
                                \\(h_3 = \\tanh(2.542) â‰ˆ 0.987\\)</li>
                            <li><strong>Encodage de la phrase :</strong><br>
                                \\(h_3 â‰ˆ 0.987\\) encode "Dakar est belle" en un seul nombre !<br>
                                Cette valeur Ã©levÃ©e pourrait indiquer un sentiment trÃ¨s positif.</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Le problÃ¨me du gradient qui disparaÃ®t dans le temps",
            content: `
                        <p><strong>âš ï¸ ProblÃ¨me fondamental des RNN :</strong> Plus la sÃ©quence est longue, plus il devient difficile d'apprendre des dÃ©pendances lointaines.</p>
                        
                        <p><strong>ğŸ” Analyse mathÃ©matique :</strong></p>
                        <p>Pour calculer \\(\\frac{\\partial L}{\\partial W_{hh}}\\), nous devons remonter dans le temps :</p>
                        <p>$$\\frac{\\partial \\vec{h}_t}{\\partial \\vec{h}_{t-1}} = \\text{diag}(\\tanh'(\\vec{z}_t)) \\cdot W_{hh}$$</p>
                        
                        <p><strong>ğŸ”— RÃ¨gle de dÃ©rivation en chaÃ®ne temporelle :</strong></p>
                        <p>$$\\frac{\\partial \\vec{h}_t}{\\partial \\vec{h}_{t-k}} = \\prod_{i=1}^{k} \\frac{\\partial \\vec{h}_{t-i+1}}{\\partial \\vec{h}_{t-i}}$$</p>
                        
                        <p><strong>ğŸ’¥ Explosion ou disparition :</strong></p>
                        <ul>
                            <li>Si \\(||\\frac{\\partial \\vec{h}_t}{\\partial \\vec{h}_{t-1}}|| > 1\\) : <strong>explosion exponentielle</strong></li>
                            <li>Si \\(||\\frac{\\partial \\vec{h}_t}{\\partial \\vec{h}_{t-1}}|| < 1\\) : <strong>disparition exponentielle</strong></li>
                        </ul>
                        
                        <p><strong>ğŸ“Š Analyse numÃ©rique :</strong></p>
                        <p>Supposons \\(||\\frac{\\partial \\vec{h}_t}{\\partial \\vec{h}_{t-1}}|| = 0.5\\) (cas typique avec tanh)</p>
                        <p>AprÃ¨s k pas de temps : \\(||\\frac{\\partial \\vec{h}_t}{\\partial \\vec{h}_{t-k}}|| = (0.5)^k\\)</p>
                        
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>ğŸ“‰ Disparition rapide :</strong><br>
                            â€¢ k=5 : gradient Ã— 0.03 (97% perdu)<br>
                            â€¢ k=10 : gradient Ã— 0.001 (99.9% perdu)<br>
                            â€¢ k=20 : gradient Ã— 10â»â¶ (pratiquement 0)
                        </div>
                        
                        <p><strong>ğŸ’¡ ConsÃ©quence pratique :</strong> Un RNN standard ne peut apprendre des dÃ©pendances que sur ~5-10 pas de temps. Pour des sÃ©quences plus longues, il "oublie" le dÃ©but !</p>
                    `,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "LSTM : l'innovation rÃ©volutionnaire",
            content: `
                        <p>Les <strong>Long Short-Term Memory</strong> (LSTM) rÃ©solvent le problÃ¨me du gradient qui disparaÃ®t grÃ¢ce Ã  une architecture gÃ©niale avec des "portes" qui contrÃ´lent le flux d'information.</p>
                        
                        <p><strong>ğŸšª Les trois portes magiques :</strong></p>
                        
                        <p><strong>1ï¸âƒ£ Porte d'oubli (Forget Gate) :</strong></p>
                        <p>$$\\vec{f}_t = \\sigma(W_f \\cdot [\\vec{h}_{t-1}, \\vec{x}_t] + \\vec{b}_f)$$</p>
                        <p><strong>RÃ´le :</strong> DÃ©cide quoi oublier de la mÃ©moire prÃ©cÃ©dente (0 = oublier, 1 = garder)</p>
                        
                        <p><strong>2ï¸âƒ£ Porte d'entrÃ©e (Input Gate) :</strong></p>
                        <p>$$\\vec{i}_t = \\sigma(W_i \\cdot [\\vec{h}_{t-1}, \\vec{x}_t] + \\vec{b}_i)$$</p>
                        <p>$$\\vec{\\tilde{C}}_t = \\tanh(W_C \\cdot [\\vec{h}_{t-1}, \\vec{x}_t] + \\vec{b}_C)$$</p>
                        <p><strong>RÃ´le :</strong> DÃ©cide quelles nouvelles informations stocker</p>
                        
                        <p><strong>3ï¸âƒ£ Porte de sortie (Output Gate) :</strong></p>
                        <p>$$\\vec{o}_t = \\sigma(W_o \\cdot [\\vec{h}_{t-1}, \\vec{x}_t] + \\vec{b}_o)$$</p>
                        <p><strong>RÃ´le :</strong> DÃ©cide quelles parties de la mÃ©moire exposer</p>
                        
                        <p><strong>ğŸ§  Mise Ã  jour de la mÃ©moire :</strong></p>
                        <p>$$\\vec{C}_t = \\vec{f}_t \\odot \\vec{C}_{t-1} + \\vec{i}_t \\odot \\vec{\\tilde{C}}_t$$</p>
                        <p>$$\\vec{h}_t = \\vec{o}_t \\odot \\tanh(\\vec{C}_t)$$</p>
                        
                        <p><strong>ğŸ’¡ GÃ©nie de l'architecture :</strong></p>
                        <ul>
                            <li>ğŸ”„ <strong>Autoroute d'information</strong> : \\(\\vec{C}_t\\) peut traverser de nombreux pas de temps sans dÃ©gradation</li>
                            <li>ğŸšª <strong>ContrÃ´le fin</strong> : les portes apprennent quand oublier/retenir/exposer</li>
                            <li>ğŸ“ˆ <strong>Gradients prÃ©servÃ©s</strong> : chemins directs pour la rÃ©tropropagation</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : calcul manuel LSTM simplifiÃ©",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>LSTM simplifiÃ© (dimension 1) pour analyser "Dakarâ†’belle" :</p>
                        
                        <p><strong>âš™ï¸ ParamÃ¨tres simplifiÃ©s :</strong></p>
                        <ul>
                            <li>\\(W_f = 0.5, W_i = 0.8, W_o = 0.6, W_C = 0.7\\)</li>
                            <li>Tous les biais = 0</li>
                            <li>Ã‰tat initial : \\(h_0 = 0, C_0 = 0\\)</li>
                        </ul>
                        
                        <p><strong>ğŸ“ SÃ©quence :</strong> xâ‚ = 1 ("Dakar"), xâ‚‚ = 2 ("belle")</p>
                        
                        <p><strong>ğŸ“ Calculez pour t=1 :</strong></p>
                        <ol>
                            <li>Porte d'oubli \\(f_1\\)</li>
                            <li>Porte d'entrÃ©e \\(i_1\\) et candidat \\(\\tilde{C}_1\\)</li>
                            <li>Nouvelle mÃ©moire \\(C_1\\)</li>
                            <li>Porte de sortie \\(o_1\\) et Ã©tat cachÃ© \\(h_1\\)</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('lstm-manual-calculation')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="lstm-manual-calculation" style="display: none;">
                        <p><strong>t=1 avec xâ‚=1, hâ‚€=0, Câ‚€=0 :</strong></p>
                        <ol>
                            <li><strong>Porte d'oubli :</strong><br>
                                \\(f_1 = \\sigma(0.5 \\times (0 + 1)) = \\sigma(0.5) â‰ˆ 0.622\\)</li>
                            <li><strong>Porte d'entrÃ©e et candidat :</strong><br>
                                \\(i_1 = \\sigma(0.8 \\times 1) = \\sigma(0.8) â‰ˆ 0.689\\)<br>
                                \\(\\tilde{C}_1 = \\tanh(0.7 \\times 1) = \\tanh(0.7) â‰ˆ 0.604\\)</li>
                            <li><strong>Nouvelle mÃ©moire :</strong><br>
                                \\(C_1 = 0.622 \\times 0 + 0.689 \\times 0.604 â‰ˆ 0.416\\)</li>
                            <li><strong>Sortie :</strong><br>
                                \\(o_1 = \\sigma(0.6 \\times 1) â‰ˆ 0.646\\)<br>
                                \\(h_1 = 0.646 \\times \\tanh(0.416) â‰ˆ 0.646 \\times 0.395 â‰ˆ 0.255\\)</li>
                        </ol>
                        <p><strong>ğŸ’¡ RÃ©sultat :</strong> \\(C_1 = 0.416\\) stocke "Dakar", \\(h_1 = 0.255\\) expose une partie de cette information.</p>
                        </div>
                    `,
          },
          {
            type: "code",
            title: "ImplÃ©mentation RNN from scratch",
            description: "CrÃ©ons un RNN simple pour prÃ©diction de sÃ©quences :",
            code: `import numpy as np
import matplotlib.pyplot as plt

class RNNSimple:
    def __init__(self, input_size, hidden_size, output_size):
        """RNN simple pour sÃ©quences"""
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # Initialisation des poids (Xavier)
        self.Wxh = np.random.randn(hidden_size, input_size) * 0.1
        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.1
        self.Why = np.random.randn(output_size, hidden_size) * 0.1
        
        # Biais
        self.bh = np.zeros((hidden_size, 1))
        self.by = np.zeros((output_size, 1))
        
        print(f"ğŸ§  RNN crÃ©Ã©: {input_size}â†’{hidden_size}â†’{output_size}")
        print(f"ğŸ“Š ParamÃ¨tres: {self.compter_parametres()}")
    
    def compter_parametres(self):
        """Compter le nombre total de paramÃ¨tres"""
        return (self.Wxh.size + self.Whh.size + self.Why.size + 
                self.bh.size + self.by.size)
    
    def forward(self, inputs):
        """Propagation avant sur une sÃ©quence"""
        seq_length = len(inputs)
        
        # Stockage des Ã©tats pour visualisation
        self.h_states = {}
        self.y_outputs = {}
        
        # Ã‰tat initial
        h = np.zeros((self.hidden_size, 1))
        self.h_states[-1] = h.copy()
        
        for t in range(seq_length):
            # EntrÃ©e au temps t
            x = np.array([[inputs[t]]])  # Reshape en colonne
            
            # Calcul Ã©tat cachÃ©
            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)
            
            # Calcul sortie
            y = self.Why @ h + self.by
            
            # Stockage
            self.h_states[t] = h.copy()
            self.y_outputs[t] = y.copy()
        
        return self.h_states, self.y_outputs
    
    def predict_next(self, sequence):
        """PrÃ©dire l'Ã©lÃ©ment suivant d'une sÃ©quence"""
        h_states, y_outputs = self.forward(sequence)
        derniere_sortie = y_outputs[len(sequence) - 1]
        return derniere_sortie[0, 0]

# Test sur sÃ©quence de nombres (Fibonacci simplifiÃ©)
print("ğŸ”¢ TEST RNN SUR SÃ‰QUENCE NUMÃ‰RIQUE")
print("=" * 40)

# CrÃ©ation du RNN
rnn = RNNSimple(input_size=1, hidden_size=3, output_size=1)

# SÃ©quence test: [1, 1, 2, 3, 5] (dÃ©but Fibonacci)
sequence_test = [1, 1, 2, 3, 5]
print(f"SÃ©quence d'entrÃ©e: {sequence_test}")

# Propagation avant
h_states, y_outputs = rnn.forward(sequence_test)

print(f"\\nÃ‰volution des Ã©tats cachÃ©s:")
for t in range(len(sequence_test)):
    h = h_states[t].flatten()
    y = y_outputs[t][0, 0]
    print(f"t={t}: x={sequence_test[t]} â†’ h={h} â†’ y={y:.3f}")

# PrÃ©diction du suivant
prediction = rnn.predict_next(sequence_test)
print(f"\\nğŸ¯ PrÃ©diction suivant: {prediction:.3f}")
print(f"ğŸ“Š Vraie valeur suivante: 8 (5+3)")`,
          },
          {
            type: "code",
            title: "ImplÃ©mentation LSTM from scratch",
            description: "CrÃ©ons un LSTM complet avec toutes les portes :",
            code: `class LSTMSimple:
    def __init__(self, input_size, hidden_size):
        """LSTM avec les 3 portes"""
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        # Poids pour les 4 transformations (forget, input, candidate, output)
        self.Wf = np.random.randn(hidden_size, input_size + hidden_size) * 0.1
        self.Wi = np.random.randn(hidden_size, input_size + hidden_size) * 0.1
        self.Wc = np.random.randn(hidden_size, input_size + hidden_size) * 0.1
        self.Wo = np.random.randn(hidden_size, input_size + hidden_size) * 0.1
        
        # Biais
        self.bf = np.zeros((hidden_size, 1))
        self.bi = np.zeros((hidden_size, 1))
        self.bc = np.zeros((hidden_size, 1))
        self.bo = np.zeros((hidden_size, 1))
        
        print(f"ğŸ§  LSTM crÃ©Ã©: dimension cachÃ©e = {hidden_size}")
    
    def sigmoid(self, x):
        """Sigmoid avec protection overflow"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def forward_step(self, x, h_prev, C_prev):
        """Un pas de temps LSTM"""
        # ConcatÃ©nation [h_{t-1}, x_t]
        concat = np.vstack([h_prev, x])
        
        # Calcul des portes
        f_t = self.sigmoid(self.Wf @ concat + self.bf)  # Forget gate
        i_t = self.sigmoid(self.Wi @ concat + self.bi)  # Input gate
        C_tilde = np.tanh(self.Wc @ concat + self.bc)   # Candidate values
        o_t = self.sigmoid(self.Wo @ concat + self.bo)  # Output gate
        
        # Mise Ã  jour de la mÃ©moire cellulaire
        C_t = f_t * C_prev + i_t * C_tilde
        
        # Nouvel Ã©tat cachÃ©
        h_t = o_t * np.tanh(C_t)
        
        return h_t, C_t, (f_t, i_t, C_tilde, o_t)
    
    def forward(self, sequence):
        """Propagation avant complÃ¨te"""
        seq_length = len(sequence)
        
        # Ã‰tats initiaux
        h = np.zeros((self.hidden_size, 1))
        C = np.zeros((self.hidden_size, 1))
        
        # Stockage pour analyse
        self.h_history = []
        self.C_history = []
        self.gates_history = []
        
        for t in range(seq_length):
            x = np.array([[sequence[t]]])
            h, C, gates = self.forward_step(x, h, C)
            
            self.h_history.append(h.copy())
            self.C_history.append(C.copy())
            self.gates_history.append(gates)
        
        return h, C

# Test LSTM sur sÃ©quence avec mÃ©moire longue
print("\\nğŸ§  TEST LSTM SUR SÃ‰QUENCE LONGUE")
print("=" * 40)

lstm = LSTMSimple(input_size=1, hidden_size=2)

# SÃ©quence avec pattern Ã  long terme: [1, 0, 0, 1, 0, 0, 1, 0, 0]
# Pattern: 1 tous les 3 Ã©lÃ©ments
sequence_longue = [1, 0, 0, 1, 0, 0, 1, 0, 0]
print(f"SÃ©quence avec pattern: {sequence_longue}")

h_final, C_final = lstm.forward(sequence_longue)

print(f"\\nÃ‰volution de la mÃ©moire cellulaire:")
for t, C_t in enumerate(lstm.C_history):
    print(f"t={t}: C={C_t.flatten()}")

print(f"\\nÃ‰tat final:")
print(f"h_final = {h_final.flatten()}")
print(f"C_final = {C_final.flatten()}")`,
          },
          {
            type: "code",
            title: "Analyse des portes LSTM",
            description: "Analysons le comportement des portes :",
            code: `def analyser_portes_lstm():
    """Analyse le comportement des portes LSTM"""
    
    print("ğŸšª ANALYSE DES PORTES LSTM")
    print("=" * 30)
    
    # RÃ©cupÃ©ration des portes de la derniÃ¨re exÃ©cution
    if hasattr(lstm, 'gates_history'):
        for t, (f_t, i_t, C_tilde, o_t) in enumerate(lstm.gates_history):
            print(f"\\nTemps t={t} (entrÃ©e: {sequence_longue[t]}):")
            print(f"  ğŸšª Forget gate: {f_t.flatten()[0]:.3f} (oubli: {1-f_t.flatten()[0]:.3f})")
            print(f"  ğŸ“¥ Input gate:  {i_t.flatten()[0]:.3f}")
            print(f"  ğŸ’­ Candidat:    {C_tilde.flatten()[0]:.3f}")
            print(f"  ğŸ“¤ Output gate: {o_t.flatten()[0]:.3f}")
            
            # InterprÃ©tation
            if f_t.flatten()[0] > 0.7:
                print(f"     â†’ Garde la mÃ©moire prÃ©cÃ©dente")
            elif f_t.flatten()[0] < 0.3:
                print(f"     â†’ Oublie la mÃ©moire prÃ©cÃ©dente")
            
            if i_t.flatten()[0] > 0.7:
                print(f"     â†’ Stocke beaucoup de nouvelle info")
            elif i_t.flatten()[0] < 0.3:
                print(f"     â†’ Ignore la nouvelle info")

analyser_portes_lstm()

# Visualisation de l'Ã©volution des portes
def visualiser_portes():
    """Visualise l'Ã©volution des portes dans le temps"""
    if not hasattr(lstm, 'gates_history'):
        return
    
    temps = range(len(lstm.gates_history))
    forget_vals = [gates[0].flatten()[0] for gates in lstm.gates_history]
    input_vals = [gates[1].flatten()[0] for gates in lstm.gates_history]
    output_vals = [gates[3].flatten()[0] for gates in lstm.gates_history]
    
    plt.figure(figsize=(12, 6))
    plt.plot(temps, forget_vals, 'r-o', label='Forget Gate', linewidth=2)
    plt.plot(temps, input_vals, 'g-s', label='Input Gate', linewidth=2)
    plt.plot(temps, output_vals, 'b-^', label='Output Gate', linewidth=2)
    
    plt.title('Ã‰volution des Portes LSTM')
    plt.xlabel('Temps')
    plt.ylabel('Activation des portes')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.ylim(0, 1)
    
    # Marquer les entrÃ©es importantes
    for t, val in enumerate(sequence_longue):
        if val == 1:
            plt.axvline(x=t, color='orange', linestyle='--', alpha=0.5)
    
    plt.show()

visualiser_portes()
print("ğŸ“Š Analyse des portes terminÃ©e !")`,
          },
          {
            type: "code",
            title: "Comparaison RNN vs LSTM",
            description: "Comparons les performances sur sÃ©quences longues :",
            code: `def comparer_rnn_lstm():
    """Compare RNN et LSTM sur mÃ©morisation long terme"""
    
    print("âš”ï¸ COMPARAISON RNN vs LSTM")
    print("=" * 35)
    
    # SÃ©quence avec dÃ©pendance trÃ¨s lointaine
    # Pattern: premier Ã©lÃ©ment dÃ©termine le dernier
    sequences_test = [
        ([1, 0, 0, 0, 0, 0, 0, 0], "Si dÃ©but=1 â†’ fin devrait Ãªtre Ã©levÃ©e"),
        ([0, 0, 0, 0, 0, 0, 0, 0], "Si dÃ©but=0 â†’ fin devrait Ãªtre faible"),
        ([1, 0, 0, 0, 0, 0, 0, 1], "Pattern complexe avec mÃ©moire"),
    ]
    
    # Test RNN
    rnn_test = RNNSimple(1, 3, 1)
    
    # Test LSTM  
    lstm_test = LSTMSimple(1, 3)
    
    print("\\nğŸ“Š RÃ©sultats de mÃ©morisation:")
    print("-" * 50)
    print(f"{'SÃ©quence':<25} {'RNN':<10} {'LSTM':<10} {'Attendu'}")
    print("-" * 50)
    
    for seq, description in sequences_test:
        # RNN
        rnn_test.forward(seq)
        rnn_final = rnn_test.h_states[len(seq)-1][0, 0]
        
        # LSTM
        h_lstm, C_lstm = lstm_test.forward(seq)
        lstm_final = h_lstm[0, 0]
        
        # Valeur attendue basÃ©e sur le premier Ã©lÃ©ment
        attendu = "Ã‰levÃ©" if seq[0] == 1 else "Faible"
        
        print(f"{str(seq[:3])+'...'+str(seq[-2:]):<25} {rnn_final:<10.3f} {lstm_final:<10.3f} {attendu}")
    
    print("-" * 50)
    print("ğŸ’¡ LSTM devrait mieux prÃ©server l'information du dÃ©but !")

comparer_rnn_lstm()

# Simulation du problÃ¨me de gradient
def simuler_gradient_temporel():
    """Simule la disparition du gradient dans le temps"""
    
    print("\\nğŸ“‰ SIMULATION: GRADIENT TEMPOREL")
    print("=" * 35)
    
    # Simulation sur 20 pas de temps
    pas_temps = range(1, 21)
    
    # RNN: gradient Ã— (dÃ©rivÃ©e tanh) Ã— (poids) Ã  chaque pas
    # Supposons dÃ©rivÃ©e tanh â‰ˆ 0.5 et poids â‰ˆ 0.8
    facteur_rnn = 0.5 * 0.8  # = 0.4 par pas
    gradients_rnn = [1.0 * (facteur_rnn ** t) for t in pas_temps]
    
    # LSTM: gradient prÃ©servÃ© grÃ¢ce aux connexions directes
    # Supposons prÃ©servation de 90% par pas
    facteur_lstm = 0.9
    gradients_lstm = [1.0 * (facteur_lstm ** t) for t in pas_temps]
    
    plt.figure(figsize=(12, 6))
    plt.semilogy(pas_temps, gradients_rnn, 'r-o', label='RNN Standard', linewidth=2)
    plt.semilogy(pas_temps, gradients_lstm, 'b-s', label='LSTM', linewidth=2)
    plt.axhline(y=1e-3, color='black', linestyle='--', alpha=0.5, label='Seuil critique')
    
    plt.title('Disparition des Gradients: RNN vs LSTM')
    plt.xlabel('Pas de temps en arriÃ¨re')
    plt.ylabel('Magnitude du gradient (Ã©chelle log)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # Analyse
    seuil_critique = 1e-3
    rnn_limite = next((t for t, g in enumerate(gradients_rnn) if g < seuil_critique), 20)
    lstm_limite = next((t for t, g in enumerate(gradients_lstm) if g < seuil_critique), 20)
    
    print(f"ğŸ“Š RNN: gradient critique aprÃ¨s {rnn_limite} pas")
    print(f"ğŸ§  LSTM: gradient critique aprÃ¨s {lstm_limite} pas")
    print(f"ğŸš€ LSTM: {lstm_limite - rnn_limite}x plus de mÃ©moire !")

simuler_gradient_temporel()`,
          },
          {
            type: "code",
            title: "Application : prÃ©diction mÃ©tÃ©o SÃ©nÃ©gal",
            description: "Appliquons LSTM Ã  la prÃ©diction mÃ©tÃ©orologique :",
            code: `# GÃ©nÃ©ration de donnÃ©es mÃ©tÃ©o rÃ©alistes pour Dakar
np.random.seed(42)

def generer_meteo_dakar(nb_jours=100):
    """GÃ©nÃ¨re des donnÃ©es mÃ©tÃ©o rÃ©alistes pour Dakar"""
    
    # Tendance saisonniÃ¨re (saison sÃ¨che vs hivernage)
    jours = np.arange(nb_jours)
    tendance_saisonniere = 28 + 5 * np.sin(2 * np.pi * jours / 365)
    
    # Variations hebdomadaires
    variation_hebdo = 2 * np.sin(2 * np.pi * jours / 7)
    
    # Bruit alÃ©atoire
    bruit = np.random.normal(0, 1.5, nb_jours)
    
    # TempÃ©rature finale
    temperatures = tendance_saisonniere + variation_hebdo + bruit
    
    # Ajout d'Ã©vÃ©nements extrÃªmes (harmattan, pluies)
    for i in range(5):  # 5 Ã©vÃ©nements alÃ©atoires
        jour_event = np.random.randint(0, nb_jours)
        if np.random.random() > 0.5:  # Harmattan
            temperatures[jour_event:jour_event+3] += np.random.uniform(8, 12)
        else:  # Pluies
            temperatures[jour_event:jour_event+2] -= np.random.uniform(3, 6)
    
    return temperatures

# GÃ©nÃ©ration des donnÃ©es
temperatures_dakar = generer_meteo_dakar(60)
jours = range(len(temperatures_dakar))

print("ğŸŒ¡ï¸ PRÃ‰DICTION MÃ‰TÃ‰O DAKAR AVEC LSTM")
print("=" * 40)
print(f"DonnÃ©es: {len(temperatures_dakar)} jours")
print(f"TempÃ©rature moyenne: {np.mean(temperatures_dakar):.1f}Â°C")
print(f"Ã‰cart-type: {np.std(temperatures_dakar):.1f}Â°C")

# Visualisation des donnÃ©es
plt.figure(figsize=(14, 6))
plt.plot(jours, temperatures_dakar, 'b-', linewidth=1.5, alpha=0.7)
plt.title('TempÃ©ratures Ã  Dakar - Dataset d\'EntraÃ®nement')
plt.xlabel('Jour')
plt.ylabel('TempÃ©rature (Â°C)')
plt.grid(True, alpha=0.3)

# Marquer les Ã©vÃ©nements extrÃªmes
seuil_haut = np.mean(temperatures_dakar) + 2 * np.std(temperatures_dakar)
seuil_bas = np.mean(temperatures_dakar) - 2 * np.std(temperatures_dakar)

extremes_hauts = np.where(temperatures_dakar > seuil_haut)[0]
extremes_bas = np.where(temperatures_dakar < seuil_bas)[0]

plt.scatter(extremes_hauts, temperatures_dakar[extremes_hauts], 
           color='red', s=50, label='Harmattan', zorder=5)
plt.scatter(extremes_bas, temperatures_dakar[extremes_bas], 
           color='blue', s=50, label='Pluies', zorder=5)

plt.legend()
plt.show()

print(f"ğŸ”¥ Ã‰vÃ©nements chauds dÃ©tectÃ©s: {len(extremes_hauts)}")
print(f"ğŸŒ§ï¸ Ã‰vÃ©nements froids dÃ©tectÃ©s: {len(extremes_bas)}")`,
          },
          {
            type: "code",
            title: "EntraÃ®nement LSTM pour prÃ©diction",
            description: "EntraÃ®nons le LSTM Ã  prÃ©dire la mÃ©tÃ©o :",
            code: `def preparer_sequences_meteo(temperatures, longueur_sequence=7):
    """PrÃ©pare les sÃ©quences pour l'entraÃ®nement"""
    X, y = [], []
    
    for i in range(len(temperatures) - longueur_sequence):
        # SÃ©quence d'entrÃ©e: 7 jours prÃ©cÃ©dents
        sequence_entree = temperatures[i:i+longueur_sequence]
        # Cible: tempÃ©rature du jour suivant
        temperature_cible = temperatures[i+longueur_sequence]
        
        X.append(sequence_entree)
        y.append(temperature_cible)
    
    return np.array(X), np.array(y)

# PrÃ©paration des donnÃ©es
X_meteo, y_meteo = preparer_sequences_meteo(temperatures_dakar, 7)
print(f"\\nğŸ“Š PRÃ‰PARATION DES DONNÃ‰ES")
print(f"SÃ©quences d'entraÃ®nement: {len(X_meteo)}")
print(f"Longueur de chaque sÃ©quence: {X_meteo.shape[1]} jours")

# Exemple de sÃ©quence
print(f"\\nğŸ“ Exemple de sÃ©quence:")
print(f"EntrÃ©e (7 jours): {X_meteo[0].round(1)}")
print(f"Cible (jour 8): {y_meteo[0]:.1f}Â°C")

# Test de prÃ©diction simple (sans entraÃ®nement complet)
lstm_meteo = LSTMSimple(input_size=1, hidden_size=4)

def predire_temperature(lstm, sequence_7_jours):
    """PrÃ©diction simple basÃ©e sur l'Ã©tat final"""
    h_final, C_final = lstm.forward(sequence_7_jours)
    
    # PrÃ©diction simple: combinaison linÃ©aire de l'Ã©tat cachÃ©
    # (En rÃ©alitÃ©, il faudrait une couche de sortie entraÃ®nÃ©e)
    prediction = np.sum(h_final) * 10 + 25  # Approximation
    return prediction

# Test sur quelques sÃ©quences
print(f"\\nğŸ¯ TESTS DE PRÃ‰DICTION (sans entraÃ®nement):")
print("-" * 45)
print(f"{'SÃ©quence (7j)':<25} {'Vraie':<8} {'PrÃ©dite':<8} {'Erreur'}")
print("-" * 45)

for i in range(0, min(5, len(X_meteo)), 1):
    vraie_temp = y_meteo[i]
    pred_temp = predire_temperature(lstm_meteo, X_meteo[i])
    erreur = abs(vraie_temp - pred_temp)
    
    seq_str = f"[{X_meteo[i][0]:.0f}...{X_meteo[i][-1]:.0f}]"
    print(f"{seq_str:<25} {vraie_temp:<8.1f} {pred_temp:<8.1f} {erreur:.1f}")

print("-" * 45)
print("âš ï¸ Note: PrÃ©dictions alÃ©atoires sans entraÃ®nement !")
print("ğŸ¯ Avec entraÃ®nement, LSTM apprendrait les patterns saisonniers")`,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : analyse de sÃ©quence wolof",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Analysez comment un LSTM traiterait la phrase wolof "Dakar dafa rafet" (Dakar est belle) :</p>
                        
                        <p><strong>ğŸ“ Encodage simplifiÃ© :</strong></p>
                        <ul>
                            <li>"Dakar" â†’ 1</li>
                            <li>"dafa" â†’ 2 (auxiliaire Ãªtre)</li>
                            <li>"rafet" â†’ 3 (belle)</li>
                        </ul>
                        
                        <p><strong>ğŸ§  Questions d'analyse :</strong></p>
                        <ol>
                            <li>Pourquoi un RNN standard pourrait oublier "Dakar" en arrivant Ã  "rafet" ?</li>
                            <li>Comment les portes LSTM prÃ©serveraient cette information ?</li>
                            <li>Quelle porte serait la plus active pour "dafa" (mot de liaison) ?</li>
                            <li>Comment le LSTM "comprendrait" que "rafet" se rapporte Ã  "Dakar" ?</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('wolof-sequence-analysis')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="wolof-sequence-analysis" style="display: none;">
                        <ol>
                            <li><strong>Oubli RNN :</strong><br>
                                AprÃ¨s 2-3 mots, l'information "Dakar" se dilue dans l'Ã©tat cachÃ©.<br>
                                Le gradient de "rafet" vers "Dakar" devient nÃ©gligeable.</li>
                            <li><strong>PrÃ©servation LSTM :</strong><br>
                                La <strong>mÃ©moire cellulaire C</strong> stocke "Dakar" directement.<br>
                                Les portes apprennent Ã  prÃ©server cette info cruciale.</li>
                            <li><strong>Porte pour "dafa" :</strong><br>
                                <strong>Forget gate faible</strong> : ne pas oublier "Dakar"<br>
                                <strong>Input gate faible</strong> : "dafa" n'apporte pas d'info nouvelle<br>
                                <strong>Output gate modÃ©rÃ©e</strong> : maintenir le contexte</li>
                            <li><strong>ComprÃ©hension LSTM :</strong><br>
                                La mÃ©moire C contient "Dakar", l'Ã©tat h expose cette info quand "rafet" arrive.<br>
                                Le LSTM "sait" que l'adjectif se rapporte au sujet initial !</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Variantes et Ã©volutions : GRU et au-delÃ ",
            content: `
                        <p><strong>ğŸ”§ GRU (Gated Recurrent Unit) : LSTM simplifiÃ©</strong></p>
                        <p>Les <strong>GRU</strong> combinent les portes d'oubli et d'entrÃ©e en une seule "porte de mise Ã  jour", rÃ©duisant la complexitÃ© tout en gardant l'efficacitÃ©.</p>
                        
                        <p><strong>ğŸ“ Ã‰quations GRU :</strong></p>
                        <p>$$\\vec{r}_t = \\sigma(W_r \\cdot [\\vec{h}_{t-1}, \\vec{x}_t])$$ (porte de reset)</p>
                        <p>$$\\vec{z}_t = \\sigma(W_z \\cdot [\\vec{h}_{t-1}, \\vec{x}_t])$$ (porte de mise Ã  jour)</p>
                        <p>$$\\vec{\\tilde{h}}_t = \\tanh(W \\cdot [\\vec{r}_t \\odot \\vec{h}_{t-1}, \\vec{x}_t])$$ (candidat)</p>
                        <p>$$\\vec{h}_t = (1 - \\vec{z}_t) \\odot \\vec{h}_{t-1} + \\vec{z}_t \\odot \\vec{\\tilde{h}}_t$$ (Ã©tat final)</p>
                        
                        <p><strong>âš–ï¸ Comparaison LSTM vs GRU :</strong></p>
                        <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                            <tr style="background: #f8f9fa;">
                                <th style="padding: 0.8rem; border: 1px solid #dee2e6;">CritÃ¨re</th>
                                <th style="padding: 0.8rem; border: 1px solid #dee2e6;">LSTM</th>
                                <th style="padding: 0.8rem; border: 1px solid #dee2e6;">GRU</th>
                            </tr>
                            <tr>
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;"><strong>Portes</strong></td>
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;">3 portes + mÃ©moire cellulaire</td>
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;">2 portes seulement</td>
                            </tr>
                            <tr style="background: #f8f9fa;">
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;"><strong>ParamÃ¨tres</strong></td>
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;">Plus nombreux</td>
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;">25% de moins</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;"><strong>Vitesse</strong></td>
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;">Plus lent</td>
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;">Plus rapide</td>
                            </tr>
                            <tr style="background: #f8f9fa;">
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;"><strong>Performance</strong></td>
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;">LÃ©gÃ¨rement meilleur</td>
                                <td style="padding: 0.8rem; border: 1px solid #dee2e6;">TrÃ¨s proche</td>
                            </tr>
                        </table>
                        
                        <p><strong>ğŸš€ Ã‰volution vers les Transformers :</strong></p>
                        <ul>
                            <li>âš¡ <strong>ProblÃ¨me RNN/LSTM</strong> : traitement sÃ©quentiel (pas de parallÃ©lisation)</li>
                            <li>ğŸ§  <strong>Solution Transformer</strong> : mÃ©canisme d'attention (traitement parallÃ¨le)</li>
                            <li>ğŸ’¬ <strong>RÃ©sultat</strong> : ChatGPT, GPT-4, rÃ©volution du NLP</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Guide de choix :</strong></p>
                        <ul>
                            <li>ğŸ”¤ <strong>SÃ©quences courtes</strong> (< 50) â†’ GRU (plus rapide)</li>
                            <li>ğŸ“š <strong>SÃ©quences longues</strong> (> 100) â†’ LSTM (plus robuste)</li>
                            <li>ğŸ’¬ <strong>Texte moderne</strong> (> 1000) â†’ Transformer (parallÃ¨le)</li>
                            <li>ğŸ“ˆ <strong>SÃ©ries temporelles</strong> â†’ LSTM/GRU (structure temporelle)</li>
                        </ul>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : architecture pour diffÃ©rents problÃ¨mes",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>Pour chaque problÃ¨me, choisissez l'architecture optimale et justifiez :</p>
                        
                        <ol>
                            <li><strong>Traduction FranÃ§aisâ†’Wolof</strong> : phrases de 5-20 mots</li>
                            <li><strong>PrÃ©diction cours bourse</strong> : 100 jours d'historique</li>
                            <li><strong>Classification sentiment</strong> : tweets de 280 caractÃ¨res</li>
                            <li><strong>GÃ©nÃ©ration de musique</strong> : sÃ©quences de 1000+ notes</li>
                            <li><strong>Reconnaissance vocale</strong> : audio de 10 secondes</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('architecture-choice-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="architecture-choice-exercise" style="display: none;">
                        <ol>
                            <li><strong>Traduction Frâ†’Wolof :</strong> <strong>Encoder-Decoder LSTM</strong><br>
                                <em>Justification :</em> SÃ©quences de longueur variable, besoin de comprendre toute la phrase source</li>
                            <li><strong>PrÃ©diction bourse :</strong> <strong>LSTM multicouche</strong><br>
                                <em>Justification :</em> DÃ©pendances long terme cruciales, patterns complexes</li>
                            <li><strong>Sentiment tweets :</strong> <strong>GRU simple</strong><br>
                                <em>Justification :</em> SÃ©quences courtes, rapiditÃ© importante</li>
                            <li><strong>GÃ©nÃ©ration musique :</strong> <strong>Transformer</strong> ou <strong>LSTM trÃ¨s profond</strong><br>
                                <em>Justification :</em> TrÃ¨s longues sÃ©quences, patterns harmoniques complexes</li>
                            <li><strong>Reconnaissance vocale :</strong> <strong>CNN + LSTM</strong><br>
                                <em>Justification :</em> CNN pour features audio, LSTM pour sÃ©quence temporelle</li>
                        </ol>
                        </div>
                    `,
          },
          {
            type: "warning",
            icon: "âš ï¸",
            title: "Impact rÃ©volutionnaire : de la mÃ©moire Ã  l'intelligence",
            content: `
                        <p><strong>ğŸš€ Les RNN/LSTM ont rÃ©volutionnÃ© l'IA en lui donnant une mÃ©moire :</strong></p>
                        
                        <p><strong>ğŸ“± Applications quotidiennes :</strong></p>
                        <ul>
                            <li>ğŸ’¬ <strong>Traduction automatique</strong> : Google Translate comprend le contexte</li>
                            <li>ğŸ—£ï¸ <strong>Assistants vocaux</strong> : Siri se souvient de la conversation</li>
                            <li>ğŸ“ <strong>Correction automatique</strong> : votre tÃ©lÃ©phone prÃ©dit le mot suivant</li>
                            <li>ğŸµ <strong>Recommandations musicales</strong> : Spotify comprend vos goÃ»ts dans le temps</li>
                            <li>ğŸ“ˆ <strong>Trading algorithmique</strong> : analyse des tendances historiques</li>
                        </ul>
                        
                        <p><strong>ğŸ‡¸ğŸ‡³ Impact au SÃ©nÃ©gal :</strong></p>
                        <ul>
                            <li>ğŸŒ¾ <strong>Agriculture</strong> : prÃ©diction des rendements selon l'historique climatique</li>
                            <li>ğŸ¥ <strong>SantÃ©</strong> : suivi de l'Ã©volution des Ã©pidÃ©mies</li>
                            <li>ğŸ’° <strong>Finance</strong> : dÃ©tection de fraudes basÃ©e sur l'historique</li>
                            <li>ğŸ“š <strong>Ã‰ducation</strong> : systÃ¨mes adaptatifs qui se souviennent des difficultÃ©s</li>
                            <li>ğŸ—£ï¸ <strong>Langues locales</strong> : traduction automatique wolof-franÃ§ais</li>
                        </ul>
                        
                        <p><strong>ğŸ”® Ã‰volution historique :</strong></p>
                        <ul>
                            <li>ğŸ“… <strong>1990s</strong> : RNN simples (mÃ©moire courte)</li>
                            <li>ğŸ“… <strong>1997</strong> : LSTM inventÃ© (mÃ©moire longue)</li>
                            <li>ğŸ“… <strong>2014</strong> : GRU (simplification efficace)</li>
                            <li>ğŸ“… <strong>2017</strong> : Transformers (rÃ©volution attention)</li>
                            <li>ğŸ“… <strong>2023</strong> : ChatGPT (mÃ©moire + crÃ©ativitÃ©)</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Point clÃ© :</strong> Les RNN/LSTM ont Ã©tÃ© le <strong>pont crucial</strong> entre l'IA "amnÃ©sique" et l'IA "intelligente". Ils ont ouvert la voie aux LLMs modernes en prouvant qu'une IA avec mÃ©moire pouvait comprendre le langage et les sÃ©quences complexes !</p>
                        
                        <p><strong>ğŸ”® Prochaine Ã©tape :</strong> Attention et Transformers - l'architecture qui a rendu possible ChatGPT et l'IA gÃ©nÃ©rative moderne !</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ğŸ¤” Quelle est la principale innovation des LSTM par rapport aux RNN standards ?",
          options: [
            "A) Ils sont plus rapides Ã  entraÃ®ner",
            "B) Ils utilisent des portes pour contrÃ´ler le flux d'information",
            "C) Ils ont plus de neurones",
            "D) Ils n'utilisent pas de fonction d'activation",
          ],
          correct: 1,
          explanation:
            "Les LSTM introduisent des 'portes' (forget, input, output) qui apprennent Ã  contrÃ´ler quelles informations oublier, stocker ou exposer. Cela rÃ©sout le problÃ¨me du gradient qui disparaÃ®t et permet d'apprendre des dÃ©pendances Ã  long terme.",
        },
        prevModule: "cnn.html",
        nextModule: "../llm/attention.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
