<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Le Perceptron | IA4Ndada</title>

    <!-- MathJax pour les formules mathÃ©matiques -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <!-- Pyodide pour Python dans le navigateur -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script>

    <link rel="stylesheet" href="../../styles/module.css" />
  </head>
  <body>
    <!-- Navigation -->
    <nav class="nav">
      <div class="nav-container">
        <div class="nav-breadcrumb">
          <a href="../../index.html">ğŸ  Accueil</a>
          <span>â€º</span>
          <span>ğŸ§  Deep Learning</span>
          <span>â€º</span>
          <span>Le Perceptron</span>
        </div>
        <div class="progress-indicator">
          <span id="progress-text">Progression: 0%</span>
          <div class="progress-bar">
            <div
              class="progress-fill"
              id="progress-fill"
              style="width: 0%"
            ></div>
          </div>
        </div>
      </div>
    </nav>

    <!-- Contenu principal -->
    <div class="container">
      <h1>âš¡ Le Perceptron : Premier Neurone Artificiel</h1>
      <p class="subtitle">Module 4.1 - Deep Learning Fondamental</p>

      <!-- Objectifs -->
      <div class="objectives">
        <h2>ğŸ¯ Objectifs d'apprentissage</h2>
        <ul id="objectives-list">
          <!-- Les objectifs seront ajoutÃ©s dynamiquement -->
        </ul>
      </div>

      <!-- Contenu du module -->
      <div id="module-content">
        <!-- Le contenu sera ajoutÃ© dynamiquement -->
      </div>

      <!-- Quiz -->
      <div class="quiz" id="module-quiz" style="display: none">
        <div class="quiz-question" id="quiz-question"></div>
        <div class="quiz-options" id="quiz-options"></div>
        <div class="quiz-feedback" id="quiz-feedback"></div>
      </div>

      <!-- Checkpoint -->
      <div class="checkpoint">
        <h3>ğŸ‰ Checkpoint - Le Perceptron</h3>
        <p>
          FÃ©licitations ! Vous comprenez maintenant le premier neurone
          artificiel et les fondements du Deep Learning.
        </p>
        <button
          class="checkpoint-btn"
          id="checkpoint-btn"
          onclick="completeCheckpoint()"
        >
          Marquer comme complÃ©tÃ©
        </button>
      </div>

      <!-- Navigation entre modules -->
      <div class="module-nav">
        <a href="../ml/validation.html" class="nav-link" id="prev-link"
          >â† Module prÃ©cÃ©dent : Validation</a
        >
        <a href="neural-networks.html" class="nav-link" id="next-link"
          >Module suivant : RÃ©seaux de Neurones â†’</a
        >
      </div>
    </div>

    <script src="../../scripts/module-engine.js"></script>
    <script>
      // Configuration du module Perceptron
      const moduleConfig = {
        id: "dl-perceptron",
        title: "Le Perceptron : Premier Neurone Artificiel",
        category: "Deep Learning",
        objectives: [
          "Comprendre la rÃ©volution conceptuelle du perceptron",
          "MaÃ®triser le modÃ¨le mathÃ©matique du neurone artificiel",
          "Calculer manuellement l'apprentissage du perceptron",
          "Comprendre les limites et l'Ã©volution vers les rÃ©seaux",
        ],
        content: [
          {
            type: "concept",
            icon: "ğŸ’¡",
            title:
              "La rÃ©volution conceptuelle : de la programmation Ã  l'apprentissage",
            content: `
                        <p>Le <strong>perceptron</strong> (1957) marque une rÃ©volution : pour la premiÃ¨re fois, une machine peut <strong>apprendre automatiquement</strong> Ã  rÃ©soudre des problÃ¨mes sans qu'on lui programme explicitement la solution.</p>
                        
                        <p><strong>ğŸ”‘ Changement de paradigme :</strong></p>
                        <ul>
                            <li>ğŸ“ <strong>Programmation classique</strong> : "Si tempÃ©rature > 30Â°C, alors climatisation ON"</li>
                            <li>ğŸ§  <strong>Perceptron</strong> : "Voici 1000 exemples tempÃ©ratureâ†’dÃ©cision. Apprends la rÃ¨gle !"</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Inspiration biologique :</strong></p>
                        <p>Le perceptron s'inspire du <strong>neurone biologique</strong> :</p>
                        <ul>
                            <li>ğŸ§  <strong>Dendrites</strong> â†’ entrÃ©es pondÃ©rÃ©es</li>
                            <li>âš¡ <strong>Corps cellulaire</strong> â†’ sommation</li>
                            <li>ğŸ”¥ <strong>Seuil d'activation</strong> â†’ fonction d'activation</li>
                            <li>ğŸ“¡ <strong>Axone</strong> â†’ sortie binaire</li>
                        </ul>
                        
                        <p><strong>ğŸš€ Impact historique :</strong></p>
                        <ul>
                            <li>ğŸ¤– <strong>1957</strong> : Rosenblatt invente le perceptron</li>
                            <li>ğŸ“ˆ <strong>1960s</strong> : Euphorie - "machines pensantes" promises</li>
                            <li>â„ï¸ <strong>1969</strong> : "Hiver de l'IA" - limitations dÃ©couvertes</li>
                            <li>ğŸ”¥ <strong>1980s+</strong> : Renaissance avec les rÃ©seaux multicouches</li>
                            <li>ğŸ§  <strong>2010s+</strong> : Deep Learning rÃ©volutionne tout</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Pourquoi crucial aujourd'hui ?</strong></p>
                        <p>Chaque neurone dans ChatGPT, dans les voitures autonomes, dans la reconnaissance faciale... est un descendant direct du perceptron de 1957 !</p>
                    `,
          },
          {
            type: "intuition",
            icon: "ğŸ§ ",
            title: "L'analogie du gardien de sÃ©curitÃ© intelligent",
            content: `
                        <p>Imaginez un <strong>gardien de sÃ©curitÃ©</strong> Ã  l'entrÃ©e d'un bÃ¢timent gouvernemental Ã  Dakar :</p>
                        
                        <p><strong>ğŸ¯ Sa mission :</strong> DÃ©cider qui peut entrer (1) ou non (0)</p>
                        
                        <p><strong>ğŸ” Informations qu'il observe :</strong></p>
                        <ul>
                            <li>ğŸ‘” <strong>Tenue vestimentaire</strong> (score 1-10)</li>
                            <li>ğŸ“‹ <strong>Documents officiels</strong> (score 1-10)</li>
                            <li>ğŸ¯ <strong>Rendez-vous confirmÃ©</strong> (score 1-10)</li>
                            <li>ğŸ˜Š <strong>Attitude</strong> (score 1-10)</li>
                        </ul>
                        
                        <p><strong>ğŸ§  Comment il prend sa dÃ©cision :</strong></p>
                        <ol>
                            <li>ğŸ“Š <strong>PondÃ©ration</strong> : il donne plus d'importance aux documents (Ã—3) qu'Ã  la tenue (Ã—1)</li>
                            <li>â• <strong>Sommation</strong> : Score total = 1Ã—tenue + 3Ã—documents + 2Ã—rdv + 1Ã—attitude</li>
                            <li>âš–ï¸ <strong>Seuil</strong> : Si score total > 15 â†’ ENTRER, sinon â†’ REFUSER</li>
                        </ol>
                        
                        <p><strong>ğŸ“ˆ Apprentissage du gardien :</strong></p>
                        <ul>
                            <li>âœ… <strong>Bonne dÃ©cision</strong> : "Mes poids sont corrects, je continue"</li>
                            <li>âŒ <strong>Erreur</strong> : "Je dois ajuster mes prioritÃ©s (poids)"</li>
                            <li>ğŸ”„ <strong>AmÃ©lioration</strong> : modifier les poids selon les erreurs</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ C'est exactement le perceptron :</strong></p>
                        <ul>
                            <li>ğŸ”¢ <strong>EntrÃ©es</strong> : caractÃ©ristiques observÃ©es</li>
                            <li>âš–ï¸ <strong>Poids</strong> : importance de chaque caractÃ©ristique</li>
                            <li>â• <strong>Sommation</strong> : combinaison pondÃ©rÃ©e</li>
                            <li>ğŸ¯ <strong>Fonction d'activation</strong> : dÃ©cision finale</li>
                            <li>ğŸ“š <strong>Apprentissage</strong> : ajustement automatique des poids</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "ModÃ¨le mathÃ©matique du perceptron",
            content: `
                        <p><strong>ğŸ“ Formalisation rigoureuse :</strong></p>
                        
                        <p><strong>ğŸ”¢ EntrÃ©es :</strong> Vecteur \\(\\vec{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\) (voir <a href="../math/vectors.html">Module 1.1</a>)</p>
                        
                        <p><strong>âš–ï¸ Poids :</strong> Vecteur \\(\\vec{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix}\\) + biais \\(b\\)</p>
                        
                        <p><strong>â• Sommation pondÃ©rÃ©e :</strong></p>
                        <p>$$z = \\vec{w}^T \\vec{x} + b = \\sum_{i=1}^{n} w_i x_i + b$$</p>
                        
                        <p><strong>ğŸ¯ Fonction d'activation (seuil) :</strong></p>
                        <p>$$y = \\begin{cases} 1 & \\text{si } z \\geq 0 \\\\ 0 & \\text{si } z < 0 \\end{cases}$$</p>
                        
                        <p><strong>ğŸ” InterprÃ©tation gÃ©omÃ©trique :</strong></p>
                        <p>L'Ã©quation \\(\\vec{w}^T \\vec{x} + b = 0\\) dÃ©finit un <strong>hyperplan</strong> qui sÃ©pare l'espace en deux rÃ©gions :</p>
                        <ul>
                            <li>ğŸŸ¢ <strong>RÃ©gion positive</strong> : \\(\\vec{w}^T \\vec{x} + b > 0\\) â†’ classe 1</li>
                            <li>ğŸ”´ <strong>RÃ©gion nÃ©gative</strong> : \\(\\vec{w}^T \\vec{x} + b < 0\\) â†’ classe 0</li>
                        </ul>
                        
                        <p><strong>ğŸ“ En 2D :</strong> L'hyperplan devient une droite \\(w_1 x_1 + w_2 x_2 + b = 0\\)</p>
                        
                        <p><strong>ğŸ¯ Objectif d'apprentissage :</strong></p>
                        <p>Trouver \\(\\vec{w}\\) et \\(b\\) qui sÃ©parent correctement les classes dans les donnÃ©es d'entraÃ®nement.</p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Calcul manuel : exemple concret",
            content: `
                        <p><strong>ğŸ“ ProblÃ¨me :</strong> Classifier si un Ã©tudiant UCAD rÃ©ussira son examen d'IA</p>
                        
                        <p><strong>ğŸ”¢ Variables d'entrÃ©e :</strong></p>
                        <ul>
                            <li>\\(x_1\\) = Heures d'Ã©tude par semaine</li>
                            <li>\\(x_2\\) = Note en mathÃ©matiques (/20)</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ Sortie :</strong> y = 1 (rÃ©ussite) ou 0 (Ã©chec)</p>
                        
                        <p><strong>ğŸ“Š Exemple d'Ã©tudiant :</strong></p>
                        <p>Aminata : 15 heures d'Ã©tude, 14/20 en maths â†’ \\(\\vec{x} = \\begin{bmatrix} 15 \\\\ 14 \\end{bmatrix}\\)</p>
                        
                        <p><strong>âš–ï¸ Poids initiaux (hypothÃ¨se) :</strong></p>
                        <p>\\(\\vec{w} = \\begin{bmatrix} 0.3 \\\\ 0.5 \\end{bmatrix}\\), \\(b = -8\\)</p>
                        
                        <p><strong>ğŸ”¢ Calcul de la prÃ©diction :</strong></p>
                        <p><strong>Ã‰tape 1 :</strong> Sommation pondÃ©rÃ©e</p>
                        <p>$$z = w_1 x_1 + w_2 x_2 + b = 0.3 \\times 15 + 0.5 \\times 14 + (-8)$$</p>
                        <p>$$z = 4.5 + 7 - 8 = 3.5$$</p>
                        
                        <p><strong>Ã‰tape 2 :</strong> Fonction d'activation</p>
                        <p>$$y = \\begin{cases} 1 & \\text{si } z \\geq 0 \\\\ 0 & \\text{si } z < 0 \\end{cases}$$</p>
                        <p>Comme \\(z = 3.5 > 0\\), alors \\(y = 1\\)</p>
                        
                        <p><strong>ğŸ‰ PrÃ©diction :</strong> Aminata va <strong>rÃ©ussir</strong> son examen d'IA !</p>
                        
                        <p><strong>ğŸ” InterprÃ©tation des poids :</strong></p>
                        <ul>
                            <li>\\(w_1 = 0.3\\) : chaque heure d'Ã©tude ajoute 0.3 au score</li>
                            <li>\\(w_2 = 0.5\\) : chaque point en maths ajoute 0.5 au score</li>
                            <li>\\(b = -8\\) : seuil de base (il faut compenser ce handicap)</li>
                        </ul>
                    `,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "Algorithme d'apprentissage du perceptron",
            content: `
                        <p><strong>ğŸ¯ Comment le perceptron apprend-il automatiquement ?</strong></p>
                        
                        <p><strong>ğŸ“‹ Algorithme d'apprentissage :</strong></p>
                        <ol>
                            <li><strong>Initialisation :</strong> \\(\\vec{w} = \\vec{0}\\), \\(b = 0\\) (ou alÃ©atoire)</li>
                            <li><strong>Pour chaque exemple</strong> \\((\\vec{x}_i, y_i)\\) :
                                <ul>
                                    <li>Calculer \\(\\hat{y}_i = \\text{seuil}(\\vec{w}^T \\vec{x}_i + b)\\)</li>
                                    <li>Si \\(\\hat{y}_i \\neq y_i\\) (erreur), mettre Ã  jour :</li>
                                </ul>
                            </li>
                        </ol>
                        
                        <p><strong>ğŸ”§ RÃ¨gle de mise Ã  jour :</strong></p>
                        <p>$$\\vec{w} \\leftarrow \\vec{w} + \\eta (y_i - \\hat{y}_i) \\vec{x}_i$$</p>
                        <p>$$b \\leftarrow b + \\eta (y_i - \\hat{y}_i)$$</p>
                        
                        <p><strong>ğŸ” DÃ©cryptage de la formule :</strong></p>
                        <ul>
                            <li>\\(\\eta\\) = <strong>taux d'apprentissage</strong> (vitesse d'adaptation)</li>
                            <li>\\((y_i - \\hat{y}_i)\\) = <strong>erreur</strong> (-1, 0, ou +1)</li>
                            <li>\\(\\vec{x}_i\\) = <strong>direction de correction</strong></li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Logique intuitive :</strong></p>
                        <ul>
                            <li>âœ… <strong>PrÃ©diction correcte</strong> : erreur = 0 â†’ pas de changement</li>
                            <li>âŒ <strong>Faux positif</strong> : erreur = -1 â†’ diminuer les poids</li>
                            <li>âŒ <strong>Faux nÃ©gatif</strong> : erreur = +1 â†’ augmenter les poids</li>
                        </ul>
                        
                        <p><strong>ğŸ¯ ThÃ©orÃ¨me de convergence :</strong></p>
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>Si les donnÃ©es sont linÃ©airement sÃ©parables, le perceptron converge en un nombre fini d'Ã©tapes vers une solution parfaite.</strong>
                        </div>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Apprentissage manuel Ã©tape par Ã©tape",
            content: `
                        <p><strong>ğŸ“ Exemple concret :</strong> Apprendre Ã  classifier si un produit se vendra bien</p>
                        
                        <p><strong>ğŸ”¢ Variables :</strong></p>
                        <ul>
                            <li>\\(x_1\\) = Prix (en milliers FCFA)</li>
                            <li>\\(x_2\\) = QualitÃ© (score 1-10)</li>
                        </ul>
                        
                        <p><strong>ğŸ“Š Dataset d'entraÃ®nement :</strong></p>
                        <table style="margin: 1rem auto; border-collapse: collapse; text-align: center;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Produit</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Prix (xâ‚)</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">QualitÃ© (xâ‚‚)</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">SuccÃ¨s (y)</th>
                            </tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">A</td><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">8</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">B</td><td style="padding: 0.5rem; border: 1px solid #ddd;">5</td><td style="padding: 0.5rem; border: 1px solid #ddd;">3</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">C</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td><td style="padding: 0.5rem; border: 1px solid #ddd;">9</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">D</td><td style="padding: 0.5rem; border: 1px solid #ddd;">6</td><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                        </table>
                        
                        <p><strong>ğŸš€ Initialisation :</strong> \\(w_1 = 0, w_2 = 0, b = 0, \\eta = 0.1\\)</p>
                        
                        <p><strong>ğŸ“Š Ã‰poque 1 - Exemple A :</strong></p>
                        <div style="background: #f0f9ff; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>PrÃ©diction :</strong><br>
                            z = 0Ã—2 + 0Ã—8 + 0 = 0<br>
                            Å· = seuil(0) = 0 (car z = 0, on prend 0)<br><br>
                            
                            <strong>Erreur :</strong> y - Å· = 1 - 0 = 1<br><br>
                            
                            <strong>Mise Ã  jour :</strong><br>
                            wâ‚ â† 0 + 0.1 Ã— 1 Ã— 2 = 0.2<br>
                            wâ‚‚ â† 0 + 0.1 Ã— 1 Ã— 8 = 0.8<br>
                            b â† 0 + 0.1 Ã— 1 = 0.1<br><br>
                            
                            <strong>Nouveaux poids :</strong> wâ‚=0.2, wâ‚‚=0.8, b=0.1
                        </div>
                        
                        <p><strong>ğŸ“Š Ã‰poque 1 - Exemple B :</strong></p>
                        <div style="background: #fff3cd; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>PrÃ©diction :</strong><br>
                            z = 0.2Ã—5 + 0.8Ã—3 + 0.1 = 1 + 2.4 + 0.1 = 3.5<br>
                            Å· = seuil(3.5) = 1<br><br>
                            
                            <strong>Erreur :</strong> y - Å· = 0 - 1 = -1<br><br>
                            
                            <strong>Mise Ã  jour :</strong><br>
                            wâ‚ â† 0.2 + 0.1 Ã— (-1) Ã— 5 = 0.2 - 0.5 = -0.3<br>
                            wâ‚‚ â† 0.8 + 0.1 Ã— (-1) Ã— 3 = 0.8 - 0.3 = 0.5<br>
                            b â† 0.1 + 0.1 Ã— (-1) = 0<br><br>
                            
                            <strong>Nouveaux poids :</strong> wâ‚=-0.3, wâ‚‚=0.5, b=0
                        </div>
                        
                        <p><strong>ğŸ”„ AprÃ¨s plusieurs Ã©poques :</strong> Les poids convergent vers une solution qui sÃ©pare parfaitement les classes !</p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice pratique : apprentissage manuel complet",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>EntraÃ®nez manuellement un perceptron pour classifier des emails (spam/non-spam) :</p>
                        
                        <p><strong>ğŸ“Š Dataset :</strong></p>
                        <table style="margin: 1rem auto; border-collapse: collapse; text-align: center;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Email</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Nb mots "gratuit" (xâ‚)</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Nb liens (xâ‚‚)</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Spam (y)</th>
                            </tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">3</td><td style="padding: 0.5rem; border: 1px solid #ddd;">5</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">3</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">4</td><td style="padding: 0.5rem; border: 1px solid #ddd;">4</td><td style="padding: 0.5rem; border: 1px solid #ddd;">6</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td></tr>
                        </table>
                        
                        <p><strong>ğŸ“ Effectuez 2 Ã©poques complÃ¨tes :</strong></p>
                        <ol>
                            <li>Initialisez : wâ‚=0, wâ‚‚=0, b=0, Î·=0.1</li>
                            <li>Pour chaque exemple, calculez z, Å·, erreur</li>
                            <li>Mettez Ã  jour les poids selon la rÃ¨gle</li>
                            <li>RÃ©pÃ©tez pour la 2Ã¨me Ã©poque</li>
                            <li>Testez la frontiÃ¨re de dÃ©cision finale</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('perceptron-manual-training')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="perceptron-manual-training" style="display: none;">
                        <p><strong>Ã‰poque 1 :</strong></p>
                        <ul>
                            <li><strong>Email 1 :</strong> z=0, Å·=0, erreur=0 â†’ pas de changement</li>
                            <li><strong>Email 2 :</strong> z=0, Å·=0, erreur=1 â†’ wâ‚=0.3, wâ‚‚=0.5, b=0.1</li>
                            <li><strong>Email 3 :</strong> z=1.4, Å·=1, erreur=-1 â†’ wâ‚=0.2, wâ‚‚=0.3, b=0</li>
                            <li><strong>Email 4 :</strong> z=2.6, Å·=1, erreur=0 â†’ pas de changement</li>
                        </ul>
                        <p><strong>Ã‰poque 2 :</strong> RÃ©pÃ©ter avec les nouveaux poids...</p>
                        <p><strong>FrontiÃ¨re finale :</strong> 0.2xâ‚ + 0.3xâ‚‚ = 0 (approximativement)</p>
                        <p><strong>InterprÃ©tation :</strong> Les liens comptent plus que les mots "gratuit" pour dÃ©tecter le spam</p>
                        </div>
                    `,
          },
          {
            type: "code",
            title: "ImplÃ©mentation from scratch",
            description: "ImplÃ©mentons un perceptron complet :",
            code: `import numpy as np
import matplotlib.pyplot as plt

class PerceptronFromScratch:
    def __init__(self, learning_rate=0.1, max_epochs=100):
        self.learning_rate = learning_rate
        self.max_epochs = max_epochs
        self.weights = None
        self.bias = None
        self.errors_history = []
    
    def activation(self, z):
        """Fonction d'activation seuil"""
        return np.where(z >= 0, 1, 0)
    
    def fit(self, X, y):
        """EntraÃ®ner le perceptron"""
        n_samples, n_features = X.shape
        
        # Initialisation
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        print(f"ğŸ§  ENTRAÃNEMENT PERCEPTRON")
        print(f"ğŸ“Š DonnÃ©es: {n_samples} exemples, {n_features} features")
        print(f"âš¡ Taux d'apprentissage: {self.learning_rate}")
        print("=" * 50)
        
        for epoch in range(self.max_epochs):
            errors = 0
            
            for i in range(n_samples):
                # PrÃ©diction
                z = np.dot(X[i], self.weights) + self.bias
                y_pred = self.activation(z)
                
                # Calcul de l'erreur
                error = y[i] - y_pred
                
                if error != 0:
                    errors += 1
                    # Mise Ã  jour des poids
                    self.weights += self.learning_rate * error * X[i]
                    self.bias += self.learning_rate * error
                    
                    if epoch < 3:  # Afficher les premiÃ¨res Ã©poques
                        print(f"Ã‰poque {epoch+1}, Exemple {i+1}:")
                        print(f"  z = {z:.2f}, Å· = {y_pred}, y = {y[i]}, erreur = {error}")
                        print(f"  Nouveaux poids: {self.weights.round(2)}, biais: {self.bias:.2f}")
            
            self.errors_history.append(errors)
            
            if errors == 0:
                print(f"\\nâœ… Convergence atteinte Ã  l'Ã©poque {epoch + 1} !")
                break
            elif epoch < 5:
                print(f"\\nÃ‰poque {epoch + 1}: {errors} erreurs")
        
        return self
    
    def predict(self, X):
        """Faire des prÃ©dictions"""
        z = np.dot(X, self.weights) + self.bias
        return self.activation(z)
    
    def decision_boundary(self):
        """Retourner les paramÃ¨tres de la frontiÃ¨re de dÃ©cision"""
        return self.weights, self.bias

# DonnÃ©es de classification : rÃ©ussite aux examens UCAD
# x1 = heures d'Ã©tude, x2 = note en maths
X_train = np.array([
    [10, 12],  # Ã‰tudiant 1
    [15, 16],  # Ã‰tudiant 2  
    [5, 8],    # Ã‰tudiant 3
    [20, 18],  # Ã‰tudiant 4
    [8, 10],   # Ã‰tudiant 5
    [18, 15]   # Ã‰tudiant 6
])

y_train = np.array([0, 1, 0, 1, 0, 1])  # 0=Ã©chec, 1=rÃ©ussite

# EntraÃ®nement
perceptron = PerceptronFromScratch(learning_rate=0.1)
perceptron.fit(X_train, y_train)

print(f"\\nğŸ† RÃ‰SULTATS FINAUX:")
print(f"Poids finaux: {perceptron.weights.round(3)}")
print(f"Biais final: {perceptron.bias:.3f}")`,
          },
          {
            type: "code",
            title: "Visualisation de la frontiÃ¨re de dÃ©cision",
            description:
              "Visualisons comment le perceptron sÃ©pare les classes :",
            code: `# Visualisation des rÃ©sultats
plt.figure(figsize=(12, 5))

# Graphique 1: DonnÃ©es et frontiÃ¨re
plt.subplot(1, 2, 1)

# Points d'entraÃ®nement
echecs = X_train[y_train == 0]
reussites = X_train[y_train == 1]

plt.scatter(echecs[:, 0], echecs[:, 1], c='red', marker='x', s=100, label='Ã‰checs')
plt.scatter(reussites[:, 0], reussites[:, 1], c='green', marker='o', s=100, label='RÃ©ussites')

# FrontiÃ¨re de dÃ©cision: w1*x1 + w2*x2 + b = 0
w1, w2 = perceptron.weights
b = perceptron.bias

if w2 != 0:  # Ã‰viter division par zÃ©ro
    x1_range = np.linspace(0, 25, 100)
    x2_boundary = -(w1 * x1_range + b) / w2
    plt.plot(x1_range, x2_boundary, 'b-', linewidth=2, label='FrontiÃ¨re de dÃ©cision')

plt.xlabel('Heures d\\'Ã©tude par semaine')
plt.ylabel('Note en mathÃ©matiques (/20)')
plt.title('Classification RÃ©ussite/Ã‰chec UCAD')
plt.legend()
plt.grid(True, alpha=0.3)

# Graphique 2: Ã‰volution des erreurs
plt.subplot(1, 2, 2)
plt.plot(range(1, len(perceptron.errors_history) + 1), perceptron.errors_history, 'bo-')
plt.xlabel('Ã‰poque')
plt.ylabel('Nombre d\\'erreurs')
plt.title('Convergence du Perceptron')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("ğŸ“Š Visualisation terminÃ©e !")`,
          },
          {
            type: "code",
            title: "Test sur nouveaux exemples",
            description: "Testons notre perceptron entraÃ®nÃ© :",
            code: `# Nouveaux Ã©tudiants Ã  classifier
nouveaux_etudiants = np.array([
    [12, 14],  # Moussa
    [6, 9],    # Fatou
    [17, 17],  # Ibrahima
    [4, 7]     # Aissatou
])

noms = ['Moussa', 'Fatou', 'Ibrahima', 'Aissatou']

print("ğŸ¯ PRÃ‰DICTIONS SUR NOUVEAUX Ã‰TUDIANTS")
print("=" * 45)

predictions = perceptron.predict(nouveaux_etudiants)

for i, (nom, etudiant, pred) in enumerate(zip(noms, nouveaux_etudiants, predictions)):
    # Calcul du score de confiance
    z = np.dot(etudiant, perceptron.weights) + perceptron.bias
    confiance = abs(z)  # Distance Ã  la frontiÃ¨re
    
    resultat = "âœ… RÃ‰USSITE" if pred == 1 else "âŒ Ã‰CHEC"
    print(f"{nom:10s}: {etudiant[0]:2d}h Ã©tude, {etudiant[1]:2d}/20 maths â†’ {resultat}")
    print(f"           Score: {z:+.2f} (confiance: {confiance:.2f})")
    print()

# Analyse de la frontiÃ¨re apprise
print(f"ğŸ§  ANALYSE DU MODÃˆLE APPRIS:")
print(f"Ã‰quation frontiÃ¨re: {perceptron.weights[0]:.3f}Ã—heures + {perceptron.weights[1]:.3f}Ã—maths + {perceptron.bias:.3f} = 0")

if perceptron.weights[1] != 0:
    pente = -perceptron.weights[0] / perceptron.weights[1]
    intercept = -perceptron.bias / perceptron.weights[1]
    print(f"Forme y = mx + c: maths = {pente:.3f}Ã—heures + {intercept:.3f}")
    
    print(f"\\nğŸ’¡ INTERPRÃ‰TATION:")
    print(f"â€¢ Importance heures d'Ã©tude: {abs(perceptron.weights[0]):.3f}")
    print(f"â€¢ Importance note maths: {abs(perceptron.weights[1]):.3f}")
    
    if abs(perceptron.weights[1]) > abs(perceptron.weights[0]):
        print("â€¢ Les maths comptent plus que les heures d'Ã©tude !")
    else:
        print("â€¢ Les heures d'Ã©tude comptent plus que les maths !")`,
          },
          {
            type: "concept",
            icon: "ğŸ’¡",
            title: "Limitation fondamentale : le problÃ¨me XOR",
            content: `
                        <p><strong>âš ï¸ Le perceptron a une limitation cruciale :</strong> il ne peut rÃ©soudre que des problÃ¨mes <strong>linÃ©airement sÃ©parables</strong>.</p>
                        
                        <p><strong>ğŸ”´ Le problÃ¨me XOR (OU exclusif) :</strong></p>
                        <table style="margin: 1rem auto; border-collapse: collapse; text-align: center;">
                            <tr style="background: #e74c3c; color: white;">
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">xâ‚</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">xâ‚‚</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">XOR</th>
                            </tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                        </table>
                        
                        <p><strong>ğŸ¤” Pourquoi impossible ?</strong></p>
                        <p>Essayez de tracer une <strong>droite</strong> qui sÃ©pare :</p>
                        <ul>
                            <li>ğŸŸ¢ <strong>Classe 1</strong> : points (0,1) et (1,0)</li>
                            <li>ğŸ”´ <strong>Classe 0</strong> : points (0,0) et (1,1)</li>
                        </ul>
                        <p><strong>Impossible !</strong> Il faudrait une frontiÃ¨re courbe, pas droite.</p>
                        
                        <p><strong>ğŸ’¡ Analogie sÃ©nÃ©galaise :</strong></p>
                        <p>Imaginez classer les rÃ©gions du SÃ©nÃ©gal selon :</p>
                        <ul>
                            <li>ğŸ–ï¸ <strong>"CÃ´tiÃ¨res"</strong> : Dakar, Saint-Louis (littoral)</li>
                            <li>ğŸœï¸ <strong>"IntÃ©rieures"</strong> : Tambacounda, KÃ©dougou (intÃ©rieur)</li>
                        </ul>
                        <p>Une droite peut sÃ©parer cÃ´te/intÃ©rieur. Mais si on veut classer :</p>
                        <ul>
                            <li>ğŸ¯ <strong>"DÃ©veloppÃ©es"</strong> : Dakar (cÃ´te) + Tambacounda (intÃ©rieur)</li>
                            <li>ğŸ¯ <strong>"En dÃ©veloppement"</strong> : Saint-Louis (cÃ´te) + KÃ©dougou (intÃ©rieur)</li>
                        </ul>
                        <p>Aucune droite ne peut sÃ©parer ces 4 groupes ! Il faut des frontiÃ¨res plus complexes.</p>
                        
                        <p><strong>ğŸ”§ Solution :</strong> RÃ©seaux de neurones multicouches (prochains modules) !</p>
                    `,
          },
          {
            type: "code",
            title: "DÃ©monstration du problÃ¨me XOR",
            description: "Montrons pourquoi le perceptron Ã©choue sur XOR :",
            code: `# DonnÃ©es XOR
X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_xor = np.array([0, 1, 1, 0])

print("ğŸ”´ TENTATIVE DE RÃ‰SOLUTION XOR")
print("=" * 35)

# Tentative d'entraÃ®nement sur XOR
perceptron_xor = PerceptronFromScratch(learning_rate=0.1, max_epochs=20)
perceptron_xor.fit(X_xor, y_xor)

# PrÃ©dictions finales
predictions_xor = perceptron_xor.predict(X_xor)

print(f"\\nğŸ“Š RÃ‰SULTATS:")
print("EntrÃ©e â†’ Attendu vs PrÃ©dit")
for i in range(len(X_xor)):
    x1, x2 = X_xor[i]
    attendu = y_xor[i]
    predit = predictions_xor[i]
    status = "âœ…" if attendu == predit else "âŒ"
    print(f"({x1},{x2}) â†’ {attendu} vs {predit} {status}")

erreurs_finales = sum(y_xor != predictions_xor)
print(f"\\nğŸ“ˆ Erreurs finales: {erreurs_finales}/4")

if erreurs_finales > 0:
    print("âŒ Le perceptron ne peut pas rÃ©soudre XOR !")
    print("ğŸ’¡ Il faut des rÃ©seaux multicouches...")
else:
    print("âœ… RÃ©solution rÃ©ussie (trÃ¨s rare !)")`,
          },
          {
            type: "code",
            title: "Visualisation du problÃ¨me XOR",
            description:
              "Visualisons pourquoi XOR est impossible pour un perceptron :",
            code: `# Visualisation du problÃ¨me XOR
plt.figure(figsize=(15, 5))

# Graphique 1: ProblÃ¨me XOR
plt.subplot(1, 3, 1)
colors = ['red' if y == 0 else 'green' for y in y_xor]
plt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, alpha=0.7)

# Annotations
labels = ['(0,0)â†’0', '(0,1)â†’1', '(1,0)â†’1', '(1,1)â†’0']
for i, label in enumerate(labels):
    plt.annotate(label, (X_xor[i, 0], X_xor[i, 1]), 
                xytext=(10, 10), textcoords='offset points')

plt.title('ProblÃ¨me XOR\\n(Impossible avec une droite)')
plt.xlabel('xâ‚')
plt.ylabel('xâ‚‚')
plt.grid(True, alpha=0.3)

# Graphique 2: ProblÃ¨me linÃ©airement sÃ©parable
plt.subplot(1, 3, 2)
X_simple = np.array([[1, 1], [2, 2], [3, 1], [1, 3], [2, 3], [3, 3]])
y_simple = np.array([0, 0, 0, 1, 1, 1])

colors_simple = ['red' if y == 0 else 'green' for y in y_simple]
plt.scatter(X_simple[:, 0], X_simple[:, 1], c=colors_simple, s=100, alpha=0.7)

# FrontiÃ¨re de sÃ©paration possible
x_line = np.linspace(0.5, 3.5, 100)
y_line = 2.5 * np.ones_like(x_line)  # Ligne horizontale
plt.plot(x_line, y_line, 'b-', linewidth=2, label='FrontiÃ¨re possible')

plt.title('ProblÃ¨me LinÃ©airement SÃ©parable\\n(Possible avec une droite)')
plt.xlabel('xâ‚')
plt.ylabel('xâ‚‚')
plt.legend()
plt.grid(True, alpha=0.3)

# Graphique 3: Ã‰volution des erreurs
plt.subplot(1, 3, 3)
plt.plot(range(1, len(perceptron_xor.errors_history) + 1), 
         perceptron_xor.errors_history, 'ro-')
plt.xlabel('Ã‰poque')
plt.ylabel('Nombre d\\'erreurs')
plt.title('Ã‰chec de Convergence XOR')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("ğŸ“Š Visualisation du problÃ¨me XOR terminÃ©e !")
print("ğŸ’¡ Observation: Le perceptron ne peut tracer qu'une droite de sÃ©paration")`,
          },
          {
            type: "mathematique",
            icon: "âˆ‘",
            title: "ThÃ©orÃ¨me de convergence du perceptron",
            content: `
                        <p><strong>ğŸ¯ ThÃ©orÃ¨me fondamental (Rosenblatt, 1962) :</strong></p>
                        
                        <div style="background: #e8f5e9; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                            <strong>Si les donnÃ©es sont linÃ©airement sÃ©parables, l'algorithme du perceptron converge en un nombre fini d'Ã©tapes vers une solution qui classe parfaitement tous les exemples d'entraÃ®nement.</strong>
                        </div>
                        
                        <p><strong>ğŸ“ Preuve intuitive :</strong></p>
                        
                        <p><strong>1ï¸âƒ£ HypothÃ¨se :</strong> Il existe \\(\\vec{w}^*, b^*\\) tels que :</p>
                        <ul>
                            <li>Pour tout exemple positif : \\((\\vec{w}^*)^T \\vec{x}_i + b^* > \\gamma > 0\\)</li>
                            <li>Pour tout exemple nÃ©gatif : \\((\\vec{w}^*)^T \\vec{x}_i + b^* < -\\gamma < 0\\)</li>
                        </ul>
                        <p>oÃ¹ \\(\\gamma\\) est la <strong>marge de sÃ©paration</strong>.</p>
                        
                        <p><strong>2ï¸âƒ£ Fonction de Lyapunov :</strong></p>
                        <p>ConsidÃ©rons la distance entre les poids actuels et optimaux :</p>
                        <p>$$V_t = ||\\vec{w}_t - \\vec{w}^*||^2 + (b_t - b^*)^2$$</p>
                        
                        <p><strong>3ï¸âƒ£ DÃ©croissance Ã  chaque erreur :</strong></p>
                        <p>Quand il y a une erreur, la mise Ã  jour rapproche les poids de la solution optimale :</p>
                        <p>$$V_{t+1} < V_t - \\text{constante positive}$$</p>
                        
                        <p><strong>4ï¸âƒ£ Conclusion :</strong></p>
                        <ul>
                            <li>\\(V_t\\) dÃ©croÃ®t Ã  chaque erreur</li>
                            <li>\\(V_t \\geq 0\\) (distance toujours positive)</li>
                            <li>Donc \\(V_t\\) converge â†’ nombre fini d'erreurs â†’ convergence</li>
                        </ul>
                        
                        <p><strong>â±ï¸ Borne sur le nombre d'itÃ©rations :</strong></p>
                        <p>$$\\text{Nombre d'erreurs} \\leq \\frac{R^2}{\\gamma^2}$$</p>
                        <p>oÃ¹ R est le rayon de la boule contenant toutes les donnÃ©es.</p>
                        
                        <p><strong>ğŸ’¡ Remarque cruciale :</strong> Si les donnÃ©es ne sont PAS linÃ©airement sÃ©parables, le perceptron ne converge jamais !</p>
                    `,
          },
          {
            type: "exemple",
            icon: "ğŸ’»",
            title: "Exercice : analyse de sÃ©parabilitÃ©",
            content: `
                        <p><strong>ğŸ¯ Exercice Ã  rÃ©soudre :</strong></p>
                        <p>DÃ©terminez si ces datasets sont linÃ©airement sÃ©parables :</p>
                        
                        <p><strong>ğŸ“Š Dataset 1 :</strong> Classification Ã¢ge/salaire</p>
                        <table style="margin: 1rem auto; border-collapse: collapse; text-align: center;">
                            <tr style="background: #3498db; color: white;">
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Ã‚ge</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Salaire (Ã—100k)</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Senior (y)</th>
                            </tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">25</td><td style="padding: 0.5rem; border: 1px solid #ddd;">3</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">30</td><td style="padding: 0.5rem; border: 1px solid #ddd;">5</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">35</td><td style="padding: 0.5rem; border: 1px solid #ddd;">7</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">40</td><td style="padding: 0.5rem; border: 1px solid #ddd;">9</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td></tr>
                        </table>
                        
                        <p><strong>ğŸ“Š Dataset 2 :</strong> Classification circulaire</p>
                        <table style="margin: 1rem auto; border-collapse: collapse; text-align: center;">
                            <tr style="background: #e74c3c; color: white;">
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">xâ‚</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">xâ‚‚</th>
                                <th style="padding: 0.8rem; border: 1px solid #ddd;">Classe</th>
                            </tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td><td style="padding: 0.5rem; border: 1px solid #ddd;">1</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                            <tr><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">2</td><td style="padding: 0.5rem; border: 1px solid #ddd;">0</td></tr>
                        </table>
                        
                        <p><strong>ğŸ“ Pour chaque dataset :</strong></p>
                        <ol>
                            <li>Dessinez les points sur un graphique</li>
                            <li>Essayez de tracer une droite de sÃ©paration</li>
                            <li>Concluez sur la sÃ©parabilitÃ© linÃ©aire</li>
                            <li>PrÃ©disez si le perceptron convergera</li>
                        </ol>
                        
                        <p><strong>âœ… Solutions :</strong></p>
                        <button class="btn btn-copy" onclick="toggleSolution('separability-analysis-exercise')" style="margin-bottom: 1rem;">
                            ğŸ‘ï¸ Voir la solution
                        </button>
                        <div id="separability-analysis-exercise" style="display: none;">
                        <p><strong>Dataset 1 (Ã¢ge/salaire) :</strong></p>
                        <ul>
                            <li>âœ… <strong>LinÃ©airement sÃ©parable</strong> : droite diagonale sÃ©pare junior (bas-gauche) des senior (haut-droite)</li>
                            <li>âœ… <strong>Perceptron convergera</strong> vers une solution parfaite</li>
                            <li>ğŸ“ <strong>FrontiÃ¨re possible</strong> : Ã¢ge + salaire > seuil</li>
                        </ul>
                        <p><strong>Dataset 2 (circulaire) :</strong></p>
                        <ul>
                            <li>âŒ <strong>Non linÃ©airement sÃ©parable</strong> : classe 1 au centre, classe 0 aux coins</li>
                            <li>âŒ <strong>Perceptron oscillera</strong> sans jamais converger</li>
                            <li>ğŸ”§ <strong>Solution nÃ©cessaire</strong> : frontiÃ¨re circulaire (xâ‚Â² + xâ‚‚Â² < seuil)</li>
                        </ul>
                        </div>
                    `,
          },
          {
            type: "code",
            title: "Perceptron multicouche : solution au XOR",
            description: "Montrons comment 2 perceptrons rÃ©solvent XOR :",
            code: `# Solution XOR avec 2 perceptrons
def perceptron_simple(x1, x2, w1, w2, b):
    """Un perceptron simple"""
    z = w1 * x1 + w2 * x2 + b
    return 1 if z >= 0 else 0

def resoudre_xor_multicouche(x1, x2):
    """RÃ©solution XOR avec 2 perceptrons + 1 perceptron de sortie"""
    
    # Couche cachÃ©e : 2 perceptrons
    # Perceptron 1 : dÃ©tecte x1 OU x2 (au moins un des deux)
    h1 = perceptron_simple(x1, x2, w1=1, w2=1, b=-0.5)
    
    # Perceptron 2 : dÃ©tecte x1 ET x2 (les deux ensemble)  
    h2 = perceptron_simple(x1, x2, w1=1, w2=1, b=-1.5)
    
    # Couche de sortie : XOR = (x1 OU x2) ET NON(x1 ET x2)
    sortie = perceptron_simple(h1, h2, w1=1, w2=-2, b=-0.5)
    
    return h1, h2, sortie

print("ğŸ§  RÃ‰SOLUTION XOR AVEC RÃ‰SEAU MULTICOUCHE")
print("=" * 50)
print("EntrÃ©e â†’ Couche cachÃ©e â†’ Sortie")

for x1 in [0, 1]:
    for x2 in [0, 1]:
        h1, h2, sortie = resoudre_xor_multicouche(x1, x2)
        attendu = x1 ^ x2  # XOR en Python
        status = "âœ…" if sortie == attendu else "âŒ"
        print(f"({x1},{x2}) â†’ ({h1},{h2}) â†’ {sortie} (attendu: {attendu}) {status}")

print("\\nğŸ’¡ EXPLICATION:")
print("â€¢ Perceptron 1 (h1): dÃ©tecte 'au moins un'")
print("â€¢ Perceptron 2 (h2): dÃ©tecte 'les deux ensemble'") 
print("â€¢ Sortie: h1 ET NON(h2) = XOR !")
print("\\nğŸš€ C'est le principe des rÃ©seaux de neurones multicouches !")`,
          },
          {
            type: "warning",
            icon: "âš ï¸",
            title: "Du perceptron aux rÃ©seaux modernes : l'Ã©volution",
            content: `
                        <p><strong>ğŸ§¬ Ã‰volution du perceptron vers l'IA moderne :</strong></p>
                        
                        <p><strong>ğŸ“ˆ Ã‰tapes historiques :</strong></p>
                        <ul>
                            <li>âš¡ <strong>1957 - Perceptron</strong> : premier neurone artificiel</li>
                            <li>ğŸ§  <strong>1986 - RÃ©tropropagation</strong> : entraÃ®nement des rÃ©seaux multicouches</li>
                            <li>ğŸ”¥ <strong>2006 - Deep Learning</strong> : rÃ©seaux trÃ¨s profonds</li>
                            <li>ğŸ¤– <strong>2017 - Transformers</strong> : rÃ©volution du NLP</li>
                            <li>ğŸ’¬ <strong>2022 - ChatGPT</strong> : IA conversationnelle grand public</li>
                        </ul>
                        
                        <p><strong>ğŸ”§ AmÃ©liorations successives :</strong></p>
                        <ul>
                            <li>ğŸ¯ <strong>Fonctions d'activation</strong> : seuil â†’ sigmoÃ¯de â†’ ReLU â†’ attention</li>
                            <li>ğŸ§  <strong>Architectures</strong> : 1 couche â†’ multicouches â†’ CNN â†’ RNN â†’ Transformers</li>
                            <li>ğŸ“Š <strong>Optimisation</strong> : rÃ¨gle simple â†’ gradient descent â†’ Adam â†’ techniques avancÃ©es</li>
                            <li>ğŸ¨ <strong>RÃ©gularisation</strong> : aucune â†’ dropout â†’ batch norm â†’ techniques modernes</li>
                        </ul>
                        
                        <p><strong>ğŸŒŸ Applications modernes :</strong></p>
                        <ul>
                            <li>ğŸ–¼ï¸ <strong>Vision</strong> : reconnaissance d'objets, voitures autonomes</li>
                            <li>ğŸ’¬ <strong>Langage</strong> : ChatGPT, traduction automatique</li>
                            <li>ğŸµ <strong>Audio</strong> : reconnaissance vocale, gÃ©nÃ©ration musicale</li>
                            <li>ğŸ® <strong>Jeux</strong> : AlphaGo, IA de jeux vidÃ©o</li>
                            <li>ğŸ§¬ <strong>Sciences</strong> : dÃ©couverte de mÃ©dicaments, prÃ©diction de protÃ©ines</li>
                        </ul>
                        
                        <p><strong>ğŸ’¡ Point clÃ© :</strong> Le perceptron de 1957 contient dÃ©jÃ  tous les ingrÃ©dients conceptuels de l'IA moderne ! Les amÃ©liorations portent sur l'architecture, l'optimisation et la puissance de calcul, mais le principe fondamental reste le mÃªme.</p>
                        
                        <p><strong>ğŸ”® Prochaine Ã©tape :</strong> RÃ©seaux de neurones multicouches pour rÃ©soudre les problÃ¨mes non-linÃ©aires !</p>
                    `,
          },
        ],
        quiz: {
          question:
            "ğŸ¤” Pourquoi le perceptron ne peut-il pas rÃ©soudre le problÃ¨me XOR ?",
          options: [
            "A) Il n'a pas assez de poids",
            "B) Le taux d'apprentissage est trop faible",
            "C) Le problÃ¨me XOR n'est pas linÃ©airement sÃ©parable",
            "D) Il faut plus d'Ã©poques d'entraÃ®nement",
          ],
          correct: 2,
          explanation:
            "Le perceptron ne peut tracer qu'une droite de sÃ©paration (hyperplan linÃ©aire). Le problÃ¨me XOR nÃ©cessite une frontiÃ¨re non-linÃ©aire pour sÃ©parer les classes, ce qui est impossible avec un seul perceptron. Il faut des rÃ©seaux multicouches.",
        },
        prevModule: "../ml/validation.html",
        nextModule: "neural-networks.html",
      };

      // Initialiser le module
      document.addEventListener("DOMContentLoaded", function () {
        initializeModule(moduleConfig);
      });
    </script>
  </body>
</html>
